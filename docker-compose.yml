services:
  # --- Message Broker ---
  redis:
    image: "redis"
    container_name: redis
    restart: always
    ports:
      - "6379:6379"

  # --- API Gateway (The Brain) ---
  api_gateway:
    container_name: api_gateway
    build:
      context: .
      dockerfile: ./services/api_gateway/Dockerfile
    ports:
      - "8788:80" # Map host port 8788 to container port 80
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    environment:
      - PYTHONPATH=/app
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis

  # --- AI Worker Services ---

  ffmpeg_service:
    container_name: ffmpeg_service
    build:
      context: .
      dockerfile: ./services/workers/ffmpeg_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "ffmpeg_queue"]
    environment:
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - redis

  paddleocr_service:
    container_name: paddleocr_service
    build:
      context: .
      dockerfile: ./services/workers/paddleocr_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "paddleocr_queue"]
    environment:
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - redis

  inpainting_service:
    container_name: inpainting_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/inpainting_service/Dockerfile
    volumes:
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: on-failure
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "inpainting_queue"]
    environment:
      - PYTHONPATH=/app
    depends_on:
      - redis

  whisperx_service:
    container_name: whisperx_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/whisperx_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: on-failure
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "whisperx_queue"]
    environment:
      - PYTHONPATH=/app
    depends_on:
      - redis

  llm_service:
    container_name: llm_service
    build:
      context: .
      dockerfile: ./services/workers/llm_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "llm_queue"]
    environment:
      - PYTHONPATH=/app
    depends_on:
      - redis

  indextts_service:
    container_name: indextts_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/indextts_service/Dockerfile
    volumes:
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: on-failure
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "indextts_queue"]
    environment:
      - PYTHONPATH=/app
    depends_on:
      - redis

  gptsovits_service:
    container_name: gptsovits_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/gptsovits_service/Dockerfile
    volumes:
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share  # <--- ADDED
      - ./config.yml:/app/config.yml
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: on-failure
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "gptsovits_queue"]
    environment:
      - PYTHONPATH=/app
    depends_on:
      - redis