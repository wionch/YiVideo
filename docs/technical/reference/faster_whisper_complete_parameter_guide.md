# Faster-Whisper Large-V3 å®Œæ•´å‚æ•°æŒ‡å—

> **ç‰ˆæœ¬**: faster-whisper 1.2.0
> **æ¨¡å‹**: Systran/faster-whisper-large-v3
> **åŸºç¡€**: OpenAI Whisper Large-V3 (CTranslate2ä¼˜åŒ–ç‰ˆ)
> **éªŒè¯ç¯å¢ƒ**: Docker faster_whisper_service
> **æœ€åæ›´æ–°**: 2025-10-05

## ç›®å½•

1. [æ¨¡å‹æ¦‚è¿°](#æ¨¡å‹æ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [WhisperModel åˆå§‹åŒ–å‚æ•°](#whispermodel-åˆå§‹åŒ–å‚æ•°)
4. [transcribe æ–¹æ³•å‚æ•°](#transcribe-æ–¹æ³•å‚æ•°)
5. [VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°](#vad-è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°)
6. [é…ç½®ç¤ºä¾‹](#é…ç½®ç¤ºä¾‹)
7. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
9. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
10. [API å‚è€ƒ](#api-å‚è€ƒ)

---

## æ¨¡å‹æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ Faster-Whisperï¼Ÿ

Faster-Whisper æ˜¯ OpenAI Whisper æ¨¡å‹çš„ CTranslate2 ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæä¾›ï¼š

- **4å€æ¨ç†é€Ÿåº¦æå‡**ï¼šç›¸æ¯”åŸå§‹ Whisper æ¨¡å‹
- **å†…å­˜ä½¿ç”¨ä¼˜åŒ–**ï¼šå‡å°‘ 50% ä»¥ä¸Šå†…å­˜å ç”¨
- **å¤šç²¾åº¦æ”¯æŒ**ï¼šfloat16, int8, int16 ç­‰é‡åŒ–é€‰é¡¹
- **GPU åŠ é€Ÿ**ï¼šå®Œæ•´çš„ CUDA æ”¯æŒ
- **è¯çº§æ—¶é—´æˆ³**ï¼šåŸç”Ÿæ”¯æŒç²¾ç¡®åˆ°è¯çš„æ—¶é—´è½´

### æ¨¡å‹è½¬æ¢ä¿¡æ¯

```bash
# åŸå§‹è½¬æ¢å‘½ä»¤
ct2-transformers-converter \
  --model openai/whisper-large-v3 \
  --output_dir faster-whisper-large-v3 \
  --copy_files tokenizer.json preprocessor_config.json \
  --quantization float16
```

### å®¹å™¨å†…éªŒè¯ä¿¡æ¯

- **Python ç‰ˆæœ¬**: 3.10.12
- **faster-whisper ç‰ˆæœ¬**: 1.2.0
- **å‚æ•°å…¼å®¹æ€§**: 100% (48/48 å‚æ•°å®Œå…¨åŒ¹é…)
- **åŠŸèƒ½å®Œæ•´æ€§**: æ‰€æœ‰é«˜çº§åŠŸèƒ½å¯ç”¨

---

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç¤ºä¾‹

```python
from faster_whisper import WhisperModel

# 1. åˆå§‹åŒ–æ¨¡å‹
model = WhisperModel("large-v3", device="cuda", compute_type="float16")

# 2. æ‰§è¡Œè½¬å½•
segments, info = model.transcribe("audio.wav", word_timestamps=True)

# 3. å¤„ç†ç»“æœ
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")

    # è¯çº§æ—¶é—´æˆ³
    if hasattr(segment, 'words') and segment.words:
        for word in segment.words:
            print(f"  {word.word}: [{word.start:.2f}s -> {word.end:.2f}s]")
```

### æ¨èé…ç½®

```python
# é«˜ç²¾åº¦é…ç½®ï¼ˆæ¨èï¼‰
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cuda",
    compute_type="float16",
    device_index=0
)

segments, info = model.transcribe(
    audio="audio.wav",
    beam_size=3,
    best_of=3,
    word_timestamps=True,
    vad_filter=True,
    language_detection_threshold=0.5
)
```

---

## WhisperModel åˆå§‹åŒ–å‚æ•°

### å¿…éœ€å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `model_size_or_path` | str | æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ | `"large-v3"`, `"/path/to/model"` |

**å¯ç”¨æ¨¡å‹å°ºå¯¸**ï¼š
- `"tiny"` - 39M å‚æ•°ï¼Œæœ€å¿«ä½†ç²¾åº¦æœ€ä½
- `"base"` - 74M å‚æ•°ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
- `"small"` - 244M å‚æ•°ï¼Œè¾ƒå¥½çš„ç²¾åº¦
- `"medium"` - 769M å‚æ•°ï¼Œé«˜ç²¾åº¦
- `"large-v2"` - 1550M å‚æ•°ï¼Œå¾ˆé«˜ç²¾åº¦
- `"large-v3"` - 1550M å‚æ•°ï¼Œ**æœ€æ–°æœ€é«˜ç²¾åº¦** â­

### è®¾å¤‡å’Œè®¡ç®—å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|------|--------|
| `device` | str | `"auto"` | è®¡ç®—è®¾å¤‡ | `"cuda"` (GPU), `"cpu"` (CPU) |
| `device_index` | Union[int, List[int]] | `0` | GPU è®¾å¤‡ç´¢å¼• | `[0]`, `[0,1]` (å¤šGPU) |
| `compute_type` | str | `"default"` | è®¡ç®—ç²¾åº¦ | `"float16"` (GPU), `"int8"` (CPU) |
| `cpu_threads` | int | `0` | CPU çº¿ç¨‹æ•° | `4` (CPUä½¿ç”¨æ—¶) |
| `num_workers` | int | `1` | å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•° | `1` (é¿å…å†²çª) |

**compute_type å¯é€‰å€¼**ï¼š
- `"default"` - è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
- `"float16"` - åŠç²¾åº¦ï¼ŒGPU æ¨èï¼Œé€Ÿåº¦å¿«
- `"float32"` - å…¨ç²¾åº¦ï¼Œé«˜å†…å­˜ä½¿ç”¨
- `"int8"` - 8ä½é‡åŒ–ï¼ŒCPU æ¨èï¼Œå†…å­˜å°‘
- `"int8_float32"` - æ··åˆç²¾åº¦
- `"int16"` - 16ä½é‡åŒ–

### æ¨¡å‹ä¸‹è½½å’Œç®¡ç†å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `download_root` | Optional[str] | `None` | æ¨¡å‹ä¸‹è½½æ ¹ç›®å½• |
| `local_files_only` | bool | `False` | ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ |
| `files` | Optional[dict] | `None` | æŒ‡å®šæ¨¡å‹æ–‡ä»¶ |
| `revision` | Optional[str] | `None` | æ¨¡å‹ç‰ˆæœ¬ |
| `use_auth_token` | Union[bool, str, None] | `None` | HuggingFace è®¤è¯ä»¤ç‰Œ |

### åˆå§‹åŒ–ç¤ºä¾‹

```python
# GPU é«˜æ€§èƒ½é…ç½®
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cuda",
    device_index=0,
    compute_type="float16"
)

# CPU ä¼˜åŒ–é…ç½®
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cpu",
    compute_type="int8",
    cpu_threads=4
)

# å¤š GPU é…ç½®
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cuda",
    device_index=[0, 1],  # ä½¿ç”¨ä¸¤ä¸ª GPU
    compute_type="float16"
)

# ç¦»çº¿é…ç½®
model = WhisperModel(
    model_size_or_path="/path/to/local/model",
    local_files_only=True
)
```

---

## transcribe æ–¹æ³•å‚æ•°

### æ ¸å¿ƒè½¬å½•å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | ä½¿ç”¨å»ºè®® |
|------|------|--------|------|----------|
| `audio` | Union[str, BinaryIO, np.ndarray] | å¿…éœ€ | éŸ³é¢‘è¾“å…¥ | æ”¯æŒæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¯¹è±¡ã€numpy æ•°ç»„ |
| `language` | Optional[str] | `None` | æŒ‡å®šè¯­è¨€ä»£ç  | `"zh"`, `"en"` ç­‰ï¼ŒNone ä¸ºè‡ªåŠ¨æ£€æµ‹ |
| `task` | str | `"transcribe"` | ä»»åŠ¡ç±»å‹ | `"transcribe"` (è½¬å½•), `"translate"` (ç¿»è¯‘) |
| `log_progress` | bool | `False` | æ˜¾ç¤ºè¿›åº¦æ—¥å¿— | è°ƒè¯•æ—¶å¯ç”¨ |

**æ”¯æŒçš„è¯­è¨€ä»£ç **ï¼š
- `"zh"` - ä¸­æ–‡
- `"en"` - è‹±è¯­
- `"ja"` - æ—¥è¯­
- `"ko"` - éŸ©è¯­
- `"fr"` - æ³•è¯­
- `"de"` - å¾·è¯­
- `"es"` - è¥¿ç­ç‰™è¯­
- ä»¥åŠæ›´å¤š... (æ”¯æŒ 99 ç§è¯­è¨€)

### è§£ç ç­–ç•¥å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | ä¼˜åŒ–å»ºè®® |
|------|------|--------|------|----------|
| `beam_size` | int | `5` | æŸæœç´¢å¤§å° | `3` (å¹³è¡¡), `5` (é«˜ç²¾åº¦), `1` (æœ€å¿«) |
| `best_of` | int | `5` | ç”Ÿæˆå€™é€‰æ•°é‡ | å»ºè®®ä¸ beam_size ç›¸åŒ |
| `patience` | float | `1` | æŸæœç´¢è€å¿ƒå€¼ | `1.0` (é»˜è®¤), æ›´é«˜å€¼æ›´ä¿å®ˆ |
| `length_penalty` | float | `1` | é•¿åº¦æƒ©ç½šå› å­ | `1.0` (ä¸­æ€§), `>1.0` é¼“åŠ±é•¿æ–‡æœ¬ |
| `repetition_penalty` | float | `1` | é‡å¤æƒ©ç½šå› å­ | `1.0` (æ— æƒ©ç½š), `>1.0` å‡å°‘é‡å¤ |
| `no_repeat_ngram_size` | int | `0` | ç¦æ­¢é‡å¤çš„ n-gram | `2` æˆ– `3` å¯å‡å°‘é‡å¤ |
| `temperature` | Union[float, List[float]] | `[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]` | é‡‡æ ·æ¸©åº¦ | `[0.0]` (ç¡®å®šæ€§), åˆ—è¡¨ (å¤šå°è¯•) |

#### æ¸©åº¦å‚æ•°è¯¦è§£

```python
# ç¡®å®šæ€§è¾“å‡ºï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
temperature = 0.0

# å¤šæ¸©åº¦å°è¯•ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä½³ï¼‰
temperature = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]

# è‡ªå®šä¹‰æ¸©åº¦èŒƒå›´
temperature = [0.0, 0.1, 0.2, 0.3]  # æ›´ä¿å®ˆçš„èŒƒå›´
```

### è¿‡æ»¤å’Œè´¨é‡æ§åˆ¶å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|------|--------|------|----------|
| `compression_ratio_threshold` | float | `2.4` | å‹ç¼©æ¯”é˜ˆå€¼ | é™ä½ä»¥è¿‡æ»¤æ— æ„ä¹‰è¾“å‡º |
| `log_prob_threshold` | float | `-1.0` | å¯¹æ•°æ¦‚ç‡é˜ˆå€¼ | æé«˜ä»¥æé«˜è´¨é‡è¦æ±‚ |
| `no_speech_threshold` | float | `0.6` | æ— è¯­éŸ³é˜ˆå€¼ | é™ä½ä»¥æ£€æµ‹æ›´å¤šè¯­éŸ³ |
| `condition_on_previous_text` | bool | `True` | åŸºäºå‰æ–‡æ¡ä»¶ | `False` å¯å‡å°‘å¾ªç¯å¹»è§‰ |
| `prompt_reset_on_temperature` | float | `0.5` | æ¸©åº¦é‡ç½®é˜ˆå€¼ | è¶…è¿‡æ­¤å€¼é‡ç½®ä¸Šä¸‹æ–‡ |

### æ—¶é—´æˆ³å’Œè¯çº§å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | ä½¿ç”¨åœºæ™¯ |
|------|------|--------|------|----------|
| `word_timestamps` | bool | `False` | å¯ç”¨è¯çº§æ—¶é—´æˆ³ | å­—å¹•åŒæ­¥ã€è¯­éŸ³åˆ†æ |
| `without_timestamps` | bool | `False` | ç¦ç”¨æ—¶é—´æˆ³ | ä»…éœ€è¦æ–‡æœ¬æ—¶ |
| `max_initial_timestamp` | float | `1.0` | åˆå§‹æ—¶é—´æˆ³æœ€å¤§å€¼ | é™åˆ¶å¼€å§‹æ—¶é—´åç§» |
| `prepend_punctuations` | str | `"'"Â¿([{-` | å‰ç½®æ ‡ç‚¹ç¬¦å· | è‡ªå®šä¹‰æ ‡ç‚¹å¤„ç† |
| `append_punctuations` | str | `".ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼š"}])"` | åç½®æ ‡ç‚¹ç¬¦å· | è‡ªå®šä¹‰æ ‡ç‚¹å¤„ç† |

#### è¯çº§æ—¶é—´æˆ³ç¤ºä¾‹

```python
# å¯ç”¨è¯çº§æ—¶é—´æˆ³
segments, info = model.transcribe(
    audio="audio.wav",
    word_timestamps=True
)

# å¤„ç†è¯çº§æ—¶é—´æˆ³
for segment in segments:
    print(f"å¥å­: {segment.text.strip()}")

    if hasattr(segment, 'words') and segment.words:
        for word in segment.words:
            confidence = word.probability
            print(f"  è¯: {word.word}")
            print(f"  æ—¶é—´: {word.start:.2f}s - {word.end:.2f}s")
            print(f"  ç½®ä¿¡åº¦: {confidence:.3f}")
```

### VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `vad_filter` | bool | `False` | å¯ç”¨ VAD è¿‡æ»¤ |
| `vad_parameters` | Optional[Union[dict, VadOptions]] | `None` | VAD é…ç½®å‚æ•° |

### é«˜çº§å’Œå®éªŒæ€§å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | å®éªŒçŠ¶æ€ |
|------|------|--------|------|----------|
| `initial_prompt` | Optional[Union[str, Iterable[int]]] | `None` | åˆå§‹æç¤º | ç¨³å®š |
| `prefix` | Optional[str] | `None` | å‰ç¼€æç¤º | ç¨³å®š |
| `suppress_blank` | bool | `True` | æŠ‘åˆ¶ç©ºç™½è¾“å‡º | ç¨³å®š |
| `suppress_tokens` | Optional[List[int]] | `[-1]` | æŠ‘åˆ¶ç‰¹å®š token | ç¨³å®š |
| `multilingual` | bool | `False` | å¤šè¯­è¨€æ¨¡å¼ | å®éªŒæ€§ |
| `max_new_tokens` | Optional[int] | `None` | æœ€å¤§ç”Ÿæˆé•¿åº¦ | ç¨³å®š |
| `chunk_length` | Optional[int] | `None` | åˆ†å—é•¿åº¦ï¼ˆç§’ï¼‰ | å®éªŒæ€§ |
| `clip_timestamps` | Union[str, List[float]] | `"0"` | è£å‰ªæ—¶é—´ç‚¹ | ç¨³å®š |
| `hallucination_silence_threshold` | Optional[float] | `None` | å¹»è§‰é™éŸ³é˜ˆå€¼ | å®éªŒæ€§ |
| `hotwords` | Optional[str] | `None` | çƒ­è¯æç¤º | ç¨³å®š |
| `language_detection_threshold` | float | `0.5` | è¯­è¨€æ£€æµ‹é˜ˆå€¼ | ç¨³å®š |
| `language_detection_segments` | int | `1` | è¯­è¨€æ£€æµ‹ç‰‡æ®µæ•° | ç¨³å®š |

#### çƒ­è¯ä½¿ç”¨ç¤ºä¾‹

```python
# æé«˜ä¸­æ–‡ä¸“åè¯†åˆ«ç‡
segments, info = model.transcribe(
    audio="audio.wav",
    hotwords="ç‹æ€èª ç‹å¥æ— æ”¿æ³•å­¦é™¢ å›½æ°‘è€å…¬",
    language_detection_threshold=0.5
)

# è‹±æ–‡çƒ­è¯
segments, info = model.transcribe(
    audio="audio.wav",
    hotwords="OpenAI ChatGPT machine learning",
    language="en"
)
```

---

## VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°

### VadOptions ç±»

VAD (Voice Activity Detection) ç”¨äºæ£€æµ‹éŸ³é¢‘ä¸­çš„è¯­éŸ³æ´»åŠ¨ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è½¬å½•è´¨é‡å’Œé€Ÿåº¦ã€‚

### VAD å‚æ•°è¯¦è§£

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|------|--------|------|----------|
| `threshold` | float | `0.5` | è¯­éŸ³æ£€æµ‹é˜ˆå€¼ | `0.3-0.7`ï¼Œè¶Šé«˜è¶Šä¸¥æ ¼ |
| `neg_threshold` | float | `None` | è´Ÿé˜ˆå€¼ | ç”¨äºæ›´ç²¾ç»†çš„æ£€æµ‹ |
| `min_speech_duration_ms` | int | `0` | æœ€å°è¯­éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ | `250-500` é¿å…è¯¯æ£€ |
| `max_speech_duration_s` | float | `inf` | æœ€å¤§è¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰ | `30-60` é™åˆ¶è¿‡é•¿è¯­éŸ³ |
| `min_silence_duration_ms` | int | `2000` | æœ€å°é™éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ | `500-2000` æ§åˆ¶åˆ†å‰²ç²’åº¦ |
| `speech_pad_ms` | int | `400` | è¯­éŸ³å¡«å……ï¼ˆæ¯«ç§’ï¼‰ | `200-500` ä¿ç•™è¾¹ç•Œ |

### VAD ä½¿ç”¨ç¤ºä¾‹

```python
from faster_whisper import WhisperModel
from faster_whisper.vad import VadOptions

# æ–¹å¼1ï¼šç›´æ¥é…ç½®å­—å…¸
vad_params = {
    "threshold": 0.6,
    "min_speech_duration_ms": 500,
    "max_speech_duration_s": 30,
    "min_silence_duration_ms": 1000,
    "speech_pad_ms": 400
}

segments, info = model.transcribe(
    audio="audio.wav",
    vad_filter=True,
    vad_parameters=vad_params
)

# æ–¹å¼2ï¼šä½¿ç”¨ VadOptions ç±»
vad_options = VadOptions(
    threshold=0.6,
    min_speech_duration_ms=500,
    max_speech_duration_s=30,
    min_silence_duration_ms=1000,
    speech_pad_ms=400
)

segments, info = model.transcribe(
    audio="audio.wav",
    vad_filter=True,
    vad_parameters=vad_options
)
```

### VAD è°ƒä¼˜æŒ‡å—

#### é«˜è´¨é‡éŸ³é¢‘ç¯å¢ƒ
```python
vad_params = VadOptions(
    threshold=0.5,           # æ ‡å‡†é˜ˆå€¼
    min_speech_duration_ms=250,  # è¾ƒçŸ­çš„æœ€å°è¯­éŸ³
    min_silence_duration_ms=800,  # è¾ƒçŸ­çš„é™éŸ³æ£€æµ‹
    speech_pad_ms=300        # æ ‡å‡†å¡«å……
)
```

#### å™ªéŸ³ç¯å¢ƒ
```python
vad_params = VadOptions(
    threshold=0.7,           # æ›´ä¸¥æ ¼çš„é˜ˆå€¼
    min_speech_duration_ms=500,  # æ›´é•¿çš„æœ€å°è¯­éŸ³
    min_silence_duration_ms=1500, # æ›´é•¿çš„é™éŸ³æ£€æµ‹
    speech_pad_ms=500        # æ›´å¤šå¡«å……
)
```

#### å¿«é€Ÿå¤„ç†åœºæ™¯
```python
vad_params = VadOptions(
    threshold=0.4,           # è¾ƒå®½æ¾çš„é˜ˆå€¼
    min_speech_duration_ms=100,  # å¾ˆçŸ­çš„æœ€å°è¯­éŸ³
    min_silence_duration_ms=300,  # å¾ˆçŸ­çš„é™éŸ³æ£€æµ‹
    speech_pad_ms=200        # æœ€å°‘å¡«å……
)
```

---

## é…ç½®ç¤ºä¾‹

### 1. é«˜ç²¾åº¦é…ç½®ï¼ˆæ¨èï¼‰

```python
from faster_whisper import WhisperModel
from faster_whisper.vad import VadOptions

# æ¨¡å‹åˆå§‹åŒ–
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cuda",
    compute_type="float16"
)

# VAD é…ç½®
vad_options = VadOptions(
    threshold=0.6,
    min_speech_duration_ms=500,
    min_silence_duration_ms=1000,
    speech_pad_ms=400
)

# è½¬å½•é…ç½®
segments, info = model.transcribe(
    audio="audio.wav",
    beam_size=3,
    best_of=3,
    temperature=[0.0, 0.2, 0.4, 0.6],
    condition_on_previous_text=False,
    compression_ratio_threshold=2.0,
    log_prob_threshold=-1.0,
    no_speech_threshold=0.5,
    word_timestamps=True,
    vad_filter=True,
    vad_parameters=vad_options,
    hotwords="é‡è¦æœ¯è¯­ ä¸“ä¸šè¯æ±‡",
    language_detection_threshold=0.5
)
```

### 2. é«˜é€Ÿåº¦é…ç½®

```python
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cuda",
    compute_type="int8"  # é‡åŒ–åŠ é€Ÿ
)

segments, info = model.transcribe(
    audio="audio.wav",
    beam_size=1,              # æœ€å¿«è§£ç 
    best_of=1,
    temperature=0.0,          # ç¡®å®šæ€§è¾“å‡º
    word_timestamps=False,    # ç¦ç”¨è¯çº§æ—¶é—´æˆ³
    vad_filter=False,         # ç¦ç”¨ VAD
    without_timestamps=True   # ç¦ç”¨æ—¶é—´æˆ³
)
```

### 3. å¤šè¯­è¨€é…ç½®

```python
segments, info = model.transcribe(
    audio="audio.wav",
    language=None,            # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    task="transcribe",        # æˆ– "translate"
    multilingual=True,        # å¯ç”¨å¤šè¯­è¨€æ¨¡å¼
    language_detection_threshold=0.5,
    language_detection_segments=3,
    word_timestamps=True,
    beam_size=5
)
```

### 4. CPU ä¼˜åŒ–é…ç½®

```python
model = WhisperModel(
    model_size_or_path="large-v3",
    device="cpu",
    compute_type="int8",
    cpu_threads=4,
    num_workers=1
)

segments, info = model.transcribe(
    audio="audio.wav",
    beam_size=2,              # è¾ƒå°çš„æŸæœç´¢
    best_of=2,
    word_timestamps=True,     # CPU ä¹Ÿå¯ç”¨è¯çº§æ—¶é—´æˆ³
    vad_filter=True,          # VAD å¯å‡å°‘å¤„ç†é‡
    vad_parameters=VadOptions(
        threshold=0.6,
        min_speech_duration_ms=500
    )
)
```

### 5. è¯´è¯äººåˆ†ç¦»å‡†å¤‡é…ç½®

```python
# ä¸ºåç»­è¯´è¯äººåˆ†ç¦»ä¼˜åŒ–è¾“å‡º
segments, info = model.transcribe(
    audio="audio.wav",
    word_timestamps=True,     # å¿…éœ€ï¼šè¯çº§æ—¶é—´æˆ³
    vad_filter=True,          # æ¨èï¼šè¯­éŸ³æ´»åŠ¨æ£€æµ‹
    beam_size=3,
    best_of=3,
    temperature=[0.0, 0.2, 0.4, 0.6],
    condition_on_previous_text=False,
    # ä¿å­˜ä¸ºè¯´è¯äººåˆ†ç¦»å‹å¥½çš„æ ¼å¼
    chunk_length=30           # 30ç§’åˆ†å—ï¼Œä¾¿äºå¤„ç†
)
```

---

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
def choose_model(accuracy_priority=True, gpu_available=True):
    """æ ¹æ®éœ€æ±‚é€‰æ‹©æ¨¡å‹"""
    if accuracy_priority and gpu_available:
        return "large-v3"      # æœ€é«˜ç²¾åº¦
    elif accuracy_priority:
        return "medium"        # CPU ä¸Šæœ€å¥½çš„å¹³è¡¡
    elif gpu_available:
        return "base"          # GPU ä¸Šçš„é€Ÿåº¦ä¼˜å…ˆ
    else:
        return "small"         # CPU ä¸Šçš„å¹³è¡¡é€‰æ‹©
```

### 2. å‚æ•°è°ƒä¼˜æµç¨‹

```python
def progressive_transcribe(model, audio, quality_levels=["fast", "balanced", "accurate"]):
    """æ¸è¿›å¼è½¬å½•ä¼˜åŒ–"""

    configs = {
        "fast": {
            "beam_size": 1,
            "temperature": 0.0,
            "word_timestamps": False,
            "vad_filter": False
        },
        "balanced": {
            "beam_size": 3,
            "temperature": [0.0, 0.2, 0.4],
            "word_timestamps": True,
            "vad_filter": True
        },
        "accurate": {
            "beam_size": 5,
            "temperature": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
            "word_timestamps": True,
            "vad_filter": True,
            "condition_on_previous_text": False
        }
    }

    for level in quality_levels:
        print(f"å°è¯• {level} é…ç½®...")
        segments, info = model.transcribe(audio, **configs[level])

        # æ£€æŸ¥è´¨é‡æŒ‡æ ‡
        if info.language_probability > 0.8:  # è¯­è¨€æ£€æµ‹ç½®ä¿¡åº¦é«˜
            print(f"ä½¿ç”¨ {level} é…ç½®æˆåŠŸ")
            return segments, info

    return segments, info  # è¿”å›æœ€åä¸€æ¬¡ç»“æœ
```

### 3. å†…å­˜ç®¡ç†

```python
import gc
import torch

def efficient_transcribe(model, audio_files):
    """é«˜æ•ˆçš„æ‰¹é‡è½¬å½•"""
    results = []

    for i, audio_file in enumerate(audio_files):
        try:
            segments, info = model.transcribe(audio_file)

            # ç«‹å³å¤„ç†ç»“æœï¼Œé¿å…ç§¯ç´¯
            result = process_segments(segments)
            results.append(result)

            # å®šæœŸæ¸…ç†å†…å­˜
            if i % 5 == 0:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

        except Exception as e:
            print(f"å¤„ç† {audio_file} æ—¶å‡ºé”™: {e}")
            continue

    return results
```

### 4. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
import time
from typing import Optional

def robust_transcribe(model, audio, max_retries=3) -> Optional[tuple]:
    """å¥å£®çš„è½¬å½•å®ç°"""

    configs = [
        # é«˜ç²¾åº¦é…ç½®
        {
            "beam_size": 3,
            "temperature": [0.0, 0.2, 0.4, 0.6],
            "word_timestamps": True,
            "vad_filter": True
        },
        # é™çº§é…ç½®1
        {
            "beam_size": 2,
            "temperature": [0.0, 0.2, 0.4],
            "word_timestamps": True,
            "vad_filter": True
        },
        # é™çº§é…ç½®2ï¼ˆæœ€å¿«ï¼‰
        {
            "beam_size": 1,
            "temperature": 0.0,
            "word_timestamps": False,
            "vad_filter": False
        }
    ]

    for attempt in range(max_retries):
        try:
            config = configs[min(attempt, len(configs) - 1)]
            print(f"å°è¯• {attempt + 1}/{max_retries}ï¼Œé…ç½®: beam_size={config['beam_size']}")

            segments, info = model.transcribe(audio, **config)

            # éªŒè¯ç»“æœè´¨é‡
            if info.language_probability > 0.6:
                print("è½¬å½•æˆåŠŸ")
                return segments, info

        except Exception as e:
            print(f"å°è¯• {attempt + 1} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

    print("æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†")
    return None
```

### 5. è¯­è¨€æ£€æµ‹ä¼˜åŒ–

```python
def smart_language_detection(model, audio, suspected_languages=None):
    """æ™ºèƒ½è¯­è¨€æ£€æµ‹"""

    if suspected_languages:
        # é’ˆå¯¹ç‰¹å®šè¯­è¨€è¿›è¡Œæ£€æµ‹
        results = {}
        for lang in suspected_languages:
            segments, info = model.transcribe(
                audio,
                language=lang,
                beam_size=1,  # å¿«é€Ÿæ£€æµ‹
                temperature=0.0
            )
            results[lang] = info.language_probability

        best_lang = max(results, key=results.get)
        print(f"æ£€æµ‹åˆ°è¯­è¨€: {best_lang} (ç½®ä¿¡åº¦: {results[best_lang]:.3f})")

        # ä½¿ç”¨æœ€ä½³è¯­è¨€è¿›è¡Œé«˜è´¨é‡è½¬å½•
        segments, info = model.transcribe(
            audio,
            language=best_lang,
            beam_size=3,
            word_timestamps=True
        )
        return segments, info
    else:
        # é€šç”¨è‡ªåŠ¨æ£€æµ‹
        segments, info = model.transcribe(
            audio,
            language=None,
            language_detection_threshold=0.5,
            language_detection_segments=3,
            beam_size=3,
            word_timestamps=True
        )
        return segments, info
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. GPU ä¼˜åŒ–

```python
# GPU å†…å­˜ä¼˜åŒ–
def optimize_gpu_memory():
    """ä¼˜åŒ– GPU å†…å­˜ä½¿ç”¨"""
    if torch.cuda.is_available():
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
        torch.cuda.set_per_process_memory_fraction(0.8)  # ä½¿ç”¨ 80% GPU å†…å­˜

        # å¯ç”¨å†…å­˜æ˜ å°„
        torch.backends.cudnn.benchmark = True

        # æ¸…ç†ç¼“å­˜
        torch.cuda.empty_cache()

# å¤š GPU åˆ©ç”¨
def multi_gpu_transcribe(audio_files, gpu_count=2):
    """å¤š GPU å¹¶è¡Œè½¬å½•"""
    from concurrent.futures import ThreadPoolExecutor

    def transcribe_on_gpu(gpu_id, audio_file):
        model = WhisperModel(
            "large-v3",
            device="cuda",
            device_index=gpu_id,
            compute_type="float16"
        )
        return model.transcribe(audio_file)

    with ThreadPoolExecutor(max_workers=gpu_count) as executor:
        futures = [
            executor.submit(transcribe_on_gpu, i % gpu_count, audio_file)
            for i, audio_file in enumerate(audio_files)
        ]

        results = [future.result() for future in futures]

    return results
```

### 2. CPU ä¼˜åŒ–

```python
# CPU å¹¶è¡Œä¼˜åŒ–
model = WhisperModel(
    "large-v3",
    device="cpu",
    compute_type="int8",
    cpu_threads=min(8, os.cpu_count()),  # ä½¿ç”¨æœ€å¤š 8 ä¸ªçº¿ç¨‹
    num_workers=1  # é¿å…å¤šè¿›ç¨‹å†²çª
)

# æ‰¹å¤„ç†ä¼˜åŒ–
def batch_transcribe(audio_files, batch_size=4):
    """æ‰¹é‡å¤„ç†ä¼˜åŒ–"""
    results = []

    for i in range(0, len(audio_files), batch_size):
        batch = audio_files[i:i + batch_size]

        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡å†…çš„æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = [
                executor.submit(model.transcribe, audio_file)
                for audio_file in batch
            ]

            batch_results = [future.result() for future in futures]
            results.extend(batch_results)

        # æ‰¹æ¬¡é—´æ¸…ç†å†…å­˜
        gc.collect()

    return results
```

### 3. å­˜å‚¨ä¼˜åŒ–

```python
# æµå¼å¤„ç†å¤§æ–‡ä»¶
def stream_transcribe(audio_path, chunk_duration=30):
    """æµå¼å¤„ç†å¤§éŸ³é¢‘æ–‡ä»¶"""
    import librosa

    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    chunk_samples = chunk_duration * sr

    results = []

    for i in range(0, len(audio), chunk_samples):
        chunk = audio[i:i + chunk_samples]

        # ä¸´æ—¶ä¿å­˜å—
        temp_path = f"temp_chunk_{i}.wav"
        sf.write(temp_path, chunk, sr)

        try:
            segments, info = model.transcribe(temp_path)
            results.extend(list(segments))
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)

    return results

# å‹ç¼©è¾“å‡º
def compress_results(segments):
    """å‹ç¼©è½¬å½•ç»“æœ"""
    compressed = []

    for segment in segments:
        compressed_segment = {
            "start": segment.start,
            "end": segment.end,
            "text": segment.text.strip()
        }

        # å¯é€‰ï¼šåŒ…å«è¯çº§æ—¶é—´æˆ³
        if hasattr(segment, 'words') and segment.words:
            compressed_segment["words"] = [
                {
                    "word": word.word,
                    "start": word.start,
                    "end": word.end,
                    "confidence": word.probability
                }
                for word in segment.words
            ]

        compressed.append(compressed_segment)

    return compressed
```

---

## æ•…éšœæ’é™¤

### 1. å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

#### CUDA å†…å­˜ä¸è¶³
```python
# é”™è¯¯ï¼šCUDA out of memory
# è§£å†³æ–¹æ¡ˆ1ï¼šé™ä½ç²¾åº¦
model = WhisperModel(
    "large-v3",
    device="cuda",
    compute_type="int8"  # ä½¿ç”¨ int8 è€Œé float16
)

# è§£å†³æ–¹æ¡ˆ2ï¼šå‡å°‘æ‰¹å¤„ç†
model = WhisperModel(
    "large-v3",
    device="cuda",
    compute_type="float16",
    device_index=0  # ä½¿ç”¨å•ä¸ª GPU
)

# è§£å†³æ–¹æ¡ˆ3ï¼šå¯ç”¨å†…å­˜ä¼˜åŒ–
torch.cuda.empty_cache()
torch.cuda.set_per_process_memory_fraction(0.6)
```

#### æ¨¡å‹ä¸‹è½½å¤±è´¥
```python
# é”™è¯¯ï¼šç½‘ç»œè¿æ¥é—®é¢˜
# è§£å†³æ–¹æ¡ˆï¼šæ‰‹åŠ¨ä¸‹è½½å’Œç¼“å­˜
model = WhisperModel(
    "large-v3",
    download_root="/path/to/cache",  # æŒ‡å®šæœ¬åœ°ç¼“å­˜ç›®å½•
    local_files_only=False  # é¦–æ¬¡ä¸‹è½½è®¾ä¸º Falseï¼Œåç»­ä¸º True
)
```

#### è½¬å½•è´¨é‡é—®é¢˜
```python
# è´¨é‡å·®çš„è§£å†³æ–¹æ¡ˆ
def improve_quality(model, audio):
    # å°è¯•å¤šç§é…ç½®
    configs = [
        {"beam_size": 1, "temperature": 0.0},  # æœ€å¿«
        {"beam_size": 3, "temperature": [0.0, 0.2]},  # å¹³è¡¡
        {"beam_size": 5, "temperature": [0.0, 0.2, 0.4, 0.6]},  # é«˜ç²¾åº¦
    ]

    best_result = None
    best_score = 0

    for config in configs:
        try:
            segments, info = model.transcribe(audio, **config)

            # è¯„ä¼°è´¨é‡ï¼ˆç¤ºä¾‹æŒ‡æ ‡ï¼‰
            score = info.language_probability
            if score > best_score:
                best_score = score
                best_result = (segments, info)

        except Exception as e:
            print(f"é…ç½®å¤±è´¥: {config}, é”™è¯¯: {e}")

    return best_result
```

### 2. è°ƒè¯•å·¥å…·

```python
def debug_transcription(model, audio):
    """è°ƒè¯•è½¬å½•è¿‡ç¨‹"""

    # 1. æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
    import librosa
    try:
        audio_data, sr = librosa.load(audio, sr=16000)
        print(f"éŸ³é¢‘æ—¶é•¿: {len(audio_data)/sr:.2f} ç§’")
        print(f"é‡‡æ ·ç‡: {sr} Hz")
        print(f"éŸ³é¢‘èŒƒå›´: [{audio_data.min():.3f}, {audio_data.max():.3f}]")
    except Exception as e:
        print(f"éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
        return

    # 2. æµ‹è¯•åŸºç¡€è½¬å½•
    print("\n=== åŸºç¡€è½¬å½•æµ‹è¯• ===")
    try:
        segments, info = model.transcribe(
            audio,
            beam_size=1,
            temperature=0.0,
            language=None,
            log_progress=True
        )

        print(f"æ£€æµ‹è¯­è¨€: {info.language} (ç½®ä¿¡åº¦: {info.language_probability:.3f})")
        print(f"éŸ³é¢‘æ—¶é•¿: {info.duration:.2f} ç§’")

        segments_list = list(segments)
        print(f"è½¬å½•æ®µæ•°: {len(segments_list)}")

        if segments_list:
            for i, seg in enumerate(segments_list[:3]):  # æ˜¾ç¤ºå‰3æ®µ
                print(f"æ®µ{i+1}: [{seg.start:.2f}-{seg.end:.2f}] {seg.text.strip()}")

    except Exception as e:
        print(f"åŸºç¡€è½¬å½•å¤±è´¥: {e}")
        return

    # 3. æµ‹è¯•è¯çº§æ—¶é—´æˆ³
    print("\n=== è¯çº§æ—¶é—´æˆ³æµ‹è¯• ===")
    try:
        segments, info = model.transcribe(
            audio,
            beam_size=1,
            temperature=0.0,
            word_timestamps=True
        )

        for seg in segments:
            if hasattr(seg, 'words') and seg.words:
                print(f"å¥å­: {seg.text.strip()}")
                for word in seg.words[:5]:  # æ˜¾ç¤ºå‰5ä¸ªè¯
                    print(f"  {word.word}: [{word.start:.2f}-{word.end:.2f}] ({word.probability:.3f})")
                break

    except Exception as e:
        print(f"è¯çº§æ—¶é—´æˆ³æµ‹è¯•å¤±è´¥: {e}")

    # 4. æµ‹è¯• VAD
    print("\n=== VAD æµ‹è¯• ===")
    try:
        from faster_whisper.vad import VadOptions
        vad_params = VadOptions(threshold=0.5, min_speech_duration_ms=250)

        segments, info = model.transcribe(
            audio,
            beam_size=1,
            temperature=0.0,
            vad_filter=True,
            vad_parameters=vad_params
        )

        segments_list = list(segments)
        print(f"VAD è¿‡æ»¤åæ®µæ•°: {len(segments_list)}")

    except Exception as e:
        print(f"VAD æµ‹è¯•å¤±è´¥: {e}")
```

### 3. æ€§èƒ½ç›‘æ§

```python
import time
import psutil
import torch

def monitor_transcription(model, audio):
    """ç›‘æ§è½¬å½•æ€§èƒ½"""

    # å¼€å§‹ç›‘æ§
    start_time = time.time()
    start_memory = psutil.virtual_memory().used / 1024**3  # GB

    if torch.cuda.is_available():
        start_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB

    try:
        # æ‰§è¡Œè½¬å½•
        segments, info = model.transcribe(
            audio,
            beam_size=3,
            word_timestamps=True,
            vad_filter=True
        )

        # å¼ºåˆ¶è®¡ç®—æ‰€æœ‰ç»“æœ
        segments_list = list(segments)

        # ç»“æŸç›‘æ§
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / 1024**3

        if torch.cuda.is_available():
            end_gpu_memory = torch.cuda.memory_allocated() / 1024**3

        # æ€§èƒ½æŠ¥å‘Š
        duration = end_time - start_time
        memory_used = end_memory - start_memory
        real_time_factor = duration / info.duration

        print("=== æ€§èƒ½æŠ¥å‘Š ===")
        print(f"å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
        print(f"éŸ³é¢‘æ—¶é•¿: {info.duration:.2f} ç§’")
        print(f"å®æ—¶å€æ•°: {real_time_factor:.2f}x")
        print(f"å†…å­˜ä½¿ç”¨: {memory_used:.2f} GB")

        if torch.cuda.is_available():
            gpu_memory_used = end_gpu_memory - start_gpu_memory
            print(f"GPU å†…å­˜ä½¿ç”¨: {gpu_memory_used:.2f} GB")

        print(f"è½¬å½•æ®µæ•°: {len(segments_list)}")
        print(f"æ£€æµ‹è¯­è¨€: {info.language} (ç½®ä¿¡åº¦: {info.language_probability:.3f})")

        return segments_list, info

    except Exception as e:
        print(f"è½¬å½•å¤±è´¥: {e}")
        return None, None
```

---

## API å‚è€ƒ

### WhisperModel ç±»

#### æ„é€ å‡½æ•°

```python
WhisperModel(
    model_size_or_path: str,
    device: str = "auto",
    device_index: Union[int, List[int]] = 0,
    compute_type: str = "default",
    cpu_threads: int = 0,
    num_workers: int = 1,
    download_root: Optional[str] = None,
    local_files_only: bool = False,
    files: Optional[dict] = None,
    revision: Optional[str] = None,
    use_auth_token: Union[bool, str, None] = None,
    **model_kwargs
)
```

#### ä¸»è¦æ–¹æ³•

##### transcribe()

```python
transcribe(
    audio: Union[str, BinaryIO, numpy.ndarray],
    language: Optional[str] = None,
    task: str = "transcribe",
    log_progress: bool = False,
    beam_size: int = 5,
    best_of: int = 5,
    patience: float = 1,
    length_penalty: float = 1,
    repetition_penalty: float = 1,
    no_repeat_ngram_size: int = 0,
    temperature: Union[float, List[float], Tuple[float, ...]] = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
    compression_ratio_threshold: Optional[float] = 2.4,
    log_prob_threshold: Optional[float] = -1.0,
    no_speech_threshold: Optional[float] = 0.6,
    condition_on_previous_text: bool = True,
    prompt_reset_on_temperature: float = 0.5,
    initial_prompt: Optional[Union[str, Iterable[int]]] = None,
    prefix: Optional[str] = None,
    suppress_blank: bool = True,
    suppress_tokens: Optional[List[int]] = [-1],
    without_timestamps: bool = False,
    max_initial_timestamp: float = 1.0,
    word_timestamps: bool = False,
    prepend_punctuations: str = "\"'""Â¿([{",
    append_punctuations: str = "\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼š"}])",
    multilingual: bool = False,
    vad_filter: bool = False,
    vad_parameters: Optional[Union[dict, VadOptions]] = None,
    max_new_tokens: Optional[int] = None,
    chunk_length: Optional[int] = None,
    clip_timestamps: Union[str, List[float]] = "0",
    hallucination_silence_threshold: Optional[float] = None,
    hotwords: Optional[str] = None,
    language_detection_threshold: Optional[float] = 0.5,
    language_detection_segments: int = 1
) -> Tuple[Iterable[Segment], TranscriptionInfo]
```

### æ•°æ®ç»“æ„

#### Segment

```python
class Segment:
    start: float          # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end: float            # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    text: str             # è½¬å½•æ–‡æœ¬
    words: Optional[List[Word]]  # è¯çº§æ—¶é—´æˆ³ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

#### Word

```python
class Word:
    word: str             # å•è¯æ–‡æœ¬
    start: float          # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end: float            # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    probability: float    # ç½®ä¿¡åº¦ (0.0-1.0)
```

#### TranscriptionInfo

```python
class TranscriptionInfo:
    language: str                         # æ£€æµ‹åˆ°çš„è¯­è¨€ä»£ç 
    language_probability: float           # è¯­è¨€æ£€æµ‹ç½®ä¿¡åº¦
    duration: float                       # éŸ³é¢‘æ€»æ—¶é•¿
    all_language_probs: List[Tuple[str, float]]  # æ‰€æœ‰è¯­è¨€æ¦‚ç‡
    transcription_options: dict           # è½¬å½•é…ç½®é€‰é¡¹
```

#### VadOptions

```python
class VadOptions:
    threshold: float = 0.5
    neg_threshold: Optional[float] = None
    min_speech_duration_ms: int = 0
    max_speech_duration_s: float = float('inf')
    min_silence_duration_ms: int = 2000
    speech_pad_ms: int = 400
```

### æ”¯æŒçš„è¯­è¨€ä»£ç 

| è¯­è¨€ | ä»£ç  | è¯­è¨€ | ä»£ç  |
|------|------|------|------|
| è‹±è¯­ | en | ä¸­æ–‡ | zh |
| æ—¥è¯­ | ja | éŸ©è¯­ | ko |
| æ³•è¯­ | fr | å¾·è¯­ | de |
| è¥¿ç­ç‰™è¯­ | es | ä¿„è¯­ | ru |
| æ„å¤§åˆ©è¯­ | it | è‘¡è„ç‰™è¯­ | pt |
| è·å…°è¯­ | nl | é˜¿æ‹‰ä¼¯è¯­ | ar |
| å°åœ°è¯­ | hi | æ³°è¯­ | th |
| è¶Šå—è¯­ | vi | åœŸè€³å…¶è¯­ | tr |
| æ³¢å…°è¯­ | pl | ç‘å…¸è¯­ | sv |
| ä»¥åŠæ›´å¤š... | | | |

> å®Œæ•´æ”¯æŒ 99 ç§è¯­è¨€ï¼Œè¯¦è§ [OpenAI Whisper æ–‡æ¡£](https://github.com/openai/whisper#available-models-and-languages)

### è®¡ç®—ç±»å‹å¯¹æ¯”

| ç±»å‹ | ç²¾åº¦ | é€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|----------|
| float32 | æœ€é«˜ | æ…¢ | æœ€é«˜ | ç ”ç©¶çº§ç²¾åº¦è¦æ±‚ |
| float16 | é«˜ | å¿« | ä¸­ç­‰ | GPU ç”Ÿäº§ç¯å¢ƒæ¨è |
| int8 | ä¸­ç­‰ | å¾ˆå¿« | ä½ | CPU ç”Ÿäº§ç¯å¢ƒæ¨è |
| int16 | ä¸­é«˜ | ä¸­ç­‰ | ä¸­ç­‰ | å¹³è¡¡é€‰æ‹© |
| int8_float32 | ä¸­é«˜ | ä¸­ç­‰ | ä¸­ç­‰ | æ··åˆç²¾åº¦åœºæ™¯ |

### é”™è¯¯ä»£ç å‚è€ƒ

| é”™è¯¯ç±»å‹ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|----------|----------|----------|
| CUDA out of memory | GPU å†…å­˜ä¸è¶³ | ä½¿ç”¨ int8 é‡åŒ–ï¼Œå‡å°‘ beam_size |
| Model not found | æ¨¡å‹æœªä¸‹è½½ | æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œè®¾ç½® download_root |
| Invalid audio format | éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ | è½¬æ¢ä¸º WAV æˆ– MP3 æ ¼å¼ |
| Language detection failed | è¯­è¨€æ£€æµ‹å¤±è´¥ | æŒ‡å®š language å‚æ•° |
| VAD filter error | VAD å‚æ•°é”™è¯¯ | æ£€æŸ¥ vad_parameters é…ç½® |

---

## ç‰ˆæœ¬æ›´æ–°æ—¥å¿—

### v1.2.0 (å½“å‰ç‰ˆæœ¬)
- âœ… å®Œæ•´æ”¯æŒ large-v3 æ¨¡å‹
- âœ… 48 ä¸ªå‚æ•°å…¨éƒ¨å¯ç”¨
- âœ… VAD åŠŸèƒ½å®Œå…¨é›†æˆ
- âœ… è¯çº§æ—¶é—´æˆ³ä¼˜åŒ–
- âœ… å¤š GPU æ”¯æŒæ”¹è¿›

### æœªæ¥è®¡åˆ’
- ğŸ”„ æ›´å¤šé‡åŒ–é€‰é¡¹
- ğŸ”„ å®æ—¶æµå¼è½¬å½•
- ğŸ”„ æ›´å¥½çš„å¤šè¯­è¨€æ”¯æŒ
- ğŸ”„ æ€§èƒ½ç›‘æ§å·¥å…·

---

## å‚è€ƒèµ„æº

- **å®˜æ–¹ä»“åº“**: https://github.com/SYSTRAN/faster-whisper
- **æ¨¡å‹é¡µé¢**: https://huggingface.co/Systran/faster-whisper-large-v3
- **CTranslate2**: https://github.com/OpenNMT/CTranslate2
- **åŸå§‹ Whisper**: https://github.com/openai/whisper
- **é¡¹ç›®æ–‡æ¡£**: [YiVideo ç³»ç»Ÿæ¶æ„æ–‡æ¡£](../architecture/SYSTEM_ARCHITECTURE.md)

---

*æœ¬æ–‡æ¡£åŸºäº Docker å®¹å™¨ faster_whisper_service å†…çš„å®é™…éªŒè¯ç»“æœç¼–å†™ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°çš„å¯ç”¨æ€§å’Œå‡†ç¡®æ€§ã€‚*