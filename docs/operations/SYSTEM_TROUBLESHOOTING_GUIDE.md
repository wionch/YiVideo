# ç³»ç»Ÿé€šç”¨æ•…éšœæ’é™¤æŒ‡å—

## ç›®å½•

1. [å¿«é€Ÿè¯Šæ–­](#å¿«é€Ÿè¯Šæ–­)
2. [å¸¸è§é”™è¯¯ä»£ç ](#å¸¸è§é”™è¯¯ä»£ç )
3. [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
4. [èµ„æºé—®é¢˜](#èµ„æºé—®é¢˜)
5. [ç½‘ç»œé—®é¢˜](#ç½‘ç»œé—®é¢˜)
6. [é…ç½®é—®é¢˜](#é…ç½®é—®é¢˜)
7. [è°ƒè¯•å·¥å…·](#è°ƒè¯•å·¥å…·)
8. [è”ç³»æ”¯æŒ](#è”ç³»æ”¯æŒ)

---

## å¿«é€Ÿè¯Šæ–­

### è¯Šæ–­æµç¨‹å›¾

```
å¼€å§‹
  â†“
æ£€æŸ¥æœåŠ¡çŠ¶æ€ â†’ æ­£å¸¸ â†’ æ£€æŸ¥å·¥ä½œæµæ‰§è¡Œ
  â†“                      â†“
å¼‚å¸¸                    æ­£å¸¸ â†’ é—®é¢˜è§£å†³
  â†“                      â†“
æ£€æŸ¥æ—¥å¿—æ–‡ä»¶            å¼‚å¸¸ â†’ æ£€æŸ¥è¾“å…¥æ•°æ®
  â†“                      â†“
ä¿®å¤é—®é¢˜                æ­£å¸¸ â†’ æ£€æŸ¥é…ç½®
  â†“                      â†“
é‡å¯æœåŠ¡                å¼‚å¸¸ â†’ æ£€æŸ¥ç¯å¢ƒ
  â†“                      â†“
éªŒè¯ä¿®å¤                ä¿®å¤ç¯å¢ƒ
  â†“                      â†“
é—®é¢˜è§£å†³                é‡å¯æœåŠ¡
```

### å¿«é€Ÿæ£€æŸ¥æ¸…å•

```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps
docker-compose logs --tail=50

# 2. æ£€æŸ¥ç³»ç»Ÿèµ„æº
nvidia-smi
free -h
df -h

# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
curl http://localhost:8788/health
curl http://localhost:6379/ping

# 4. æ£€æŸ¥ GPU é”çŠ¶æ€
curl http://localhost:8788/api/v1/monitoring/gpu-lock/health

# 5. æ£€æŸ¥æœ€è¿‘çš„å·¥ä½œæµ
curl http://localhost:8788/v1/workflows/recent
```

---

## å¸¸è§é”™è¯¯ä»£ç 

### API é”™è¯¯ (4xx/5xx)

#### 400 Bad Request

**é”™è¯¯ä¿¡æ¯**: "Bad Request: invalid input parameters"

**å¯èƒ½åŸå› **:
- è¯·æ±‚å‚æ•°æ ¼å¼é”™è¯¯
- ç¼ºå°‘å¿…éœ€å‚æ•°
- å‚æ•°å€¼ä¸åœ¨æœ‰æ•ˆèŒƒå›´å†…

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥è¯·æ±‚æ ¼å¼
curl -X POST http://localhost:8788/v1/workflows \
  -H "Content-Type: application/json" \
  -d '{"video_path": "/app/videos/test.mp4"}'

# éªŒè¯å‚æ•°
python scripts/validate_request.py
```

#### 401 Unauthorized

**é”™è¯¯ä¿¡æ¯**: "Unauthorized: invalid API key"

**å¯èƒ½åŸå› **:
- API å¯†é’¥æ— æ•ˆæˆ–ç¼ºå¤±
- è®¤è¯æœåŠ¡ä¸å¯ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ API å¯†é’¥é…ç½®
grep -r "api_key" config.yml

# é‡å¯è®¤è¯æœåŠ¡
docker-compose restart auth_service
```

#### 404 Not Found

**é”™è¯¯ä¿¡æ¯**: "Not Found: workflow does not exist"

**å¯èƒ½åŸå› **:
- å·¥ä½œæµ ID ä¸å­˜åœ¨
- å·¥ä½œæµå·²è¿‡æœŸ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥å·¥ä½œæµçŠ¶æ€
curl http://localhost:8788/v1/workflows/status/{workflow_id}

# æŸ¥çœ‹æœ€è¿‘çš„å·¥ä½œæµ
curl http://localhost:8788/v1/workflows/recent
```

#### 500 Internal Server Error

**é”™è¯¯ä¿¡æ¯**: "Internal Server Error"

**å¯èƒ½åŸå› **:
- æœåŠ¡å™¨å†…éƒ¨é”™è¯¯
- æ•°æ®åº“è¿æ¥å¤±è´¥
- GPU èµ„æºä¸è¶³
- ASRæˆ–è¯´è¯äººåˆ†ç¦»æœåŠ¡å†…éƒ¨é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥è¯¦ç»†æ—¥å¿—
docker-compose logs --tail=100 api_gateway
docker-compose logs --tail=100 faster_whisper_service
docker-compose logs --tail=100 pyannote_audio_service

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
docker-compose exec redis redis-cli ping

# æ£€æŸ¥ GPU çŠ¶æ€
nvidia-smi

# æ£€æŸ¥ ASR æœåŠ¡çŠ¶æ€
docker exec faster_whisper_service celery -A app.celery_app inspect active

# æ£€æŸ¥è¯´è¯äººåˆ†ç¦»æœåŠ¡çŠ¶æ€
docker exec pyannote_audio_service celery -A app.celery_app inspect active
```

### ASR ä¸è¯´è¯äººåˆ†ç¦»æœåŠ¡ç‰¹å®šé”™è¯¯ (faster-whisper & pyannote)

#### æ¨¡å‹åŠ è½½é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: "Failed to load ASR/Diarization model"

**å¯èƒ½åŸå› **:
- ASR (Faster-Whisper) æˆ–è¯´è¯äººåˆ†ç¦» (Pyannote) æ¨¡å‹æ–‡ä»¶æŸå
- ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•ä» Hugging Face ä¸‹è½½æ¨¡å‹
- æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ (è·¯å¾„å–å†³äº config.yml)
# ls -la /path/to/your/model/cache/

# æ¸…ç†æ¨¡å‹ç¼“å­˜
# rm -rf /path/to/your/model/cache/*

# è§¦å‘æ¨¡å‹é‡æ–°ä¸‹è½½
docker-compose restart faster_whisper_service pyannote_audio_service
```

#### Hugging Face è®¤è¯é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: "Failed to download model from Hugging Face Hub" æˆ– "Authorization required" (é€šå¸¸æ¥è‡ª `pyannote_audio_service`)

**å¯èƒ½åŸå› **:
- HF_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–æ— æ•ˆ
- use_auth_token å‚æ•°æœªæ­£ç¡®é…ç½®
- ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ HF_TOKEN æ˜¯å¦é…ç½® (ä¸»è¦å½±å“ pyannote_audio_service)
docker exec pyannote_audio_service env | grep HF_TOKEN

# 2. å¦‚æœéœ€è¦ï¼Œé‡æ–°æ„å»ºå®¹å™¨
docker-compose build pyannote_audio_service --no-cache
docker-compose up -d pyannote_audio_service

# 3. éªŒè¯
docker-compose logs --tail=20 pyannote_audio_service
```


#### Faster-Whisper åç«¯é—®é¢˜

**é”™è¯¯ä¿¡æ¯**: "Faster-Whisper initialization failed" æˆ–æ€§èƒ½ä¸‹é™

**å¯èƒ½åŸå› **:
- ctranslate2 ç‰ˆæœ¬ä¸å…¼å®¹
- faster-whisper é…ç½®é”™è¯¯
- GPU é©±åŠ¨é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ faster-whisper ä¾èµ–ç‰ˆæœ¬
docker exec faster_whisper_service pip list | grep -E "(faster-whisper|ctranslate2)"

# 2. éªŒè¯é…ç½®
grep -A 10 "faster_whisper" config.yml

# 3. æµ‹è¯•åŸç”Ÿåç«¯é™çº§

# 4. é‡å¯æœåŠ¡
docker-compose restart faster_whisper_service

# 5. æ€§èƒ½å¯¹æ¯”æµ‹è¯• (ç¤ºä¾‹)
# time python scripts/performance_benchmark.py --service faster_whisper
```

#### GPU å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯**: "CUDA out of memory"

**å¯èƒ½åŸå› **:
- æ‰¹å¤„ç†å¤§å°è¿‡å¤§
- æ¨¡å‹è¿‡å¤§
- å…¶ä»–è¿›ç¨‹å ç”¨æ˜¾å­˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
nvidia-smi

# è°ƒæ•´æ‰¹å¤„ç†å¤§å°
# ç¼–è¾‘ config.yml
faster_whisper_service:
  batch_size: 2  # å‡å°æ‰¹å¤„ç†å¤§å°

# é‡å¯æœåŠ¡
docker-compose restart faster_whisper_service
```

#### éŸ³é¢‘å¤„ç†é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: "Audio processing failed"

**å¯èƒ½åŸå› **:
- éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ
- éŸ³é¢‘æ–‡ä»¶æŸå
- FFmpeg ä¸å¯ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ FFmpeg (é€šå¸¸åœ¨ ffmpeg_service ä¸­)
docker-compose exec ffmpeg_service ffmpeg -version

# éªŒè¯éŸ³é¢‘æ–‡ä»¶
docker-compose exec ffmpeg_service ffprobe /app/videos/test.mp4

# é‡æ–°å®‰è£… FFmpeg
docker-compose build --no-cache ffmpeg_service
```

---

## æ€§èƒ½é—®é¢˜

### æ‰§è¡Œæ—¶é—´è¿‡é•¿

#### è¯Šæ–­æ­¥éª¤

```bash
# 1. æ£€æŸ¥ GPU åˆ©ç”¨ç‡
watch -n 1 nvidia-smi

# 2. æ£€æŸ¥ CPU ä½¿ç”¨
htop

# 3. æ£€æŸ¥ç£ç›˜ I/O
iostat -x 1

# 4. æ£€æŸ¥ç½‘ç»œå¸¦å®½
iftop

# 5. è¿è¡Œæ€§èƒ½åˆ†æ
python scripts/performance_analysis.py
```

#### ä¼˜åŒ–æ–¹æ¡ˆ

```yaml
# config.yml ä¼˜åŒ–
faster_whisper_service:
  batch_size: 4              # ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
  compute_type: "float16"    # ä½¿ç”¨åŠç²¾åº¦
  use_faster_whisper: true    # å¯ç”¨ Faster-Whisper
  faster_whisper_threads: 4  # ä¼˜åŒ–çº¿ç¨‹æ•°

pipeline:
  detect_keyframes: true     # å¯ç”¨å…³é”®å¸§æ£€æµ‹
  use_image_concat: true     # å¯ç”¨å›¾åƒæ‹¼æ¥
```

### å†…å­˜æ³„æ¼

#### è¯Šæ–­æ­¥éª¤

```bash
# 1. ç›‘æ§å†…å­˜ä½¿ç”¨
watch -n 1 'free -h'

# 2. æ£€æŸ¥ Docker å®¹å™¨å†…å­˜
docker stats --format "table {{.Container}}\t{{.MemUsage}}"

# 3. åˆ†æå†…å­˜ä½¿ç”¨
python scripts/memory_analysis.py
```

#### è§£å†³æ–¹æ¡ˆ

```python
# æ·»åŠ å†…å­˜æ¸…ç†ä»£ç 
import torch
import gc

def cleanup_memory():
    """æ¸…ç†å†…å­˜"""
    torch.cuda.empty_cache()
    gc.collect()
```

### GPU åˆ©ç”¨ç‡ä½

#### è¯Šæ–­æ­¥éª¤

```bash
# 1. æ£€æŸ¥ GPU çŠ¶æ€
nvidia-smi dmon

# 2. æ£€æŸ¥ GPU è¿›ç¨‹
nvidia-smi pmon

# 3. æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
# è°ƒæ•´ config.yml ä¸­çš„ batch_size

# 2. å¯ç”¨ GPU åŠ é€Ÿ
ç¡®ä¿ config.yml ä¸­ device: "cuda"

# 3. æ£€æŸ¥ GPU é©±åŠ¨
sudo nvidia-smi --query-gpu=driver_version,name --format=csv
```

---

## èµ„æºé—®é¢˜

### CPU èµ„æºä¸è¶³

#### è¯Šæ–­

```bash
# æ£€æŸ¥ CPU ä½¿ç”¨ç‡
top
htop

# æ£€æŸ¥ CPU æ ¸å¿ƒæ•°
nproc

# æ£€æŸ¥è¿›ç¨‹æ•°
ps aux | wc -l
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. é™åˆ¶è¿›ç¨‹æ•°
ulimit -u 4096

# 2. ä¼˜åŒ– CPU äº²å’Œæ€§
taskset -c 0,1,2,3 python script.py

# 3. å‡çº§ CPU æˆ–å¢åŠ æ ¸å¿ƒæ•°
```

### å†…å­˜ä¸è¶³

#### è¯Šæ–­

```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
cat /proc/meminfo

# æ£€æŸ¥å†…å­˜æ³„æ¼
valgrind --leak-check=full python script.py
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. å¢åŠ äº¤æ¢ç©ºé—´
sudo fallocate -l 4G /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 2. ä¼˜åŒ–å†…å­˜ä½¿ç”¨
# è°ƒæ•´ config.yml ä¸­çš„å†…å­˜ç›¸å…³å‚æ•°

# 3. å¢åŠ ç‰©ç†å†…å­˜
```

### ç£ç›˜ç©ºé—´ä¸è¶³

#### è¯Šæ–­

```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨
df -h
du -sh /app/*

# æŸ¥æ‰¾å¤§æ–‡ä»¶
find /app -type f -size +100M
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. æ¸…ç†æ—¥å¿—æ–‡ä»¶
find /logs -name "*.log" -mtime +7 -delete

# 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find /tmp -name "*.tmp" -delete

# 3. æ¸…ç† Docker ç¼“å­˜
docker system prune -f

# 4. æ‰©å±•ç£ç›˜ç©ºé—´
```

---

## ç½‘ç»œé—®é¢˜

### è¿æ¥è¶…æ—¶

#### è¯Šæ–­

```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping 8.8.8.8

# æ£€æŸ¥ç«¯å£ç›‘å¬
netstat -tlnp | grep 8788

# æ£€æŸ¥é˜²ç«å¢™
sudo ufw status
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
sudo ufw allow 8788
sudo ufw allow 6379

# 2. é‡å¯ç½‘ç»œæœåŠ¡
sudo systemctl restart network

# 3. æ£€æŸ¥ DNS é…ç½®
cat /etc/resolv.conf
```

### Redis è¿æ¥é—®é¢˜

#### è¯Šæ–­

```bash
# æ£€æŸ¥ Redis çŠ¶æ€
docker-compose ps redis
docker-compose logs redis

# æµ‹è¯• Redis è¿æ¥
redis-cli -h redis -p 6379 ping
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. é‡å¯ Redis
docker-compose restart redis

# 2. æ£€æŸ¥ Redis é…ç½®
docker-compose exec redis redis-cli config get *

# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
docker-compose exec redis redis-cli info clients
```

### API é™æµ

#### è¯Šæ–­

```bash
# æ£€æŸ¥ API è¯·æ±‚é¢‘ç‡
curl -I http://localhost:8788/health

# æ£€æŸ¥é™æµé…ç½®
grep -r "rate_limit" config.yml
```

#### è§£å†³æ–¹æ¡ˆ

```yaml
# è°ƒæ•´é™æµé…ç½®
api_gateway:
  rate_limit:
    requests_per_minute: 100
    burst_size: 10
```

---

## é…ç½®é—®é¢˜

### é…ç½®æ–‡ä»¶é”™è¯¯

#### è¯Šæ–­

```bash
# éªŒè¯é…ç½®æ–‡ä»¶
python scripts/validate_config.py

# æ£€æŸ¥é…ç½®æ–‡ä»¶è¯­æ³•
python -c "import yaml; yaml.safe_load(open('config.yml'))"
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. å¤‡ä»½å½“å‰é…ç½®
cp config.yml config.yml.backup

# 2. ä½¿ç”¨é»˜è®¤é…ç½®
cp config.yml.default config.yml

# 3. é‡æ–°é…ç½®
python scripts/setup_config.py
```

### ç¯å¢ƒå˜é‡é—®é¢˜

#### è¯Šæ–­

```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
env | grep -i -E "(faster_whisper|pyannote)"

# æ£€æŸ¥ Docker ç¯å¢ƒå˜é‡
docker-compose exec api_gateway env
```

#### è§£å†³æ–¹æ¡ˆ

```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
# export OBSOLETE_WHISPERX_MODEL_PATH=/app/models

# 2. æ›´æ–° docker-compose.yml
environment:
  - CUDA_VISIBLE_DEVICES=0
  # - OBSOLETE_WHISPERX_MODEL_PATH=/app/models
```

---

## è°ƒè¯•å·¥å…·

### æ—¥å¿—åˆ†æå·¥å…·

```bash
# 1. å®æ—¶æ—¥å¿—ç›‘æ§
docker-compose logs -f --tail=100

# 2. é”™è¯¯æ—¥å¿—è¿‡æ»¤
docker-compose logs 2>&1 | grep -i error

# 3. æ€§èƒ½æ—¥å¿—åˆ†æ
docker-compose logs faster_whisper_service | grep "execution_time"
```

### æ€§èƒ½åˆ†æå·¥å…·

```python
# æ€§èƒ½åˆ†æè„šæœ¬
import time
import psutil
import torch

def analyze_performance():
    """åˆ†æç³»ç»Ÿæ€§èƒ½"""
    print("=== æ€§èƒ½åˆ†ææŠ¥å‘Š ===")
    print(f"CPU ä½¿ç”¨ç‡: {psutil.cpu_percent()}%")
    print(f"å†…å­˜ä½¿ç”¨ç‡: {psutil.virtual_memory().percent}%")

    if torch.cuda.is_available():
        print(f"GPU æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        print(f"GPU åˆ©ç”¨ç‡: {get_gpu_utilization()}%")

def get_gpu_utilization():
    """è·å– GPU åˆ©ç”¨ç‡"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
            capture_output=True, text=True
        )
        return float(result.stdout.strip())
    except:
        return 0.0
```

### å¥åº·æ£€æŸ¥å·¥å…·

```bash
#!/bin/bash
# å¥åº·æ£€æŸ¥è„šæœ¬

echo "=== ç³»ç»Ÿå¥åº·æ£€æŸ¥ ==="

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "1. æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
docker-compose ps

# æ£€æŸ¥ API å¥åº·åº¦
echo "2. æ£€æŸ¥ API å¥åº·åº¦..."
curl -f http://localhost:8788/health || echo "API ä¸å¥åº·"

# æ£€æŸ¥ Redis è¿æ¥
echo "3. æ£€æŸ¥ Redis è¿æ¥..."
docker-compose exec redis redis-cli ping || echo "Redis è¿æ¥å¤±è´¥"

# æ£€æŸ¥ GPU çŠ¶æ€
echo "4. æ£€æŸ¥ GPU çŠ¶æ€..."
nvidia-smi --query-gpu=utilization.gpu,name --format=csv,noheader,nounits

# æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "5. æ£€æŸ¥ç£ç›˜ç©ºé—´..."
df -h | grep -E "Filesystem|/dev/sda"
```

---

## è”ç³»æ”¯æŒ

### æ”¯æŒæ¸ é“

- **æŠ€æœ¯æ”¯æŒ**: support@yivideo.com
- **ç´§æ€¥è”ç³»**: +86-xxx-xxxx-xxxx
- **GitHub Issues**: https://github.com/yivideo/issues
- **æ–‡æ¡£ä¸­å¿ƒ**: https://docs.yivideo.com

### æŠ¥å‘Šé—®é¢˜æ—¶è¯·æä¾›

1. **ç³»ç»Ÿä¿¡æ¯**:
   ```bash
   uname -a
   docker --version
   docker-compose --version
   nvidia-smi
   ```

2. **æ—¥å¿—æ–‡ä»¶**:
   ```bash
   docker-compose logs > logs.txt
   ```

3. **é…ç½®æ–‡ä»¶**:
   ```bash
   cat config.yml
   ```

4. **é”™è¯¯æè¿°**:
   - é—®é¢˜æè¿°
   - å¤ç°æ­¥éª¤
   - æœŸæœ›ç»“æœ
   - å®é™…ç»“æœ

### å¸¸è§é—®é¢˜è§£ç­”

**Q: å¦‚ä½•æé«˜å¤„ç†é€Ÿåº¦ï¼Ÿ**
A: è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼Œå¯ç”¨ Faster-Whisperï¼Œä¼˜åŒ– GPU é…ç½®ã€‚

**Q: å¦‚ä½•å¤„ç† GPU å†…å­˜ä¸è¶³ï¼Ÿ**
A: å‡å°æ‰¹å¤„ç†å¤§å°ï¼Œä½¿ç”¨åŠç²¾åº¦ï¼Œæ¸…ç†æ˜¾å­˜ç¼“å­˜ã€‚

**Q: å¦‚ä½•ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Ÿ**
A: ä½¿ç”¨ Prometheus + Grafanaï¼ŒæŸ¥çœ‹æ€§èƒ½ä»ªè¡¨æ¿ã€‚

**Q: å¦‚ä½•å¤‡ä»½ç³»ç»Ÿæ•°æ®ï¼Ÿ**
A: å®šæœŸå¤‡ä»½é…ç½®æ–‡ä»¶å’Œ Redis æ•°æ®ã€‚

---

## ASR ä¸è¯´è¯äººåˆ†ç¦»æœåŠ¡ Docker æ„å»ºæœ€ä½³å®è·µ

### ğŸ”§ æ„å»ºå‰æ£€æŸ¥

#### 1. ç¯å¢ƒéªŒè¯
```bash
# æ£€æŸ¥åŸºç¡€é•œåƒ
docker pull ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.1.1-gpu-cuda11.8-cudnn8.9

# éªŒè¯ç½‘ç»œè¿æ¥
curl -I https://huggingface.co
curl -I https://pypi.org/simple/faster-whisper
curl -I https://pypi.org/simple/pyannote-audio

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h /var/lib/docker
```

#### 2. ä¾èµ–ç‰ˆæœ¬ç¡®è®¤
```bash
# æ£€æŸ¥æœåŠ¡ä¾èµ–ç‰ˆæœ¬
pip show faster-whisper
pip show pyannote.audio

# ç¡®è®¤å…¼å®¹çš„ä¾èµ–ç‰ˆæœ¬
pip show faster-whisper ctranslate2
```

### ğŸ—ï¸ æ„å»ºè¿‡ç¨‹ä¼˜åŒ–

#### 1. åˆ†å±‚æ„å»ºç­–ç•¥
```dockerfile
# åŸºç¡€ç³»ç»Ÿå±‚
FROM ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.1.1-gpu-cuda11.8-cudnn8.9

# ç³»ç»Ÿä¾èµ–å±‚ (å˜åŒ–é¢‘ç‡ä½)
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg libsox-dev libsndfile1-dev curl wget git && \
    rm -rf /var/lib/apt/lists/*

# Python ä¾èµ–å±‚ (å˜åŒ–é¢‘ç‡ä¸­ç­‰)
COPY requirements.txt /tmp/
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# åº”ç”¨ä»£ç å±‚ (å˜åŒ–é¢‘ç‡é«˜)
COPY . /app/
```

#### 2. ç¼“å­˜ä¼˜åŒ–
```bash
# ä½¿ç”¨ --no-cache é‡æ–°æ„å»º
docker-compose build faster_whisper_service pyannote_audio_service --no-cache

# æˆ–è€…é€‰æ‹©æ€§æ¸…ç†ç¼“å­˜
docker builder prune -f
```

### ğŸ› å¸¸è§æ„å»ºé—®é¢˜è§£å†³


#### 2. ç½‘ç»œè¶…æ—¶
```dockerfile
# è®¾ç½®å›½å†…é•œåƒæº
RUN pip install --no-cache-dir -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com \
    faster-whisper pyannote.audio

# å¢åŠ è¶…æ—¶æ—¶é—´
RUN pip install --timeout 300 --retries 3 faster-whisper pyannote.audio
```

#### 3. æƒé™é—®é¢˜
```dockerfile
# ç¡®ä¿æ­£ç¡®çš„ç”¨æˆ·æƒé™
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app && \
    chmod -R 755 /app/.cache
```

### âœ… æ„å»ºéªŒè¯æ¸…å•

#### 1. åŠŸèƒ½éªŒè¯
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps faster_whisper_service pyannote_audio_service

# éªŒè¯ Celery Worker
docker exec faster_whisper_service celery -A app.celery_app inspect active
docker exec pyannote_audio_service celery -A app.celery_app inspect active

# æµ‹è¯•æ¨¡å‹åŠ è½½
docker exec faster_whisper_service python -c "from faster_whisper import WhisperModel; print('Import successful')"
docker exec pyannote_audio_service python -c "from pyannote.audio import Pipeline; print('Import successful')"
```

#### 2. é…ç½®éªŒè¯
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
docker exec faster_whisper_service env | grep "TRANSFORMERS"
docker exec pyannote_audio_service env | grep "HF_"

# æ£€æŸ¥ç¼“å­˜ç›®å½• (è·¯å¾„å–å†³äº config.yml)
# docker exec <service_name> ls -la /path/to/your/model/cache/
```

#### 3. æ€§èƒ½éªŒè¯
```bash
# è¿è¡Œç®€å•æµ‹è¯• (ç¤ºä¾‹)
# python scripts/test_service.py --service faster_whisper

# æ£€æŸ¥æ—¥å¿—
docker-compose logs --tail=50 faster_whisper_service

# ç›‘æ§èµ„æºä½¿ç”¨
docker stats faster_whisper_service
```

### ğŸš€ ç”Ÿäº§éƒ¨ç½²å»ºè®®

#### 1. é•œåƒç®¡ç†
```bash
# æ ‡è®°ç”Ÿäº§é•œåƒ
docker tag yivideo-faster_whisper_service:latest yivideo-faster_whisper_service:v2.0.1
docker tag yivideo-pyannote_audio_service:latest yivideo-pyannote_audio_service:v2.0.1

# æ¨é€åˆ°ç§æœ‰ä»“åº“
docker push registry.example.com/yivideo-faster_whisper_service:v2.0.1
docker push registry.example.com/yivideo-pyannote_audio_service:v2.0.1
```

#### 2. å¥åº·æ£€æŸ¥
```dockerfile
# åœ¨ Dockerfile ä¸­æ·»åŠ å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD celery -A app.tasks.celery_app inspect active || exit 1
```

#### 3. æ—¥å¿—ç®¡ç†
```yaml
# åœ¨ docker-compose.yml ä¸­é…ç½®æ—¥å¿—
logging:
  driver: "json-file"
  options:
    max-size: "100m"
    max-file: "5"
```

### ğŸ“Š æ€§èƒ½è°ƒä¼˜

#### 1. èµ„æºé™åˆ¶
```yaml
# docker-compose.yml ä¸­çš„èµ„æºé…ç½®
deploy:
  resources:
    limits:
      memory: 8G
      cpus: '4'
    reservations:
      memory: 4G
      cpus: '2'
```

#### 2. å­˜å‚¨ä¼˜åŒ–
```yaml
# ä½¿ç”¨ tmpfs æå‡ä¸´æ—¶æ–‡ä»¶æ€§èƒ½
tmpfs:
  - /tmp
```

#### 3. ç½‘ç»œä¼˜åŒ–
```yaml
# ä½¿ç”¨ä¸“ç”¨ç½‘ç»œ
networks:
  asr_net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## æ€»ç»“

æœ¬æ•…éšœæ’é™¤æŒ‡å—æä¾›äº† YiVideo ç³»ç»Ÿå¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆï¼Œä»¥åŠDockeræ„å»ºçš„æœ€ä½³å®è·µã€‚è¯·æŒ‰ç…§æœ¬æŒ‡å—çš„æ­¥éª¤è¿›è¡Œæ•…éšœæ’é™¤ï¼Œå¦‚ä»æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚

å®šæœŸè¿›è¡Œç³»ç»Ÿç»´æŠ¤å’Œæ€§èƒ½ç›‘æ§ï¼Œå¯ä»¥æœ‰æ•ˆé¢„é˜²é—®é¢˜çš„å‘ç”Ÿã€‚

---

# IndexTTS2 æ•…éšœæ’é™¤æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—æä¾› IndexTTS2 è¯­éŸ³åˆæˆæœåŠ¡å¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

## ç³»ç»ŸçŠ¶æ€æ£€æŸ¥

### åŸºç¡€çŠ¶æ€æ£€æŸ¥

```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps | grep indextts

# 2. æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
docker exec indextts_service python3 -c "
from services.workers.indextts_service.app import health_check
result = health_check()
print('å¥åº·æ£€æŸ¥ç»“æœ:', result)
"

# 3. æ£€æŸ¥ GPU çŠ¶æ€
docker exec indextts_service nvidia-smi

# 4. æ£€æŸ¥ Redis è¿æ¥
docker exec indextts_service python3 -c "
import redis
r = redis.Redis(host='redis', port=6379, db=2)
print('Redisè¿æ¥çŠ¶æ€:', r.ping())
"
```

### æ—¥å¿—æ£€æŸ¥

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
docker-compose logs -f indextts_service

# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯æ—¥å¿—
docker-compose logs --tail=100 indextts_service | grep -i error

# æŸ¥çœ‹GPUç›¸å…³æ—¥å¿—
docker-compose logs indextts_service | grep -i gpu
```

## å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### é”™è¯¯ç±»å‹1: å‚æ•°éªŒè¯é”™è¯¯

#### é”™è¯¯ä¿¡æ¯: "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"

**ç—‡çŠ¶**:
```
{'status': 'error', 'error': 'è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º', 'task_id': 'abc123'}
```

**åŸå› **: ç¼ºå°‘å¿…éœ€çš„ `text` å‚æ•°

**è§£å†³æ–¹æ¡ˆ**:
```json
{
  "workflow_config": {
    "text": "è¿™é‡Œæ˜¯è¦è½¬æ¢çš„æ–‡æœ¬å†…å®¹",
    // ... å…¶ä»–å‚æ•°
  }
}
```

#### é”™è¯¯ä¿¡æ¯: "è¾“å‡ºè·¯å¾„ä¸èƒ½ä¸ºç©º"

**ç—‡çŠ¶**:
```
{'status': 'error', 'error': 'è¾“å‡ºè·¯å¾„ä¸èƒ½ä¸ºç©º', 'task_id': 'abc123'}
```

**åŸå› **: ç¼ºå°‘å¿…éœ€çš„ `output_path` å‚æ•°

**è§£å†³æ–¹æ¡ˆ**:
```json
{
  "workflow_config": {
    "output_path": "/share/workflows/output/speech.wav",
    // ... å…¶ä»–å‚æ•°
  }
}
```

#### é”™è¯¯ä¿¡æ¯: "ç¼ºå°‘å¿…éœ€å‚æ•°: spk_audio_prompt"

**ç—‡çŠ¶**:
```
{'status': 'error', 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°: spk_audio_prompt (è¯´è¯äººå‚è€ƒéŸ³é¢‘)', 'hint': 'IndexTTS2æ˜¯åŸºäºå‚è€ƒéŸ³é¢‘çš„è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œå¿…é¡»æä¾›è¯´è¯äººå‚è€ƒéŸ³é¢‘'}
```

**åŸå› **: ç¼ºå°‘å¿…éœ€çš„è¯´è¯äººå‚è€ƒéŸ³é¢‘

**è§£å†³æ–¹æ¡ˆ**:
```json
{
  "workflow_config": {
    "spk_audio_prompt": "/share/reference/speaker_sample.wav",
    // ... å…¶ä»–å‚æ•°
  }
}
```

#### é”™è¯¯ä¿¡æ¯: "å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨"

**ç—‡çŠ¶**:
```
{'status': 'error', 'error': 'å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: /path/to/reference.wav'}
```

**åŸå› **: æŒ‡å®šçš„å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
docker exec indextts_service ls -la /path/to/reference.wav

# æ£€æŸ¥æ–‡ä»¶æƒé™
docker exec indextts_service stat /path/to/reference.wav

# æ£€æŸ¥æ–‡ä»¶æ ¼å¼
docker exec indextts_service file /path/to/reference.wav
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. ç¡®ä¿å‚è€ƒéŸ³é¢‘æ–‡ä»¶å­˜åœ¨
docker exec indextts_service mkdir -p /share/reference/
docker cp /path/to/your/reference.wav /share/reference/

# 2. éªŒè¯æ–‡ä»¶å¯è¯»
docker exec indextts_service chmod 644 /share/reference/reference.wav

# 3. æ›´æ–°å·¥ä½œæµé…ç½®ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
```

### é”™è¯¯ç±»å‹2: GPUç›¸å…³é”™è¯¯

#### é”™è¯¯ä¿¡æ¯: CUDA out of memory

**ç—‡çŠ¶**:
```
RuntimeError: CUDA out of memory
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
docker exec indextts_service nvidia-smi

# æ£€æŸ¥GPUè¿›ç¨‹
docker exec indextts_service ps aux | grep python

# æ£€æŸ¥æ¨¡å‹å¤§å°
docker exec indextts_service du -sh /models/indextts/
```

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# ç¼–è¾‘ config.yml
indextts_service:
  use_fp16: true                    # å¯ç”¨FP16èŠ‚çœæ˜¾å­˜
  use_deepspeed: false              # ç¦ç”¨DeepSpeed
  num_workers: 1                    # ä¿æŒå•å·¥ä½œè¿›ç¨‹
```

```bash
# é‡å¯æœåŠ¡
docker-compose restart indextts_service
```

#### é”™è¯¯ä¿¡æ¯: GPU not available

**ç—‡çŠ¶**:
```
RuntimeError: No CUDA GPUs are available
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥Docker GPUæ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu22.04 nvidia-smi

# æ£€æŸ¥å®¹å™¨GPUæŒ‚è½½
docker inspect indextts_service | grep -A 10 -B 10 DeviceRequests
```

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# docker-compose.yml ä¸­ç¡®ä¿GPUæŒ‚è½½æ­£ç¡®
indextts_service:
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

### é”™è¯¯ç±»å‹3: æ–‡ä»¶ç³»ç»Ÿé”™è¯¯

#### é”™è¯¯ä¿¡æ¯: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•

**ç—‡çŠ¶**:
```
{'status': 'error', 'error': 'æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½• /share/output: Permission denied'}
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥ç›®å½•æƒé™
docker exec indextts_service ls -la /share/

# æ£€æŸ¥ç£ç›˜ç©ºé—´
docker exec indextts_service df -h

# å°è¯•æ‰‹åŠ¨åˆ›å»ºç›®å½•
docker exec indextts_service mkdir -p /share/workflows/output/test
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä¿®å¤ç›®å½•æƒé™
docker exec indextts_service chmod -R 755 /share/workflows/

# ç¡®ä¿ç›®å½•å¯å†™
docker exec indextts_service touch /share/workflows/output/.write_test
```

#### é”™è¯¯ä¿¡æ¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨

**ç—‡çŠ¶**:
```
FileNotFoundError: [Errno 2] No such file or directory: '/models/indextts/checkpoints/config.yaml'
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥æ¨¡å‹ç›®å½•
docker exec indextts_service ls -la /models/indextts/

# æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•
docker exec indextts_service ls -la /models/indextts/checkpoints/

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
docker exec indextts_service find /models/indextts/ -name "*.yaml" -o -name "*.pt" | head -10
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡æ–°ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
# è¿™é€šå¸¸éœ€è¦é‡æ–°æ„å»ºå®¹å™¨æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶

# æ£€æŸ¥Dockerå·æŒ‚è½½
docker volume ls | grep indextts
docker volume inspect yivideo_indextts_models_volume
```

### é”™è¯¯ç±»å‹4: ç½‘ç»œå’ŒæœåŠ¡é”™è¯¯

#### é”™è¯¯ä¿¡æ¯: Redisè¿æ¥å¤±è´¥

**ç—‡çŠ¶**:
```
ConnectionError: Error 111 connecting to redis:6379
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥RedisæœåŠ¡çŠ¶æ€
docker-compose ps | grep redis

# æµ‹è¯•Redisè¿æ¥
docker exec redis redis-cli ping

# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker exec indextts_service ping redis
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡å¯RedisæœåŠ¡
docker-compose restart redis

# æ£€æŸ¥ç½‘ç»œé…ç½®
docker network ls | grep wionch
```

#### é”™è¯¯ä¿¡æ¯: ä»»åŠ¡è¶…æ—¶

**ç—‡çŠ¶**:
```
TaskTimeoutError: Task indextts.generate_speech timed out after 1800 seconds
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
docker exec indextts_service python3 -c "
from services.common.locks import SmartGpuLockManager
manager = SmartGpuLockManager()
print('å½“å‰é”çŠ¶æ€:', manager.get_lock_info())
"

# æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
docker exec indextts_service nvidia-smi
```

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# å¢åŠ è¶…æ—¶æ—¶é—´
# åœ¨ celery_app.conf.update ä¸­è°ƒæ•´
task_soft_time_limit: 3600  # 60åˆ†é’Ÿè½¯è¶…æ—¶
task_time_limit: 4200       # 70åˆ†é’Ÿç¡¬è¶…æ—¶
```

## æ€§èƒ½é—®é¢˜è¯Šæ–­

### å¤„ç†é€Ÿåº¦æ…¢

**ç—‡çŠ¶**: ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿‡é•¿

**è¯Šæ–­æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥GPUåˆ©ç”¨ç‡
watch -n 2 "docker exec indextts_service nvidia-smi"

# 2. æ£€æŸ¥ç³»ç»Ÿèµ„æº
docker stats indextts_service

# 3. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker-compose logs indextts_service | grep -E "(å¤„ç†æ—¶é—´|GPU|memory)"
```

**ä¼˜åŒ–å»ºè®®**:
```yaml
indextts_service:
  use_fp16: true                    # å¯ç”¨FP16
  max_text_tokens_per_segment: 80  # å‡å°‘åˆ†æ®µé•¿åº¦
  enable_monitoring: true            # å¯ç”¨ç›‘æ§
```

### å†…å­˜ä½¿ç”¨è¿‡é«˜

**ç—‡çŠ¶**: å†…å­˜å ç”¨æŒç»­å¢é•¿

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
docker stats indextts_service --no-stream

# æ£€æŸ¥Pythonè¿›ç¨‹å†…å­˜
docker exec indextts_service python3 -c "
import psutil
p = psutil.Process()
print(f'å†…å­˜ä½¿ç”¨: {p.memory_info().rss / 1024 / 1024:.2f} MB')
"

# æ£€æŸ¥GPUå†…å­˜
docker exec indextts_service nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨ä»»åŠ¡å®Œæˆåå¼ºåˆ¶æ¸…ç†GPUå†…å­˜
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

## é…ç½®é—®é¢˜

### é…ç½®æ–‡ä»¶ä¸ç”Ÿæ•ˆ

**ç—‡çŠ¶**: ä¿®æ”¹é…ç½®åæ²¡æœ‰ç”Ÿæ•ˆ

**è¯Šæ–­æ­¥éª¤**:
```bash
# 1. éªŒè¯é…ç½®æ–‡ä»¶è¯­æ³•
python3 -c "
import yaml
with open('/app/config.yml', 'r') as f:
    config = yaml.safe_load(f)
    print('indextts_serviceé…ç½®:', config.get('indextts_service', 'NOT_FOUND'))
"

# 2. æ£€æŸ¥æœåŠ¡æ˜¯å¦åŠ è½½äº†æ–°é…ç½®
docker-compose logs indextts_service | grep -E "(é…ç½®|config|Configuration)"

# 3. é‡å¯æœåŠ¡
docker-compose restart indextts_service
```

### ç¯å¢ƒå˜é‡å†²çª

**ç—‡çŠ¶**: ç¯å¢ƒå˜é‡è®¾ç½®ä¸æ­£ç¡®

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
docker exec indextts_service env | grep INDEX_TTS

# æ£€æŸ¥é…ç½®ä¼˜å…ˆçº§
docker exec indextts_service python3 -c "
from services.common.config_loader import get_config
config = get_config()
print('use_fp16:', config.get('indextts_service', {}).get('use_fp16'))
print('ç¯å¢ƒå˜é‡use_fp16:', config.get('use_fp16', 'NOT_SET'))
"
```

## æ—¥å¿—åˆ†æ

### å¸¸ç”¨æ—¥å¿—å‘½ä»¤

```bash
# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
docker-compose logs --tail=50 indextts_service | grep -i error

# æŸ¥çœ‹GPUç›¸å…³æ—¥å¿—
docker-compose logs indextts_service | grep -i gpu

# æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œæ—¥å¿—
docker-compose logs indextts_service | grep "å¼€å§‹æ‰§è¡Œ\|å®Œæˆ\|å¤±è´¥"

# æŸ¥çœ‹æ€§èƒ½ç›¸å…³æ—¥å¿—
docker-compose logs indextts_service | grep -E "(æ—¶é—´|ç§’|åˆ†é’Ÿ|æ€§èƒ½)"
```

### æ—¥å¿—åˆ†ææŠ€å·§

1. **æ—¶é—´æˆ³åŒ¹é…**: æ‰¾åˆ°ç‰¹å®šä»»åŠ¡çš„æ—¶é—´æˆ³
2. **é”™è¯¯æ¨¡å¼**: è¯†åˆ«é‡å¤å‡ºç°çš„é”™è¯¯æ¨¡å¼
3. **æ€§èƒ½æŒ‡æ ‡**: æå–å¤„ç†æ—¶é—´ã€å†…å­˜ä½¿ç”¨ç­‰æŒ‡æ ‡

## æ¢å¤æ“ä½œ

### æœåŠ¡é‡å¯

```bash
# è½¯é‡å¯
docker-compose restart indextts_service

# å¼ºåˆ¶é‡å¯
docker-compose stop indextts_service
docker-compose start indextts_service

# é‡å»ºå®¹å™¨
docker-compose up -d --force-recreate indextts_service
```

### æ•°æ®æ¢å¤

```bash
# å¤‡ä»½é‡è¦æ•°æ®
docker exec indextts_service tar -czf /share/backup/indextts_backup_$(date +%Y%m%d).tar.gz /share/workflows/ /share/reference/

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
docker exec indextts_service find /tmp -name "*.tmp" -delete
docker exec indextts_service find /share/tmp -name "*.wav" -delete
```

### é‡ç½®é…ç½®

```bash
# æ¢å¤é»˜è®¤é…ç½®
docker exec indextts_service cp /app/config.yml.backup /app/config.yml

# é‡æ–°åŠ è½½é…ç½®
docker-compose restart indextts_service
```

## ç›‘æ§å’Œé¢„é˜²

### å®šæœŸæ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# health_check.sh

echo "=== IndexTTS2 å¥åº·æ£€æŸ¥ ==="

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
if docker-compose ps | grep indextts_service | grep -q "Up"; then
    echo "âœ… IndexTTS2 æœåŠ¡è¿è¡Œæ­£å¸¸"
else
    echo "âŒ IndexTTS2 æœåŠ¡æœªè¿è¡Œ"
    exit 1
fi

# æ£€æŸ¥GPUçŠ¶æ€
if docker exec indextts_service nvidia-smi &>/dev/null; then
    echo "âœ… GPU å¯ç”¨"
else
    echo "âŒ GPU ä¸å¯ç”¨"
    exit 1
fi

# æ£€æŸ¥Redisè¿æ¥
if docker exec indextts_service python3 -c "import redis; r=redis.Redis(host='redis', port=6379, db=2); r.ping()" &>/dev/null; then
    echo "âœ… Redis è¿æ¥æ­£å¸¸"
else
    echo "âŒ Redis è¿æ¥å¤±è´¥"
    exit 1
fi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
disk_usage=$(docker exec indextts_service df /share | awk 'NR==2 {print $5}')
if [[ ${disk_usage%?} -lt 90 ]]; then
    echo "âœ… ç£ç›˜ç©ºé—´å……è¶³ (${disk_usage})"
else
    echo "âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ (${disk_usage})"
fi

echo "=== å¥åº·æ£€æŸ¥å®Œæˆ ==="
```

### æ€§èƒ½ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# performance_monitor.sh

echo "=== IndexTTS2 æ€§èƒ½ç›‘æ§ ==="

# GPUä½¿ç”¨ç‡
echo "GPUä½¿ç”¨ç‡:"
docker exec indextts_service nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits

# å†…å­˜ä½¿ç”¨
echo "å†…å­˜ä½¿ç”¨:"
docker stats indextts_service --no-stream | awk 'NR==2 {print $4}'} | sed 's/MiB/MB/'

# æ´»è·ƒä»»åŠ¡æ•°
echo "æ´»è·ƒä»»åŠ¡æ•°:"
docker exec indextts_service python3 -c "
from services.common.locks import SmartGpuLockManager
try:
    manager = SmartGpuLockManager()
    status = manager.get_lock_info()
    if status:
        print('GPUè¢«å ç”¨')
    else:
        print('GPUç©ºé—²')
except:
    print('æ— æ³•è·å–GPUçŠ¶æ€')
"

echo "=== æ€§èƒ½ç›‘æ§å®Œæˆ ==="
```

## è”ç³»æ”¯æŒ

å¦‚æœä»¥ä¸Šè§£å†³æ–¹æ¡ˆéƒ½æ— æ³•è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·ï¼š

1. **æ”¶é›†æ—¥å¿—**:
   ```bash
   docker-compose logs indextts_service > indextts_debug.log 2>&1
   ```

2. **æ”¶é›†ç³»ç»Ÿä¿¡æ¯**:
   ```bash
   docker exec indextts_service python3 -c "
   import torch
   import sys
   print('Pythonç‰ˆæœ¬:', sys.version)
   print('PyTorchç‰ˆæœ¬:', torch.__version__)
   print('CUDAç‰ˆæœ¬:', torch.version.cuda)
   print('GPUæ•°é‡:', torch.cuda.device_count())
   if torch.cuda.is_available():
       print('GPUåç§°:', torch.cuda.get_device_name(0))
   "
   ```

3. **æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯**:
   - å®Œæ•´çš„é”™è¯¯å †æ ˆ
   - é‡ç°æ­¥éª¤
   - ä½¿ç”¨çš„é…ç½®å’Œå‚æ•°

å°†è¿™äº›ä¿¡æ¯æäº¤ç»™æŠ€æœ¯æ”¯æŒå›¢é˜Ÿä»¥è·å¾—è¿›ä¸€æ­¥çš„å¸®åŠ©ã€‚