# S2ST 工作流调研发现

## 文档更新日志
| 日期 | 更新内容 | 更新人 |
|------|---------|--------|
| 2026-01-17 | 初始化文档 | Claude |
| 2026-01-17 | 完成 Phase 0 所有调研内容 | Claude |

## 1. 现有代码分析

### 1.1 转录数据结构
**位置**: `share/workflows/video_to_subtitle_task/nodes/faster_whisper.transcribe_audio/data/transcribe_data_video_to.json`

**数据结构**:
```json
{
  "metadata": {
    "task_name": "faster_whisper.transcribe_audio",
    "workflow_id": "video_to_subtitle_task",
    "audio_file": "223_(Vocals)_htdemucs.flac",
    "total_duration": 341.79775,
    "language": "en",
    "word_timestamps_enabled": true,
    "model_name": "Systran/faster-whisper-large-v3",
    "device": "cuda"
  },
  "segments": [
    {
      "id": 1,
      "seek": 0,
      "start": 11.4,
      "end": 19.56,
      "text": "Well, little kitty, if you really want to learn how to catch flies, you've got to study",
      "tokens": [...],
      "avg_logprob": -0.36,
      "compression_ratio": 1.32,
      "no_speech_prob": 0.14,
      "words": [
        {
          "word": " Well,",
          "start": 11.4,
          "end": 12.24,
          "probability": 0.64
        },
        ...
      ]
    }
  ]
}
```

**关键字段**:
- `id`: segment 唯一标识
- `start/end`: 时间区间（秒）
- `text`: 转录文本
- `words`: 词级时间戳（用于精确对齐）

### 1.2 YiVideo 架构要点
**位置**: `CLAUDE.md`

**核心约束**:
1. 所有节点必须继承 `BaseNodeExecutor`
2. 实现三个核心方法：`validate_input()`, `execute_core_logic()`, `get_cache_key_fields()`
3. 统一任务签名：`task(self: Task, context: dict) -> dict`
4. GPU 任务使用 `@gpu_lock()` 装饰器
5. 遵循 KISS/DRY/YAGNI/SOLID 原则

**文件路径约定**:
- 输入：MinIO URL 或本地路径
- 临时文件：`/app/tmp/<task_id>/`
- 输出：自动上传到 MinIO 并生成 `*_minio_url` 字段

## 2. 行业标准调研

### 2.1 字幕时长与字数标准
**调研日期**: 2026-01-17

#### 通用标准（2025 行业共识）

**CPS (Characters Per Second) 标准**:
- **最佳范围**: 15-17 CPS（舒适阅读速度）
- **最大值**: 20-21 CPS（一般标准），某些平台允许 23 CPS
- **儿童内容**: 13-15 CPS（需要更慢的阅读速度）

**时长标准**:
- **最短时长**: 1 秒（极短文本可放宽至 0.83-1 秒）
- **最长时长**: 6-7 秒（单条字幕）
- **最佳范围**: 1.5-6 秒
- **字幕间隙**: 最小 2-4 帧（0.08-0.16 秒）

**每行字符限制**:
- **英文**: 每行最多 37-42 字符
- **双行字幕**: 总计 74-84 字符
- **Netflix 标准**: 每行最多 42 字符

**行数标准**:
- **标准**: 最多 2 行
- **SDH/闭合字幕**: 可允许 3 行

#### 中文字幕特殊标准

**字符限制**:
- **每行最多**: 13-15 个中文字符
- **CPS 计算**: 由于中文字符密度高，需要单独计算

**时长要求**:
- 与通用标准一致：1-7 秒
- 最佳范围：2-5 秒

#### 其他约束
- **镜头切换**: 字幕应尽量避免跨越镜头切换
- **换行原则**: 应遵循自然语言单元（短语、从句）

**调研来源**: Web Search 2025 行业标准

### 2.2 翻译装词与 S2ST 标准
**调研日期**: 2026-01-17

#### S2ST 核心挑战
**翻译装词 (Translation Fitting)** 是 S2ST 行业的核心难题，目标是在保证翻译准确性的同时，实现：
1. **时长对齐**: 译文语音时长与原文相近
2. **音画同步**: 避免口型不匹配和时间偏移累积
3. **自然度**: 保持翻译文本的流畅性和自然度

#### 关键技术因素

**时长一致性影响因素**:
1. **字数/音节数**: 不同语言的字数比例差异大
2. **语速**: 目标语言的正常语速
3. **音素密度**: 不同语言的音素发音时长差异
4. **韵律**: 重音、停顿、语调变化

**主流解决方案** (2024-2025):
1. **等时翻译 (Isochronous Translation)**:
   - 保持源语言和目标语言时长相似
   - 通过调整用词和句式实现时长匹配

2. **视觉感知配音 (Visually-Aware Dubbing)**:
   - 结合唇形同步算法
   - 调整翻译以匹配口型运动

3. **神经配音系统**:
   - 使用 AI 自动调整翻译长度
   - Meta, Google, Microsoft 等公司的商业方案

#### 质量评估指标
1. **时长偏差**: 目标 ±10% 以内
2. **翻译准确性**: BLEU/COMET 分数
3. **自然度评分**: 人工评估 MOS (Mean Opinion Score)
4. **口型同步度**: 视觉对齐分数

**调研来源**: Web Search S2ST 技术调研 2024-2025

## 3. 技术方案调研

### 3.1 Edge-TTS SSML 功能
**项目**: https://github.com/rany2/edge-tts
**调研日期**: 2026-01-17

#### ⚠️ 重要限制：不支持自定义 SSML

**核心发现**: Edge-TTS **明确移除了自定义 SSML 支持**，原因是：
> "Microsoft prevents the use of any SSML that could not be generated by Microsoft Edge itself"

**实际可用的 SSML 控制**:
- 仅支持**单个** `<voice>` 标签
- 仅支持**单个** `<prosody>` 标签（嵌套在 voice 内）
- 所有 prosody 可用选项已通过库/CLI 参数暴露

#### Rate 参数控制

**语法**:
```bash
edge-tts --rate=-50% --text "Hello, world!" --write-media output.mp3
```

**关键点**:
- 负值必须使用 `--rate=-50%` 格式（不能用空格）
- Rate 调整直接影响语音时长
- 可与 volume、pitch 同时使用

**精度**:
- 支持百分比调整（如 -50%, +20%）
- 具体精度范围未明确说明（需实测）

#### 支持的语言和音色

**语言覆盖**: 广泛多语言支持，包括：
- Afrikaans, Amharic, Arabic（多方言）
- 中文、英文、法文、德文等主流语言
- 完整列表：`edge-tts --list-voices`

**音色属性**:
- 性别：Male/Female
- 内容类别分类
- 语音特征：通常为 "Friendly, Positive"

#### API 使用

**命令行**:
```bash
edge-tts --text "Hello" --write-media hello.mp3 --write-subtitles hello.srt
```

**Python 模块**:
- 示例代码在 `/examples/` 目录
- 实现参考：`/src/edge_tts/util.py`

#### 成本与限制

**优势**:
- **无需 API 密钥**
- 免费使用

**限制**:
1. 依赖 Microsoft Edge 在线服务（需网络连接）
2. 单一 voice/prosody 限制
3. 无法使用复杂的自定义 SSML
4. 播放需要 mpv（除 Windows 外）

#### 时长控制策略结论

**对 S2ST 的影响**:
- ✅ 可以通过 `--rate` 调整语速来控制时长
- ❌ 无法使用 SSML `<prosody duration="3s">` 精确指定时长
- **建议**: 使用 rate 参数预调整 + ffmpeg rubberband 后置微调

**调研来源**: https://github.com/rany2/edge-tts (WebFetch 2026-01-17)

### 3.2 IndexTTS2 集成方案
**项目**: https://github.com/index-tts/index-tts
**调研日期**: 2026-01-17

#### 参考音 (Reference Audio) 机制

**格式要求**:
- **音频格式**: WAV 文件（示例使用 `.wav`）
- **时长**: 文档未明确说明最小/最大时长
- **使用方式**: 通过 `spk_audio_prompt` 参数指定单个参考音文件

**示例**:
```python
tts.infer(spk_audio_prompt='examples/voice_01.wav', text="...", output_path="gen.wav")
```

#### 音色与情感分离控制

**核心特性**: IndexTTS2 支持**音色克隆**和**情感控制**分离：

1. **音色克隆** (`spk_audio_prompt`):
   - 克隆说话人身份特征
   - 使用原始说话人的音频片段

2. **情感控制** (三种方式):

   **方式 1: 音频参考** (`emo_audio_prompt`):
   ```python
   emo_audio_prompt='examples/emotion_happy.wav'
   ```

   **方式 2: 向量控制** (8 维情感向量):
   ```python
   emo_vector=[happy, angry, sad, afraid, disgusted, melancholic, surprised, calm]
   # 8个浮点数，范围未说明
   ```

   **方式 3: 文本描述** (`use_emo_text=True`):
   ```python
   use_emo_text=True  # 将文本描述转换为情感向量
   emo_alpha=0.6      # 建议 ≤0.6，控制情感强度 (0.0-1.0)
   ```

3. **情感强度调节** (`emo_alpha`):
   - 范围: 0.0-1.0
   - 文本模式建议: ≤0.6
   - 控制情感表达的强弱

#### SSML 支持

**结论**: **不支持 SSML**
- 文档未提及 SSML 功能
- 仅支持拼音标注（用于中文发音控制）

#### 安装与依赖

**包管理器**: 必须使用 `uv`（不支持 conda/pip）
- 声称比 pip 快 115 倍

**GPU 要求**:
- NVIDIA CUDA Toolkit 12.8+
- 支持 FP16 推理（降低显存占用）
- 提供编译 CUDA 内核以提升速度
- GPU 检测工具: `uv run tools/gpu_check.py`

**安装步骤**:
```bash
# 1. 安装依赖
uv sync --all-extras

# 2. 下载模型
hf download IndexTeam/IndexTTS-2 --local-dir=checkpoints

# 3. (可选) WebUI
# 4. (可选) DeepSpeed 加速
```

#### API 使用示例

```python
from indextts.infer_v2 import IndexTTS2

tts = IndexTTS2(cfg_path="checkpoints/config.yaml", model_dir="checkpoints")

tts.infer(
    spk_audio_prompt='examples/voice_01.wav',  # 音色参考
    emo_audio_prompt='examples/emotion.wav',   # (可选) 情感参考
    text="Your text here",
    output_path="gen.wav"
)
```

#### 时长对齐策略

**对 S2ST 的影响**:
- ❌ **不支持 SSML**，无法直接指定时长
- ❌ **无法预测生成时长**，必须后置对齐
- ✅ **建议方案**: 生成语音后使用 ffmpeg rubberband 调整时长

**参考音策略建议**:
1. 从原始音频提取对应时间区间的片段作为参考音
2. 如果参考音时长不足，通过**重复拼接**调整到所需时长
3. 同一说话人必须使用同一说话人的参考音

#### 其他特性
- **中文拼音标注**: 支持精确控制发音
- **WebUI**: 提供可视化界面
- **DeepSpeed 加速**: 可选加速推理

**调研来源**: https://github.com/index-tts/index-tts (WebFetch 2026-01-17)

### 3.3 语音时长对齐方案
**调研日期**: 2026-01-17

#### 行业主流方案分类

**方案 1: SSML/TTS 原生控制**
- **适用**: 支持 SSML duration 的 TTS 系统
- **优势**: 直接生成目标时长，音质最佳
- **劣势**: IndexTTS2 和 Edge-TTS 均不支持
- **结论**: **不适用于本项目**

**方案 2: 后处理时间拉伸**
- **工具**: ffmpeg rubberband, WORLD vocoder, PSOLA
- **适用**: 所有 TTS 系统
- **优势**: 通用性强，保持音高
- **劣势**: 极端拉伸会损失音质
- **结论**: **本项目主要方案**

**方案 3: 混合方案**
- **策略**: TTS rate 预调整 + 后处理微调
- **适用**: 支持 rate 参数的 TTS（如 Edge-TTS）
- **优势**: 减少后处理幅度，保持音质
- **结论**: **Edge-TTS 推荐方案**

#### 现代 TTS 模型的时长控制

**FastSpeech/FastSpeech2**:
- 使用 Duration Predictor 显式控制时长
- 非自回归模型，速度快

**Glow-TTS, VITS**:
- 基于流的模型，支持时长控制
- 需要模型原生支持

**结论**: IndexTTS2 和 Edge-TTS 均不属于此类模型，需后处理

#### 时长对齐技术细节

**Dynamic Time Warping (DTW)**:
- 用于序列对齐
- 适用于语音识别后对齐
- 不适用于 TTS 生成

**Forced Alignment**:
- 使用声学模型对齐音素
- 适用于字幕生成
- 不适用于时长控制

**Attention Mechanisms**:
- Tacotron 等模型内部机制
- 需要模型原生支持

#### 推荐方案总结

| TTS 系统 | 预处理策略 | 后处理策略 | 时长容差 |
|---------|----------|----------|---------|
| **Edge-TTS** | rate 参数调整 | rubberband 微调 | ±5-10% |
| **IndexTTS2** | 无 | rubberband 完整对齐 | ±10-15% |

**时长容差说明**:
- 理想目标: ±5% 以内
- 可接受范围: ±10%
- 极限容忍: ±15%（累积误差可能影响同步）

**调研来源**: Web Search TTS Duration Alignment 2024-2025

### 3.4 FFmpeg Rubberband 参数优化
**调研日期**: 2026-01-17

#### Rubberband 核心功能

**特性**: 高质量时间拉伸和音高变换
- 保持音高的同时改变速度（或反之）
- 基于 Rubberband 库实现

#### 关键参数详解

**1. tempo（速度拉伸）**
```bash
ffmpeg -i input.mp3 -af "rubberband=tempo=<factor>" output.mp3
```
- **值范围**: 0.5 = 半速，2.0 = 双速
- **默认**: 1.0（不变）
- **推荐**: 0.8-1.2 范围内音质最佳

**2. pitch（音高变换）**
- 单位: 半音 (semitones)
- 正值 = 升调，负值 = 降调
- 可与 tempo 组合

**3. transients（瞬态处理）**
- **crisp**: 适合打击乐（清晰瞬态）
- **mixed**: 通用平衡（默认推荐）
- **smooth**: 适合持续音符（人声、弦乐）
- **语音推荐**: `smooth`

**4. detector（相位检测）**
- **compound**: 综合模式（默认）
- **percussive**: 打击乐模式
- **soft**: 柔和模式
- **语音推荐**: `compound`

**5. window（处理窗口）**
- **standard**: 标准平衡（默认）
- **short**: 时间分辨率优先（电子音乐）
- **long**: 频率精度优先（古典音乐、人声）
- **语音推荐**: `long`（更好的音质）

**6. formant（共振峰保持）**
- **shifted**: 随音高移动（默认）
- **preserved**: 保持原共振峰（**人声必用**）
- **语音推荐**: `preserved`（保持自然音色）

**7. pitchq（音质模式）**
- **quality**: 最佳质量，较慢
- **speed**: 更快处理，可接受质量
- **consistency**: 不同速度下最一致
- **语音推荐**: `quality`

**8. smoothing（频率平滑）**
- **on**: 启用
- **off**: 禁用（默认）

**9. channels（通道处理）**
- **apart**: 独立处理各通道（默认）
- **together**: 联合处理

#### 针对 S2ST 的最佳实践

**人声时长对齐优化配置**:
```bash
ffmpeg -i vocal.wav -af "rubberband=tempo=0.95:transients=smooth:formant=preserved:pitchq=quality:window=long" output.wav
```

**参数解释**:
- `tempo=0.95`: 轻微减速（5%），保持接近原时长
- `transients=smooth`: 人声平滑处理
- `formant=preserved`: 保持声音特征（**关键**）
- `pitchq=quality`: 最佳质量
- `window=long`: 更好的频率精度

**时长调整范围建议**:
- **最佳**: `tempo=0.9-1.1`（±10%）
- **可接受**: `tempo=0.8-1.2`（±20%）
- **极限**: `tempo=0.7-1.3`（±30%，音质明显下降）
- **超出范围**: 考虑多次处理或重新生成

#### 进阶优化策略

**1. 分段处理**:
- 长音频分段后逐段对齐
- 避免累积误差

**2. 多次小幅调整**:
```bash
# 如果需要 tempo=0.7（30% 减速）
# 分两次：0.85 × 0.85 ≈ 0.72
ffmpeg -i input.wav -af "rubberband=tempo=0.85:..." temp.wav
ffmpeg -i temp.wav -af "rubberband=tempo=0.85:..." output.wav
```

**3. 根据内容类型选择参数**:
- **纯人声**: `transients=smooth, formant=preserved`
- **人声+背景音乐**: `transients=mixed, formant=preserved`
- **纯音乐**: `transients=mixed, window=long`

#### 性能 vs 质量权衡

**快速处理**（实时场景）:
```bash
rubberband=tempo=X:pitchq=speed:window=standard
```

**高质量处理**（离线处理）:
```bash
rubberband=tempo=X:pitchq=quality:window=long:formant=preserved:transients=smooth
```

#### 音质损失评估

**可听感影响**:
- **tempo=0.9-1.1**: 几乎无感知差异
- **tempo=0.8-1.2**: 轻微可察觉，普通用户难以注意
- **tempo=0.7-1.3**: 明显可察觉，专业听众容易发现
- **tempo<0.7 或 >1.3**: 显著音质下降，不推荐

**建议**:
- 优先优化翻译装词，减少时长偏差
- rubberband 仅作为最后的微调手段
- 始终使用无损格式（WAV/FLAC）处理，避免二次编码损失

**调研来源**: Web Search FFmpeg Rubberband 2025 最佳实践

## 4. 指令集设计

### 4.1 极简指令集规范 (v1.0)
**设计日期**: 2026-01-17
**设计原则**:
1. 单字符键名，最小化 token 消耗
2. 基于词索引操作，保留时间戳信息
3. 本地可精确重构，无歧义
4. 支持批量操作

#### 操作类型定义

所有操作使用统一的 JSON 对象格式，通过 `"t"` 字段区分类型：

**1. 移动操作 (Move) - 断句优化**

用于将一个 segment 的部分词移动到另一个 segment，修正断句错误。

```json
{
  "t": "m",
  "i": 1,
  "f": 10,
  "e": 12,
  "to": 2
}
```

**字段说明**:
- `t`: type = "m" (move)
- `i`: segment ID (源 segment)
- `f`: from (起始词索引，包含)
- `e`: end (结束词索引，不包含)
- `to`: 目标 segment ID

**本地处理逻辑**:
1. 从 segment 1 的 `words` 数组提取 `words[10:12]`
2. 移动到 segment 2 的开头或结尾（取决于 to 与 i 的关系）
3. 重新计算 segment 1 和 2 的 `start`/`end` 时间
4. 重新生成 `text` 字段
5. 保留词级时间戳

**2. 替换操作 (Replace) - 纠错**

用于替换、删除或插入词，修正错别字和语义错误。

```json
{
  "t": "r",
  "i": 1,
  "f": 5,
  "e": 6,
  "w": "correct"
}
```

**字段说明**:
- `t`: type = "r" (replace)
- `i`: segment ID
- `f`: from (起始词索引)
- `e`: end (结束词索引，不包含)
- `w`: new words (新文本)

**本地处理逻辑**:
1. 替换 `words[f:e]` 为新文本
2. 对于新文本，使用原词的时间戳估算新词的时间戳
3. 重新生成 `text` 字段
4. 保持 segment 的 `start`/`end` 不变

**特殊用法**:
- 删除：`"w": ""`（删除 words[f:e]）
- 插入：`f == e`（在位置 f 插入）
- 单词纠错：`e = f + 1`（替换单个词）

**3. 合并操作 (Merge) - 合并字幕**

用于将多个短 segment 合并为一个，满足最小时长要求。

```json
{
  "t": "g",
  "i": [1, 2, 3]
}
```

**字段说明**:
- `t`: type = "g" (group/merge)
- `i`: segment IDs 数组（按顺序）

**本地处理逻辑**:
1. 合并所有 segment 的 `words` 数组
2. 新 segment 的 `start` = 第一个 segment 的 `start`
3. 新 segment 的 `end` = 最后一个 segment 的 `end`
4. 重新生成 `text`
5. 删除其他 segments
6. 新 segment 保留第一个 ID

**4. 拆分操作 (Split) - 拆分字幕**

用于将长 segment 拆分为多个，满足最大时长和 CPS 要求。

```json
{
  "t": "s",
  "i": 1,
  "p": [5, 10]
}
```

**字段说明**:
- `t`: type = "s" (split)
- `i`: segment ID
- `p`: positions (拆分点，词索引数组)

**本地处理逻辑**:
1. 在词索引 5 和 10 处拆分
2. 生成 3 个新 segments:
   - Segment A: `words[0:5]`
   - Segment B: `words[5:10]`
   - Segment C: `words[10:]`
3. 每个新 segment 重新计算 `start`/`end`
4. 重新生成 `text`
5. 处理完所有操作后统一重新编号

#### 指令集示例

**完整操作序列**:
```json
[
  {"t":"r", "i":1, "f":5, "e":6, "w":"kitty"},
  {"t":"m", "i":2, "f":0, "e":2, "to":1},
  {"t":"g", "i":[3, 4]},
  {"t":"s", "i":5, "p":[8]}
]
```

**执行顺序**: 从前到后依次执行，ID 变化后续操作自动适配。

#### 提交给 LLM 的数据格式

为节省 token，只提交必要字段：

```json
{
  "segs": [
    {
      "i": 1,
      "t": "Well, little kitty, if you really want to learn",
      "s": 11.4,
      "e": 19.56,
      "d": 8.16,
      "wc": 9,
      "cps": 16.5
    }
  ],
  "std": {
    "min_d": 1.5,
    "max_d": 6,
    "opt_cps": [15, 17],
    "max_cps": 21
  }
}
```

**字段说明**:
- `i`: segment ID
- `t`: text（仅供 LLM 理解语义，不用于重构）
- `s`: start 时间
- `e`: end 时间
- `d`: duration（= e - s，方便 LLM 判断）
- `wc`: word count（词数）
- `cps`: characters per second（当前 CPS）
- `std`: 行业标准（供 LLM 参考）

**不提交的数据**:
- `words` 数组（本地已有，无需传输）
- `tokens`, `avg_logprob`, `compression_ratio` 等（无关）

#### 关键设计决策

**决策 1: 使用词索引而非字符位置**
- **原因**: 保留词级时间戳信息，重构时更精确
- **优势**: 可以自动重新计算 segment 的 start/end
- **劣势**: LLM 需要理解词索引概念

**决策 2: 单字符键名**
- **原因**: 最小化 JSON token 消耗
- **token 节省**: 约 40-60% (相比 "type", "segment_id" 等全名)

**决策 3: 处理后统一重新编号**
- **原因**: 避免复杂的 ID 管理（1.1, 1.2 等）
- **实现**: 所有操作完成后，按时间顺序重新编号 1, 2, 3, ...

**决策 4: LLM 只返回指令，本地负责重构**
- **原因**: 减少 LLM 输出 token，降低成本
- **职责分离**: LLM 做语义判断，本地做数据处理

### 4.2 System Prompt 设计
**设计日期**: 2026-01-17

#### 字幕优化 System Prompt (v1.0)

```
你是一位专业的字幕优化专家，精通视频字幕的断句、纠错和时长控制。

## 任务
分析输入的字幕数据（segments），返回优化指令集。你的目标是使字幕符合行业标准，提升观看体验。

## 输入数据格式
你将收到 JSON 格式的字幕数据：
```json
{
  "segs": [
    {
      "i": segment ID (整数),
      "t": "文本内容",
      "s": 开始时间(秒),
      "e": 结束时间(秒),
      "d": 时长(秒),
      "wc": 词数,
      "cps": 每秒字符数
    }
  ],
  "std": {
    "min_d": 最小时长,
    "max_d": 最大时长,
    "opt_cps": [最佳CPS下限, 上限],
    "max_cps": 最大CPS
  }
}
```

## 优化规则

### 1. 断句优化（移动操作）
- 按语义单元断句：不要切断短语、从句或固定搭配
- 示例：如果 segment 1 的最后几个词和 segment 2 的开头构成完整短语，应移动词到segment 2
- 使用词索引操作，保留时间戳

### 2. 错别字和语义纠正（替换操作）
- 修正明显的拼写错误和语法错误
- 修正 ASR 识别错误（如 "your" 误识别为 "you're"）
- 不要修改专有名词、俚语或口语表达
- 仅在有明确错误时修正

### 3. 合并短字幕（合并操作）
- 时长 < min_d 的 segment 应考虑合并
- 合并后的字幕不能超过 max_d
- 合并后的 CPS 不能超过 max_cps
- 只合并语义连贯的 segments

### 4. 拆分长字幕（拆分操作）
- 时长 > max_d 的 segment 必须拆分
- CPS > max_cps 的 segment 应考虑拆分
- 拆分点应选择自然的语义边界（如逗号、连词处）
- 拆分后每个 segment 应满足 min_d 要求

## 输出格式

返回 JSON 数组，包含所有优化操作：

**操作类型**:

1. **移动 (move)**: 将 segment i 的词 [f:e) 移动到 segment to
```json
{"t":"m", "i":源segment_ID, "f":起始词索引, "e":结束词索引(不包含), "to":目标segment_ID}
```

2. **替换 (replace)**: 替换 segment i 的词 [f:e) 为新文本 w
```json
{"t":"r", "i":segment_ID, "f":起始词索引, "e":结束词索引(不包含), "w":"新文本"}
```

3. **合并 (merge)**: 合并多个 segments
```json
{"t":"g", "i":[segment_ID1, segment_ID2, ...]}
```

4. **拆分 (split)**: 在词索引 p 处拆分 segment
```json
{"t":"s", "i":segment_ID, "p":[词索引1, 词索引2, ...]}
```

**示例输出**:
```json
[
  {"t":"r", "i":1, "f":5, "e":6, "w":"kitty"},
  {"t":"m", "i":2, "f":0, "e":2, "to":1},
  {"t":"g", "i":[3, 4]},
  {"t":"s", "i":5, "p":[8]}
]
```

**如果没有需要优化的内容，返回空数组**: `[]`

## 重要约束

1. **基于词索引操作**: 所有索引都是词索引（word index），不是字符索引
2. **不修改时间戳**: 时间戳会被本地自动重新计算
3. **保持语义完整**: 优先保证字幕语义准确和完整
4. **谨慎操作**: 只在明确需要时进行优化，不要过度修改
5. **遵循标准**: 严格遵守 std 中的时长和 CPS 标准
6. **顺序执行**: 操作按数组顺序执行，注意 ID 变化

## 分析流程

1. 检查每个 segment 的 CPS 和时长是否符合标准
2. 识别断句不合理的地方
3. 识别明显的错别字和语法错误
4. 生成优化指令集
5. 验证优化后是否符合标准

现在开始分析字幕数据。
```

#### 关键设计要点

**1. 清晰的角色和任务定义**
- 明确专家身份
- 说明任务目标

**2. 详细的指令集格式说明**
- 每种操作都有完整示例
- 字段含义清晰
- 包含空数组情况

**3. 明确的优化规则**
- 断句、纠错、合并、拆分各有具体规则
- 提供判断标准（CPS、时长）
- 强调语义优先

**4. 重要约束声明**
- 词索引 vs 字符索引
- 本地负责时间戳计算
- 不要过度修改

**5. 分析流程指引**
- 给 LLM 提供思考框架
- 确保系统化分析

### 4.3 User Prompt 模板
**设计日期**: 2026-01-17

#### 提交 Prompt (v1.0)

```
请分析以下字幕数据并返回优化指令集：

```json
{提取的字幕数据}
```

要求：
1. 识别所有需要优化的问题（断句、纠错、时长、CPS）
2. 返回JSON格式的指令数组
3. 如果没有问题，返回空数组 []
```

**变量替换**:
- `{提取的字幕数据}`: 从完整 segments 提取的精简数据（见 4.1 节）

#### 并发批次提交策略

由于字幕数据可能很长，需要分批提交给 LLM。每批提交格式：

```
请分析以下字幕数据片段（第 {batch_num}/{total_batches} 批）并返回优化指令集：

注意：这是完整字幕的一部分。前面有 {prev_segments} 条字幕，后面还有 {next_segments} 条字幕。

```json
{当前批次数据}
```

上下文信息：
- 前一条字幕："{previous_text}"（segment {prev_id}）
- 后一条字幕："{next_text}"（segment {next_id}）

要求：
1. 考虑前后文进行断句判断
2. 仅返回当前批次的优化指令
3. 对于跨批次的断句问题，使用移动操作连接到相邻批次
```

**变量说明**:
- `batch_num`, `total_batches`: 批次信息
- `prev_segments`, `next_segments`: 前后 segments 数量
- `previous_text`, `next_text`: 上下文字幕文本
- `prev_id`, `next_id`: 上下文 segment IDs

## 5. 并发处理策略

### 5.1 重叠窗口设计
**设计日期**: 2026-01-17

#### 窗口大小策略

**基础原则**:
- 基于 LLM token 限制（输入 + 输出）
- 保留足够的 context 进行语义判断
- 考虑重叠区域

**窗口大小计算**:
```python
# 假设 LLM 输入限制 = 32K tokens
# 预留 system prompt ≈ 1K tokens
# 预留输出指令集 ≈ 2K tokens
# 可用于字幕数据 ≈ 29K tokens

# 每个 segment 平均消耗（估算）:
# {"i":1,"t":"...","s":11.4,"e":19.56,"d":8.16,"wc":9,"cps":16.5}
# 约 100-150 tokens（含文本）

# 窗口大小 ≈ 29000 / 125 ≈ 230 segments
# 保守估计：150-200 segments/批次
```

**推荐配置**:
- **窗口大小**: 150 segments
- **重叠区域**: 10 segments（前后各 5）
- **最小窗口**: 50 segments（最后一批可能较小）

#### 重叠窗口机制

**目的**: 确保跨窗口边界的断句正确性

**实现方式**:
```
批次 1: segments [1-150]     (包含重叠区域 146-150)
批次 2: segments [146-295]   (包含重叠区域 146-150 和 291-295)
批次 3: segments [291-440]   (包含重叠区域 291-295)
...
```

**重叠区域处理**:
1. **前 5 条**: 供 LLM 理解前文语境，不生成优化指令
2. **当前批次主体**: 生成优化指令
3. **后 5 条**: 供 LLM 理解后文语境，不生成优化指令

**边界处理**:
- 第一批：无前重叠
- 最后一批：无后重叠
- 中间批次：前后都有重叠

#### 并发度控制

**并发策略**:
- **顺序处理**: 保证指令执行顺序正确
- **批次独立**: 每批次的指令集独立生成
- **合并指令**: 最后合并所有批次的指令集

**原因**: 避免并发导致的 ID 冲突和指令顺序问题

**优化方向**:
- 未来可考虑：按说话人分组并发（不同说话人的字幕并发处理）
- 当前阶段：顺序处理，确保正确性

#### 冲突解决策略

**场景 1: 跨批次移动操作**
```json
// 批次 1 返回：
{"t":"m", "i":149, "f":10, "e":12, "to":151}

// 问题：segment 151 在批次 2 中
// 解决：允许跨批次引用，最后统一处理
```

**场景 2: 合并操作跨批次**
```json
// 批次 1 返回：
{"t":"g", "i":[148, 149, 150]}

// 批次 2 返回：
{"t":"g", "i":[150, 151, 152]}

// 冲突：segment 150 被两次合并
// 解决：优先采用第一个批次的操作，忽略第二个
```

**解决原则**:
1. **优先级**: 先执行的批次优先
2. **去重**: 检测重复操作并去重
3. **验证**: 执行前验证所有 segment ID 存在

#### 并发处理流程

```
1. 分割字幕数据为多个批次（含重叠）
2. FOR EACH 批次 IN 顺序:
     a. 提取批次数据（主体 + 重叠区域）
     b. 提交给 LLM
     c. 接收指令集
     d. 过滤重叠区域的指令（仅保留主体区域）
3. 合并所有批次的指令集
4. 去重和冲突解决
5. 验证指令集
6. 执行指令集重构字幕数据
```

#### 窗口大小自适应

**动态调整策略**:
```python
def calculate_window_size(segments, model_context_limit):
    """
    基于实际 token 消耗动态调整窗口大小
    """
    # 采样前 10 个 segments 估算平均 token
    sample_tokens = estimate_tokens(segments[:10])
    avg_tokens_per_seg = sample_tokens / 10

    # 计算可用 tokens
    available = model_context_limit - SYSTEM_PROMPT_TOKENS - OUTPUT_RESERVE

    # 计算窗口大小
    window_size = int(available / avg_tokens_per_seg * 0.8)  # 留 20% 余量

    # 限制范围
    return max(50, min(window_size, 300))
```

**实施建议**:
- 初始版本：固定窗口 150
- 优化版本：动态自适应

## 6. 数据流设计

### 6.1 整体数据流
```
[转录数据]
    ↓
[LLM 字幕优化] → [优化后字幕]
    ↓
[LLM 翻译装词] → [翻译后字幕]
    ↓
[语音生成 (IndexTTS2/Edge-TTS)] → [新语音文件列表]
    ↓
[视频合并 (ffmpeg)] → [最终 S2ST 视频]
```

### 6.2 LLM 翻译装词设计

**设计日期**: 2026-01-17
**设计目标**: 实现考虑时长对齐的翻译装词功能

#### 核心挑战

**翻译装词 vs 普通翻译的区别**:

| 维度 | 普通翻译 | 翻译装词 |
|------|---------|---------|
| **输入** | 源语言文本 | 源语言文本 + 时长信息 |
| **输出** | 目标语言文本 | 目标语言文本（时长对齐） |
| **约束** | 语义对等、流畅度 | 语义对等 + 时长对齐 + CPS 标准 |
| **优化目标** | 准确性、自然度 | 准确性 + 时长一致性 |

**关键技术问题**:
1. 如何让 LLM 理解"时长对齐"的抽象概念？
2. 不同语言的 CPS 标准不同，如何统一处理？
3. 字符数约束 vs 语音时长，哪个更精确？

#### 输出格式设计

**设计决策**: 不使用指令集，直接输出翻译文本

**理由**:
1. 翻译是整体替换 `segment.text`，无需精确的词级操作
2. 时间戳信息（`start`, `end`）保持不变
3. `words` 数组由后续 TTS 服务生成
4. 简化输出格式，减少 token 消耗

**输出 JSON 格式**:
```json
{
  "segments": [
    {"id": 1, "text": "翻译后的文本1"},
    {"id": 2, "text": "翻译后的文本2"},
    {"id": 3, "text": "翻译后的文本3"}
  ]
}
```

**本地处理逻辑**:
1. 接收 LLM 返回的翻译文本
2. 根据 `id` 匹配原 segments
3. 替换 `text` 字段
4. 保留 `start`, `end`, `speaker` 等元数据
5. 清空 `words` 数组（待 TTS 生成后填充）

#### 时长约束计算策略

**基本公式**:
```
目标字符数范围 = 时长(秒) × CPS范围
```

**多语言 CPS 配置**:

| 语言 | 理想 CPS | 最大 CPS | 备注 |
|------|---------|---------|------|
| **中文** | 15-17 | 20-21 | 字符 = 汉字 |
| **英文** | 根据音节计算 | - | 需考虑单词长度 |
| **日文** | 13-15 | 18-20 | 假名为单位 |
| **韩文** | 14-16 | 19-21 | 音节为单位 |

**计算示例（中文）**:
```python
# 假设 segment 时长为 3.5 秒
duration = 3.5

# 理想范围（±0%）
ideal_min = duration * 15  # 52.5 字
ideal_max = duration * 17  # 59.5 字

# 可接受范围（±10% 容差）
acceptable_min = duration * 13  # 45.5 字
acceptable_max = duration * 21  # 73.5 字

# 在 Prompt 中表达
char_range = f"{int(ideal_min)}-{int(ideal_max)}"  # "52-59"
```

**时长容差设计**:
- **±0-5%**: 理想区间，无需 rubberband 调整
- **±5-10%**: 可接受区间，需轻微调整
- **±10-15%**: 极限区间，明显音质影响
- **\u003e±15%**: 不可接受，翻译失败

#### System Prompt 设计

**角色定义**:
```
你是一位专业的视频字幕翻译专家，擅长等时翻译（Isochronous Translation）。
你的核心任务是将源语言字幕翻译为目标语言，同时确保翻译后的字幕时长与原字幕一致。
```

**任务说明**:
```
在视频配音场景中，翻译后的字幕将用于语音生成，因此必须严格控制字符数以匹配原语音时长。
每条字幕都标注了字符数范围限制，超出范围会导致语音与视频不同步。
```

**翻译原则**（优先级从高到低）:
1. **时长对齐**: 翻译后的字符数必须在指定范围内
2. **语义准确**: 在满足时长约束的前提下，保证核心语义不丢失
3. **表达自然**: 使用符合目标语言习惯的口语化表达
4. **上下文连贯**: 考虑前后字幕的语境关系

**约束说明**:
```
- 每条字幕都有 "char_range" 字段，表示允许的字符数范围（例如 "52-59"）
- 超出范围的翻译会被拒绝，请务必遵守
- 如果直译无法满足时长，可以：
  * 使用同义词替换（长词换短词，或反之）
  * 适当简化或扩充表达
  * 调整句式结构
  * 省略非关键信息（仅在极端情况）
```

**输出格式要求**:
```
输出格式为 JSON 数组，每个元素包含：
- "id": 字幕 ID（整数）
- "text": 翻译后的文本（字符串，必须在字符数范围内）

示例：
{
  "segments": [
    {"id": 1, "text": "翻译后的文本1"},
    {"id": 2, "text": "翻译后的文本2"}
  ]
}
```

**完整 System Prompt**:
```markdown
你是一位专业的视频字幕翻译专家，擅长等时翻译（Isochronous Translation）。你的核心任务是将源语言字幕翻译为目标语言，同时确保翻译后的字幕时长与原字幕一致。

在视频配音场景中，翻译后的字幕将用于语音生成，因此必须严格控制字符数以匹配原语音时长。每条字幕都标注了字符数范围限制，超出范围会导致语音与视频不同步。

**翻译原则**（优先级从高到低）:
1. **时长对齐**: 翻译后的字符数必须在指定范围内
2. **语义准确**: 在满足时长约束的前提下，保证核心语义不丢失
3. **表达自然**: 使用符合目标语言习惯的口语化表达
4. **上下文连贯**: 考虑前后字幕的语境关系

**约束说明**:
- 每条字幕都有 "char_range" 字段，表示允许的字符数范围（例如 "52-59"）
- 超出范围的翻译会被拒绝，请务必遵守
- 如果直译无法满足时长，可以：
  * 使用同义词替换（长词换短词，或反之）
  * 适当简化或扩充表达
  * 调整句式结构
  * 省略非关键信息（仅在极端情况）

**输出格式**:
输出格式为 JSON 数组，每个元素包含：
- "id": 字幕 ID（整数）
- "text": 翻译后的文本（字符串，必须在字符数范围内）

示例输出：
```json
{
  "segments": [
    {"id": 1, "text": "翻译后的文本1"},
    {"id": 2, "text": "翻译后的文本2"}
  ]
}
```
```

#### User Prompt 模板设计

**基础模板**（单批次）:
```markdown
请将以下 {source_lang} 字幕翻译为 {target_lang}，严格遵守每条字幕的字符数限制：

[
  {
    "id": 1,
    "source": "Well, little kitty, it's time for your medicine.",
    "duration": 3.5,
    "char_range": "52-59"
  },
  {
    "id": 2,
    "source": "Open wide. Good kitty.",
    "duration": 2.0,
    "char_range": "30-34"
  }
]

要求：
1. 每条翻译的字符数必须在 char_range 范围内
2. 保持语义准确和口语化
3. 考虑上下文连贯性
4. 输出格式：{"segments": [{"id": 1, "text": "..."}, ...]}
```

**并发批次模板**（含上下文）:
```markdown
请将以下 {source_lang} 字幕翻译为 {target_lang}，严格遵守每条字幕的字符数限制：

**上文参考**（仅供理解语境，无需翻译）:
[
  {"id": 97, "source": "..."},
  {"id": 98, "source": "..."},
  {"id": 99, "source": "..."}
]

**当前批次**（需要翻译）:
[
  {"id": 100, "source": "...", "duration": 3.5, "char_range": "52-59"},
  {"id": 101, "source": "...", "duration": 2.0, "char_range": "30-34"},
  ...
  {"id": 199, "source": "...", "duration": 1.8, "char_range": "27-30"}
]

**下文参考**（仅供理解语境，无需翻译）:
[
  {"id": 200, "source": "..."},
  {"id": 201, "source": "..."},
  {"id": 202, "source": "..."}
]

要求：
1. 仅翻译"当前批次"中的字幕
2. 每条翻译的字符数必须在 char_range 范围内
3. 参考上下文提高翻译连贯性
4. 输出格式：{"segments": [{"id": 100, "text": "..."}, ...]}
```

**变量替换说明**:
- `{source_lang}`: 源语言名称（如 "英文"）
- `{target_lang}`: 目标语言名称（如 "中文"）
- `char_range`: 根据时长和目标语言 CPS 动态计算

#### 并发处理策略

**窗口设计**:
- **窗口大小**: 100 segments（翻译比优化更耗 token）
- **上下文重叠**: 前后各 3 segments
- **目的**: 提供语境参考，提高翻译连贯性

**与 Phase 1 的区别**:

| 维度 | Phase 1 (字幕优化) | Phase 2 (翻译装词) |
|------|-------------------|-------------------|
| **窗口大小** | 150 segments | 100 segments |
| **重叠区域** | 10 segments (前后各5) | 6 segments (前后各3) |
| **重叠目的** | 避免断句边界冲突 | 提供上下文参考 |
| **是否输出** | 主体区域输出指令 | 仅主体区域输出翻译 |

**实现机制**:
```
批次 1: segments [1-100]      (上下文: 无 | 主体: 1-100 | 下文: 101-103)
批次 2: segments [98-200]     (上文: 98-100 | 主体: 101-197 | 下文: 198-200)
批次 3: segments [198-297]    (上文: 198-200 | 主体: 201-297 | 下文: 无)
```

**上下文处理**:
1. **上文参考**: 提供前 3 条字幕的原文（无需翻译）
2. **当前批次**: 需要翻译的主体内容
3. **下文参考**: 提供后 3 条字幕的原文（无需翻译）

**优势**:
- 减少 token 消耗（重叠仅 6 条 vs 10 条）
- 保持上下文连贯性
- 无需冲突解决（每条仅翻译一次）

**并发度控制**:
- **顺序处理**: 避免并发导致的上下文混乱
- **未来优化**: 可按说话人分组并发

#### 质量验证机制

**输出验证**:
```python
def validate_translation(segment_id, translated_text, char_range, tolerance=0.1):
    """
    验证翻译是否满足字符数约束

    Args:
        segment_id: 字幕 ID
        translated_text: 翻译后的文本
        char_range: 字符数范围（如 "52-59"）
        tolerance: 容差比例（默认 10%）

    Returns:
        (is_valid, error_message)
    """
    min_chars, max_chars = map(int, char_range.split('-'))
    actual_chars = len(translated_text)

    # 理想范围
    if min_chars \u003c= actual_chars \u003c= max_chars:
        return (True, None)

    # 容差范围
    tolerance_min = min_chars * (1 - tolerance)
    tolerance_max = max_chars * (1 + tolerance)

    if tolerance_min \u003c= actual_chars \u003c= tolerance_max:
        # 发出警告但接受
        return (True, f"Warning: segment {segment_id} 字符数 {actual_chars} 在容差范围内")

    # 超出容差
    return (False, f"Error: segment {segment_id} 字符数 {actual_chars} 超出范围 {char_range}")
```

**自动重试机制**:
1. 如果某条翻译超出字符数范围
2. 单独重新提交该条（附加更严格的约束）
3. 最多重试 2 次
4. 仍失败则标记为人工审核

#### 关键设计决策总结

**决策 1: 不使用指令集**
- **原因**: 翻译是整体替换，无需词级精确操作
- **好处**: 简化输出格式，减少 token 消耗

**决策 2: 预计算字符数范围**
- **原因**: 降低 LLM 计算负担，减少理解偏差
- **好处**: 更精确的约束，更容易验证

**决策 3: 上下文重叠窗口**
- **原因**: 翻译需要语境理解
- **好处**: 提高翻译连贯性，平衡成本

**决策 4: 优先级 - 时长对齐 \u003e 语义准确**
- **原因**: S2ST 场景下，音画同步是刚需
- **好处**: 确保最终视频质量

### 6.3 IndexTTS2 语音生成设计

**设计日期**: 2026-01-18
**设计目标**: 实现基于 IndexTTS2 的语音生成和时长对齐功能

#### 核心技术挑战

**IndexTTS2 vs 传统 TTS 的特点**:

| 维度 | 传统 TTS | IndexTTS2 |
|------|---------|-----------|
| **音色控制** | 预定义音色库 | 参考音自定义（Zero-shot） |
| **情感控制** | 有限/无 | 音色和情感分离控制 |
| **时长控制** | 部分支持 SSML | 不支持，需后处理 |
| **质量** | 中等 | 高质量、自然度高 |

**S2ST 场景的关键需求**:
1. 使用原视频说话人的音频作为参考音（保持音色一致）
2. 生成的语音时长需要与翻译后的字幕时长对齐
3. 同一说话人的语音音色必须一致
4. 处理多说话人场景

#### 参考音提取策略

**策略选择：每个说话人提取一次全局参考音**

**理由**:
1. 同一说话人应使用相同参考音，确保音色一致性
2. 减少 IO 操作（vs 每条字幕单独提取）
3. 可以确保参考音时长充足
4. 符合 IndexTTS2 的 Zero-shot 克隆原理

**提取流程**:

```python
def extract_reference_audio(original_audio_path, segments, speaker_id):
    """
    为指定说话人提取参考音

    Args:
        original_audio_path: 原始音频文件路径
        segments: 所有字幕 segments
        speaker_id: 说话人 ID（如 "SPEAKER_00"）

    Returns:
        参考音文件路径
    """
    # 1. 筛选该说话人的所有 segments
    speaker_segments = [s for s in segments if s.speaker == speaker_id]

    # 2. 选择提取策略
    if len(speaker_segments) == 0:
        raise ValueError(f"No segments for speaker {speaker_id}")

    # 策略 A: 选择最长的连续片段
    longest_segment = max(speaker_segments, key=lambda s: s.end - s.start)
    start_time = longest_segment.start
    end_time = longest_segment.end

    # 策略 B: 选择前 N 秒（如 10 秒）
    # start_time = speaker_segments[0].start
    # end_time = min(start_time + 10.0, speaker_segments[-1].end)

    # 3. 使用 ffmpeg 提取
    ref_audio_path = f"/tmp/{task_id}/ref_{speaker_id}.wav"
    subprocess.run([
        "ffmpeg", "-i", original_audio_path,
        "-ss", str(start_time),
        "-to", str(end_time),
        "-ar", "24000",  # IndexTTS2 推荐采样率
        "-ac", "1",       # 单声道
        ref_audio_path
    ])

    return ref_audio_path
```

**提取策略对比**:

| 策略 | 优点 | 缺点 | 推荐场景 |
|------|------|------|---------|
| **最长片段** | 质量最高，连续性好 | 可能不够长 | 说话人片段较长 |
| **前 N 秒** | 时长可控 | 可能跨越多个 segments | 说话人片段较短 |
| **合并多个片段** | 时长充足 | 拼接可能有瑕疵 | 所有片段都很短 |

**推荐配置**:
- 优先使用"最长片段"策略
- 如果最长片段 < 3 秒，使用"前 10 秒"策略
- 如果仍不足，使用重复拼接

#### 参考音时长调整

**最小时长要求**: 3 秒（经验值，待实测验证）

**时长不足的处理**:

```bash
# 检测音频时长
duration=$(ffprobe -v error -show_entries format=duration \
  -of default=noprint_wrappers=1:nokey=1 ref.wav)

# 如果 < 3 秒，使用 crossfade 重复拼接
if (( $(echo "$duration < 3.0" | bc -l) )); then
  loop_count=$(echo "scale=0; 3.0 / $duration + 1" | bc)

  ffmpeg -i ref.wav -filter_complex \
    "aloop=loop=$loop_count:size=999999999[a]; \
     [a]atrim=0:3.0[out]" \
    -map "[out]" ref_looped.wav
fi
```

**Crossfade 拼接（高质量版本）**:

```bash
ffmpeg -i ref.wav -filter_complex \
  "[0:a]asplit=2[a1][a2]; \
   [a1]adelay=delays=$(echo \"$duration * 1000 - 100\" | bc)[delayed]; \
   [a2][delayed]acrossfade=d=0.1:c1=tri:c2=tri[loop]; \
   [loop]aloop=loop=1:size=999999999[out]" \
  -map "[out]" -t 3.0 ref_looped.wav
```

**配置参数**:
```yaml
reference_audio:
  min_duration: 3.0          # 最小时长（秒）
  target_duration: 5.0       # 目标时长（秒）
  crossfade_duration: 0.1    # 交叉淡化时长（秒）
  sample_rate: 24000         # 采样率
  channels: 1                # 单声道
```

#### IndexTTS2 语音生成流程

**整体流程设计**:

```
[翻译后字幕] + [原始音频]
    ↓
[阶段 1: 参考音预处理]
    - 按 speaker 分组
    - 提取每个说话人的参考音
    - 检查时长并调整
    - 缓存参考音文件
    ↓
[阶段 2: 语音生成]
    - 逐条调用 IndexTTS2 API
    - 使用对应 speaker 的参考音
    - 生成原始语音文件
    ↓
[阶段 3: 时长对齐]
    - 检测生成语音的实际时长
    - 计算 tempo 参数
    - 使用 Rubberband 调整时长
    ↓
[阶段 4: 上传和输出]
    - 上传所有音频文件到 MinIO
    - 生成输出数据结构
```

**阶段 1: 参考音预处理**（每个说话人执行一次）

```python
def preprocess_reference_audios(segments, original_audio_url, task_id):
    """
    预处理所有说话人的参考音

    Returns:
        Dict[speaker_id, ref_audio_path]
    """
    # 下载原始音频
    original_audio = download_from_minio(original_audio_url)

    # 获取所有说话人列表
    speakers = set(s.speaker for s in segments)

    ref_audios = {}
    for speaker in speakers:
        # 提取参考音
        ref_path = extract_reference_audio(original_audio, segments, speaker)

        # 检查时长
        duration = get_audio_duration(ref_path)
        if duration < 3.0:
            # 重复拼接
            ref_path = loop_audio_with_crossfade(ref_path, target_duration=5.0)

        # 上传到 MinIO（可选，便于调试）
        ref_minio_url = upload_to_minio(ref_path, f"ref_{speaker}.wav")

        ref_audios[speaker] = {
            "local_path": ref_path,
            "minio_url": ref_minio_url,
            "duration": duration
        }

    return ref_audios
```

**阶段 2: 语音生成**（逐条或并发）

```python
def generate_single_audio(segment, ref_audio_path, output_path):
    """
    为单条字幕生成语音

    Args:
        segment: 字幕 segment（包含 text, speaker）
        ref_audio_path: 参考音路径
        output_path: 输出音频路径

    Returns:
        生成的音频信息
    """
    # 调用 IndexTTS2 API
    # 注：实际 API 调用方式需参考 IndexTTS2 文档
    result = indextts_client.generate(
        text=segment.text,
        spk_audio_prompt=ref_audio_path,
        language="zh",  # 根据翻译目标语言动态设置
        output_path=output_path,
        # 可选参数
        speed=1.0,
        emo_audio_prompt=None,  # 情感参考（可选）
    )

    # 检测生成的音频时长
    generated_duration = get_audio_duration(output_path)

    return {
        "segment_id": segment.id,
        "audio_path": output_path,
        "generated_duration": generated_duration,
        "target_duration": segment.end - segment.start
    }
```

**并发生成策略**:

```python
from celery import group

def generate_all_audios_concurrent(segments, ref_audios, task_id):
    """
    并发生成所有字幕的语音
    """
    # 创建子任务列表
    subtasks = []
    for segment in segments:
        ref_audio = ref_audios[segment.speaker]
        subtask = generate_single_audio_task.s(
            segment=segment.to_dict(),
            ref_audio_path=ref_audio["local_path"],
            task_id=task_id
        )
        subtasks.append(subtask)

    # 并发执行（Celery group）
    job = group(subtasks)
    results = job.apply_async()

    # 等待所有任务完成
    audio_info_list = results.get(timeout=1800)  # 30 分钟超时

    return audio_info_list
```

**并发度控制**:
- 使用 Celery queue 配置
- 推荐最多 4 个并发 TTS 任务（单 GPU 场景）
- 配置示例：
```python
# celery config
CELERY_ROUTES = {
    'generate_single_audio_task': {'queue': 'indextts_queue'},
}

# worker 启动
celery -A app worker -Q indextts_queue --concurrency=4
```

**阶段 3: Rubberband 时长对齐**

```python
def align_audio_duration(audio_path, target_duration, output_path):
    """
    使用 Rubberband 调整音频时长

    Args:
        audio_path: 输入音频路径
        target_duration: 目标时长（秒）
        output_path: 输出音频路径

    Returns:
        对齐信息（tempo, quality_warning）
    """
    # 获取当前时长
    current_duration = get_audio_duration(audio_path)

    # 计算 tempo
    tempo = current_duration / target_duration

    # 判断是否需要对齐
    deviation = abs(tempo - 1.0)

    if deviation < 0.05:  # ±5% 以内，无需调整
        shutil.copy(audio_path, output_path)
        return {"tempo": tempo, "quality_warning": False, "adjusted": False}

    # 质量检查
    quality_warning = False
    if tempo < 0.8 or tempo > 1.2:
        logger.warning(f"Tempo {tempo} 超出推荐范围 (0.8-1.2)")
        quality_warning = True

    # Rubberband 调整
    rubberband_params = (
        f"tempo={tempo}:"
        f"transients=smooth:"
        f"formant=preserved:"
        f"pitchq=quality:"
        f"window=long"
    )

    subprocess.run([
        "ffmpeg", "-i", audio_path,
        "-af", f"rubberband={rubberband_params}",
        output_path
    ], check=True)

    return {
        "tempo": tempo,
        "quality_warning": quality_warning,
        "adjusted": True,
        "current_duration": current_duration,
        "target_duration": target_duration
    }
```

**Tempo 质量等级**:

| Tempo 范围 | 质量等级 | 是否调整 | 警告级别 |
|-----------|---------|---------|---------|
| 0.95 - 1.05 | 优秀 | 否 | 无 |
| 0.90 - 1.10 | 良好 | 是 | 无 |
| 0.80 - 1.20 | 可接受 | 是 | Info |
| 0.70 - 1.30 | 质量风险 | 是 | Warning |
| < 0.70 或 > 1.30 | 不可接受 | 是 | Error |

**优化建议**:
如果大量 segments 的 tempo 超出理想范围（0.9-1.1），说明 Phase 2 的翻译装词字符数计算不够精确，需要：
1. 调整目标语言的 CPS 配置
2. 优化翻译 Prompt（强调简洁或扩充）
3. 增加翻译后的字符数验证

#### 数据结构设计

**输入数据结构**（来自 Phase 2 - 翻译后字幕）:

```json
{
  "segments": [
    {
      "id": 1,
      "text": "小猫咪，该吃药了。",
      "start": 11.4,
      "end": 14.9,
      "speaker": "SPEAKER_00",
      "duration": 3.5
    },
    {
      "id": 2,
      "text": "张大嘴。乖猫咪。",
      "start": 14.9,
      "end": 17.0,
      "speaker": "SPEAKER_00",
      "duration": 2.1
    }
  ],
  "original_audio_minio_url": "minio://yivideo/task-001/original.wav",
  "source_language": "en",
  "target_language": "zh"
}
```

**输出数据结构**（传递给 Phase 5 - 视频合并）:

```json
{
  "audio_files": [
    {
      "segment_id": 1,
      "audio_minio_url": "minio://yivideo/task-001/audio_1.wav",
      "start": 11.4,
      "end": 14.9,
      "duration": 3.5,
      "speaker": "SPEAKER_00",
      "alignment_info": {
        "generated_duration": 3.7,
        "target_duration": 3.5,
        "tempo": 1.057,
        "adjusted": true,
        "quality_warning": false
      }
    },
    {
      "segment_id": 2,
      "audio_minio_url": "minio://yivideo/task-001/audio_2.wav",
      "start": 14.9,
      "end": 17.0,
      "duration": 2.1,
      "speaker": "SPEAKER_00",
      "alignment_info": {
        "generated_duration": 2.0,
        "target_duration": 2.1,
        "tempo": 0.952,
        "adjusted": true,
        "quality_warning": false
      }
    }
  ],
  "reference_audios": {
    "SPEAKER_00": {
      "minio_url": "minio://yivideo/task-001/ref_speaker_00.wav",
      "duration": 5.2,
      "was_looped": false
    },
    "SPEAKER_01": {
      "minio_url": "minio://yivideo/task-001/ref_speaker_01.wav",
      "duration": 3.8,
      "was_looped": true
    }
  },
  "statistics": {
    "total_segments": 150,
    "total_speakers": 2,
    "avg_tempo": 1.02,
    "max_tempo": 1.15,
    "min_tempo": 0.91,
    "quality_warnings": 3,
    "error_count": 0
  }
}
```

**中间文件路径约定**:
```
/tmp/{task_id}/
  ├── original.wav              # 下载的原始音频
  ├── ref_SPEAKER_00.wav        # 说话人 0 参考音
  ├── ref_SPEAKER_01.wav        # 说话人 1 参考音
  ├── generated_1.wav           # 生成的原始音频（未对齐）
  ├── generated_2.wav
  ├── aligned_1.wav             # 对齐后的音频
  └── aligned_2.wav
```

#### 错误处理和边界情况

**异常场景及处理策略**:

| 异常场景 | 原因 | 处理策略 | 回退方案 |
|---------|------|---------|---------|
| **原始音频不存在** | MinIO URL 无效 | 任务失败，返回错误 | 无（必需） |
| **参考音提取失败** | 时间区间无效、音频损坏 | 记录错误，跳过该 speaker | 使用默认参考音 |
| **无 speaker 信息** | 未执行说话人分离 | 使用默认 speaker ID | 提取音频前 N 秒 |
| **IndexTTS2 生成失败** | API 错误、GPU OOM | 重试 3 次，记录失败 | 标记人工处理 |
| **Tempo 超出范围** | 翻译文本过长/过短 | 记录警告，仍生成 | 建议重新翻译 |
| **Rubberband 失败** | FFmpeg 错误 | 使用原始音频 | 记录警告 |

**错误处理代码示例**:

```python
def safe_generate_audio(segment, ref_audio, max_retries=3):
    """
    安全的语音生成（含重试）
    """
    for attempt in range(max_retries):
        try:
            return generate_single_audio(segment, ref_audio)
        except IndexTTSAPIError as e:
            logger.warning(f"Attempt {attempt+1} failed: {e}")
            if attempt == max_retries - 1:
                # 最后一次重试失败
                return {
                    "segment_id": segment.id,
                    "error": str(e),
                    "status": "failed"
                }
            time.sleep(5 * (attempt + 1))  # 指数退避
```

**配置参数**:

```yaml
indextts_config:
  # API 配置
  api_endpoint: "http://localhost:9880"
  timeout: 60  # 单次生成超时（秒）

  # 重试配置
  max_retries: 3
  retry_delay: 5

  # 参考音配置
  min_ref_duration: 3.0
  target_ref_duration: 5.0
  crossfade_duration: 0.1

  # 时长对齐配置
  tempo_tolerance: 0.05  # ±5% 内不调整
  tempo_warning_threshold: 0.2  # ±20% 发出警告
  tempo_max_deviation: 0.3  # ±30% 最大允许

  # 并发配置
  max_concurrent_tasks: 4

  # 默认值
  default_speaker: "DEFAULT"
  default_language: "zh"
  sample_rate: 24000
```

#### YiVideo 架构集成

**Celery 任务定义**:

```python
@celery_app.task(bind=True)
@gpu_lock(timeout=1800)  # 30 分钟 GPU 锁
def indextts_generate_speech(self: Task, context: dict) -> dict:
    """
    IndexTTS2 语音生成任务

    Args:
        context: WorkflowContext 字典

    Returns:
        更新后的 WorkflowContext
    """
    executor = IndexTTSExecutor(context)
    return executor.execute()
```

**BaseNodeExecutor 实现**:

```python
class IndexTTSExecutor(BaseNodeExecutor):
    """
    IndexTTS2 语音生成节点执行器
    """

    def validate_input(self):
        """验证输入参数"""
        required_fields = [
            "segments",
            "original_audio_minio_url",
            "target_language"
        ]
        for field in required_fields:
            if field not in self.input_params:
                raise ValueError(f"Missing required field: {field}")

        # 验证 segments 格式
        segments = self.input_params["segments"]
        if not isinstance(segments, list) or len(segments) == 0:
            raise ValueError("segments must be a non-empty list")

    def get_cache_key_fields(self):
        """获取缓存键字段"""
        return [
            "segments",  # 翻译文本
            "target_language",
            "speaker_config"  # 说话人配置（如果有）
        ]

    def execute_core_logic(self):
        """核心执行逻辑"""
        segments = self.input_params["segments"]
        original_audio_url = self.input_params["original_audio_minio_url"]

        # 阶段 1: 预处理参考音
        ref_audios = self.preprocess_reference_audios(segments, original_audio_url)

        # 阶段 2: 生成语音（并发）
        audio_files = self.generate_all_audios(segments, ref_audios)

        # 阶段 3: 时长对齐
        aligned_files = self.align_all_audios(audio_files)

        # 阶段 4: 上传 MinIO
        uploaded_files = self.upload_audio_files(aligned_files)

        # 生成统计信息
        statistics = self.calculate_statistics(uploaded_files)

        return {
            "audio_files": uploaded_files,
            "reference_audios": ref_audios,
            "statistics": statistics
        }
```

**状态复用机制**:

```python
def get_cache_key(self):
    """
    生成缓存键

    基于：翻译文本 + speaker 配置
    如果翻译未变，直接返回缓存的语音文件
    """
    segments_hash = hash(
        tuple((s["id"], s["text"], s["speaker"]) for s in self.input_params["segments"])
    )
    return f"{self.task_id}:indextts:{segments_hash}"
```

#### 性能优化

**并发策略**:

| 场景 | 并发方式 | 预估耗时 |
|------|---------|---------|
| **顺序生成** | 逐条调用 | 150 条 × 5s = 750s（12.5 分钟） |
| **并发生成（4 并发）** | Celery group | 150 / 4 × 5s = 187s（3 分钟） |
| **批量生成（优化后）** | 批量 API 调用 | ~120s（2 分钟） |

**优化措施**:

1. **模型预加载**:
```python
# Worker 启动时预加载模型
@worker_process_init.connect
def preload_indextts_model(**kwargs):
    global indextts_model
    indextts_model = IndexTTS2Model.load()
    indextts_model.to_gpu()
```

2. **参考音缓存**:
```python
# 缓存参考音，避免重复提取
ref_audio_cache = {}  # {speaker_id: ref_audio_path}
```

3. **批量上传**:
```python
# 使用多线程批量上传到 MinIO
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=8) as executor:
    futures = [
        executor.submit(upload_to_minio, audio_path)
        for audio_path in audio_files
    ]
    results = [f.result() for f in futures]
```

4. **GPU 内存管理**:
```python
# 及时清理 GPU 内存
import torch
torch.cuda.empty_cache()
```

#### 关键设计决策总结

**决策 1: 每个说话人提取一次全局参考音**
- **原因**: 保证音色一致性，减少 IO 操作
- **好处**: 同一说话人的所有语音音色统一

**决策 2: Rubberband 后处理时长对齐**
- **原因**: IndexTTS2 不支持精确时长控制
- **好处**: 通用方案，适用于所有 TTS 系统

**决策 3: 并发生成 + GPU 锁控制**
- **原因**: 提升性能，避免 GPU 资源竞争
- **好处**: 3 分钟完成 150 条语音生成

**决策 4: 完整的质量监控和统计**
- **原因**: S2ST 质量依赖时长对齐精度
- **好处**: 便于调试和优化

### 6.4 Edge-TTS 语音生成设计

**设计日期**: 2026-01-18
**设计目标**: 实现基于 Edge-TTS 的快速语音生成和时长对齐功能

#### 核心技术特点

**Edge-TTS vs IndexTTS2 对比**:

| 维度 | IndexTTS2 | Edge-TTS |
|------|-----------|----------|
| **音色控制** | 参考音自定义（Zero-shot） | 预定义音色库 |
| **音色还原** | 可还原原说话人 | 无法还原原说话人 |
| **时长控制** | 不支持 | 不支持（仅 rate 参数） |
| **质量** | 高质量、自然度高 | 中等质量 |
| **资源需求** | GPU 必需 | CPU only |
| **速度** | 较慢（5s/条） | 快速（2s/条） |
| **成本** | 需要部署 | 免费使用 |
| **适用场景** | 高质量配音、还原原音色 | 快速原型、资源受限环境 |

**S2ST 场景的定位**:
- **作为 IndexTTS2 的快速备选方案**
- 适合不要求还原原说话人音色的场景
- 适合快速验证翻译装词效果
- 适合资源受限环境（无 GPU 或 GPU 繁忙）

#### 时长对齐策略

**rate 参数的工作原理**:
- Edge-TTS 支持 `--rate` 参数调整语速
- 范围：通常 -50% 到 +100%（即 0.5x 到 2.0x）
- 格式：`--rate=+20%` 或 `--rate=-30%`

**时长对齐方案对比**:

| 方案 | 流程 | 优点 | 缺点 | 推荐度 |
|------|------|------|------|--------|
| **A: 两次生成** | 1. 默认 rate 生成<br>2. 计算所需 rate<br>3. 重新生成<br>4. Rubberband 微调 | rate 调整精确 | 时间成本翻倍 | ⭐⭐ |
| **B: 一次生成** | 1. 默认 rate 生成<br>2. Rubberband 直接调整 | 快速高效 | Rubberband 幅度可能较大 | ⭐⭐⭐⭐⭐ |
| **C: rate 预估** | 1. 根据字符数估算 rate<br>2. 用估算 rate 生成<br>3. Rubberband 微调 | 理论最优 | 估算不准确，复杂度高 | ⭐⭐⭐ |

**推荐方案：B - 一次生成 + Rubberband**

**理由**:
1. **Phase 2 已做时长对齐**：翻译装词时已根据 CPS 计算字符数，生成的语音时长应该接近目标（±10%）
2. **Edge-TTS 速度快**：单条生成仅 2 秒，重新生成的性价比不高
3. **Rubberband 质量可接受**：在 ±20% 范围内音质损失可接受
4. **简单高效**：减少系统复杂度

**优化方向**（未来）:
- 为 Edge-TTS 维护"语速校准系数"
- 在 Phase 2 翻译时应用 TTS 系统特定的 CPS 调整
- 例如：Edge-TTS 中文语速比理想 CPS 快 15%，翻译时适当增加字符数

#### 语音生成流程

**整体流程设计**:

```
[翻译后字幕]
    ↓
[阶段 1: 音色选择]
    - 根据 speaker 映射到 Edge-TTS voice
    - 配置目标语言
    - 准备 Edge-TTS 参数
    ↓
[阶段 2: 语音生成（并发）]
    - 逐条调用 Edge-TTS CLI
    - 使用默认 rate (+0%)
    - 生成原始语音文件
    ↓
[阶段 3: 时长对齐]
    - 测量生成语音的实际时长
    - 使用 Rubberband 调整至目标时长
    ↓
[阶段 4: 上传和输出]
    - 上传所有音频文件到 MinIO
    - 生成输出数据结构
```

**与 IndexTTS2 流程对比**:

| 阶段 | IndexTTS2 | Edge-TTS |
|------|-----------|----------|
| **阶段 0** | 参考音预处理（耗时） | **无需参考音**（简化） |
| **阶段 1** | 语音生成 | 音色选择 + 语音生成 |
| **阶段 2** | 时长对齐 | 时长对齐（相同） |
| **阶段 3** | 上传输出 | 上传输出（相同） |

**关键简化**:
- 无需参考音提取和预处理（节省大量 IO 时间）
- 无需 GPU 锁（CPU 任务）
- 并发度可以更高（8-16 vs 4）

#### 音色选择策略

**音色映射机制**:

```python
def select_voice(speaker_id, target_language, user_mapping=None):
    """
    为 speaker 选择合适的 Edge-TTS voice

    Args:
        speaker_id: 说话人 ID（如 "SPEAKER_00"）
        target_language: 目标语言（如 "zh", "en"）
        user_mapping: 用户自定义映射（可选）

    Returns:
        Edge-TTS voice 名称
    """
    # 策略 1: 优先使用用户自定义映射
    if user_mapping and speaker_id in user_mapping:
        return user_mapping[speaker_id]

    # 策略 2: 根据 speaker_id 后缀推断性别
    # 约定：SPEAKER_00, SPEAKER_02 → 偶数 → 女声
    #       SPEAKER_01, SPEAKER_03 → 奇数 → 男声
    speaker_index = int(speaker_id.split("_")[-1])
    gender = "female" if speaker_index % 2 == 0 else "male"

    # 策略 3: 返回默认音色
    default_voices = {
        "zh": {
            "female": "zh-CN-XiaoxiaoNeural",
            "male": "zh-CN-YunxiNeural"
        },
        "en": {
            "female": "en-US-AriaNeural",
            "male": "en-US-GuyNeural"
        },
        "ja": {
            "female": "ja-JP-NanamiNeural",
            "male": "ja-JP-KeitaNeural"
        },
        "ko": {
            "female": "ko-KR-SunHiNeural",
            "male": "ko-KR-InJoonNeural"
        }
    }

    return default_voices.get(target_language, {}).get(gender, "zh-CN-XiaoxiaoNeural")
```

**默认音色配置**:

| 语言 | 女声 | 男声 |
|------|------|------|
| **中文（zh）** | zh-CN-XiaoxiaoNeural | zh-CN-YunxiNeural |
| **英文（en）** | en-US-AriaNeural | en-US-GuyNeural |
| **日文（ja）** | ja-JP-NanamiNeural | ja-JP-KeitaNeural |
| **韩文（ko）** | ko-KR-SunHiNeural | ko-KR-InJoonNeural |

**用户自定义映射**（输入参数）:
```json
{
  "voice_mapping": {
    "SPEAKER_00": "zh-CN-XiaoyiNeural",  // 自定义女声
    "SPEAKER_01": "zh-CN-YunjianNeural"  // 自定义男声
  }
}
```

#### 语音生成实现

**阶段 1: 音色预处理**

```python
def preprocess_voice_mapping(segments, target_language, user_mapping=None):
    """
    为所有说话人选择音色

    Returns:
        Dict[speaker_id, voice_name]
    """
    speakers = set(s.speaker for s in segments)

    voice_mapping = {}
    for speaker in speakers:
        voice = select_voice(speaker, target_language, user_mapping)
        voice_mapping[speaker] = voice
        logger.info(f"Speaker {speaker} → Voice {voice}")

    return voice_mapping
```

**阶段 2: 语音生成（单条）**

```python
def generate_single_audio_edgetts(segment, voice, output_path):
    """
    使用 Edge-TTS 生成单条字幕的语音

    Args:
        segment: 字幕 segment（包含 text）
        voice: Edge-TTS voice 名称
        output_path: 输出音频路径

    Returns:
        生成的音频信息
    """
    # Edge-TTS CLI 调用
    cmd = [
        "edge-tts",
        "--text", segment.text,
        "--voice", voice,
        "--rate", "+0%",  # 使用默认语速
        "--write-media", output_path
    ]

    subprocess.run(cmd, check=True, capture_output=True)

    # 检测生成的音频时长
    generated_duration = get_audio_duration(output_path)

    return {
        "segment_id": segment.id,
        "audio_path": output_path,
        "generated_duration": generated_duration,
        "target_duration": segment.end - segment.start,
        "voice": voice
    }
```

**阶段 2: 并发生成策略**

```python
from celery import group

def generate_all_audios_edgetts(segments, voice_mapping, task_id):
    """
    并发生成所有字幕的语音
    """
    # 创建子任务列表
    subtasks = []
    for segment in segments:
        voice = voice_mapping[segment.speaker]
        subtask = generate_single_audio_edgetts_task.s(
            segment=segment.to_dict(),
            voice=voice,
            task_id=task_id
        )
        subtasks.append(subtask)

    # 并发执行（Celery group）
    job = group(subtasks)
    results = job.apply_async()

    # 等待所有任务完成
    audio_info_list = results.get(timeout=900)  # 15 分钟超时

    return audio_info_list
```

**并发度配置**:
```python
# celery config
CELERY_ROUTES = {
    'generate_single_audio_edgetts_task': {'queue': 'edgetts_queue'},
}

# worker 启动（CPU 密集，推荐 8-16 并发）
celery -A app worker -Q edgetts_queue --concurrency=8
```

**阶段 3: Rubberband 时长对齐**

复用 IndexTTS2 的 `align_audio_duration()` 函数，参数和逻辑完全相同：
- Rubberband 参数：`transients=smooth:formant=preserved:pitchq=quality:window=long`
- Tempo 质量等级：与 IndexTTS2 一致
- 容差配置：±5% 内不调整，±20% 发出警告

#### 数据结构设计

**输入数据结构**:

```json
{
  "segments": [
    {
      "id": 1,
      "text": "小猫咪，该吃药了。",
      "start": 11.4,
      "end": 14.9,
      "speaker": "SPEAKER_00",
      "duration": 3.5
    }
  ],
  "target_language": "zh",
  "voice_mapping": {  // 可选，用户自定义
    "SPEAKER_00": "zh-CN-XiaoxiaoNeural",
    "SPEAKER_01": "zh-CN-YunxiNeural"
  }
}
```

**输出数据结构**（与 IndexTTS2 格式一致）:

```json
{
  "audio_files": [
    {
      "segment_id": 1,
      "audio_minio_url": "minio://yivideo/task-001/audio_1.wav",
      "start": 11.4,
      "end": 14.9,
      "duration": 3.5,
      "speaker": "SPEAKER_00",
      "voice": "zh-CN-XiaoxiaoNeural",  // Edge-TTS 特有字段
      "alignment_info": {
        "generated_duration": 3.7,
        "target_duration": 3.5,
        "tempo": 1.057,
        "adjusted": true,
        "quality_warning": false
      }
    }
  ],
  "voice_mapping": {
    "SPEAKER_00": "zh-CN-XiaoxiaoNeural",
    "SPEAKER_01": "zh-CN-YunxiNeural"
  },
  "statistics": {
    "total_segments": 150,
    "total_speakers": 2,
    "avg_tempo": 1.02,
    "max_tempo": 1.15,
    "min_tempo": 0.91,
    "quality_warnings": 3,
    "error_count": 0
  }
}
```

**中间文件路径约定**:
```
/tmp/{task_id}/
  ├── generated_edgetts_1.wav   # 生成的原始音频
  ├── generated_edgetts_2.wav
  ├── aligned_edgetts_1.wav     # 对齐后的音频
  └── aligned_edgetts_2.wav
```

#### 错误处理和配置

**异常场景及处理策略**:

| 异常场景 | 原因 | 处理策略 | 回退方案 |
|---------|------|---------|---------|
| **Edge-TTS CLI 错误** | 网络问题、服务不可用 | 重试 3 次 | 标记失败 |
| **不支持的语言** | target_language 无效 | 返回错误 | 使用中文默认 |
| **音色不存在** | voice 配置错误 | 使用默认音色 | - |
| **文本过长** | text 超过 Edge-TTS 限制 | 拆分生成后拼接 | 标记警告 |
| **Rubberband 失败** | FFmpeg 错误 | 使用原始音频 | 记录警告 |

**配置参数**:

```yaml
edgetts_config:
  # API 配置
  timeout: 30  # 单次生成超时（秒）

  # 重试配置
  max_retries: 3
  retry_delay: 2

  # 默认音色映射
  default_voices:
    zh:
      female: "zh-CN-XiaoxiaoNeural"
      male: "zh-CN-YunxiNeural"
    en:
      female: "en-US-AriaNeural"
      male: "en-US-GuyNeural"
    ja:
      female: "ja-JP-NanamiNeural"
      male: "ja-JP-KeitaNeural"
    ko:
      female: "ko-KR-SunHiNeural"
      male: "ko-KR-InJoonNeural"

  # 时长对齐配置（与 IndexTTS2 相同）
  tempo_tolerance: 0.05  # ±5% 内不调整
  tempo_warning_threshold: 0.2  # ±20% 发出警告
  tempo_max_deviation: 0.3  # ±30% 最大允许

  # 并发配置
  max_concurrent_tasks: 8

  # Edge-TTS 限制
  max_text_length: 500  # 单次请求字符限制

  # 语速校准（未来优化）
  speed_calibration:
    zh: 1.0  # 中文语速校准系数
    en: 1.0  # 英文语速校准系数
```

**文本长度处理**:

```python
def split_long_text(text, max_length=500):
    """
    如果文本过长，拆分为多个片段
    """
    if len(text) <= max_length:
        return [text]

    # 按句子拆分
    sentences = re.split(r'([。！？.!?])', text)

    chunks = []
    current_chunk = ""

    for i in range(0, len(sentences), 2):
        sentence = sentences[i]
        delimiter = sentences[i+1] if i+1 < len(sentences) else ""

        if len(current_chunk) + len(sentence) + len(delimiter) <= max_length:
            current_chunk += sentence + delimiter
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence + delimiter

    if current_chunk:
        chunks.append(current_chunk)

    return chunks
```

#### YiVideo 架构集成

**Celery 任务定义**:

```python
@celery_app.task(bind=True)
# 注意：无需 @gpu_lock()，Edge-TTS 是 CPU 任务
def edgetts_generate_speech(self: Task, context: dict) -> dict:
    """
    Edge-TTS 语音生成任务

    Args:
        context: WorkflowContext 字典

    Returns:
        更新后的 WorkflowContext
    """
    executor = EdgeTTSExecutor(context)
    return executor.execute()
```

**BaseNodeExecutor 实现**:

```python
class EdgeTTSExecutor(BaseNodeExecutor):
    """
    Edge-TTS 语音生成节点执行器
    """

    def validate_input(self):
        """验证输入参数"""
        required_fields = ["segments", "target_language"]
        for field in required_fields:
            if field not in self.input_params:
                raise ValueError(f"Missing required field: {field}")

        # 验证 segments 格式
        segments = self.input_params["segments"]
        if not isinstance(segments, list) or len(segments) == 0:
            raise ValueError("segments must be a non-empty list")

    def get_cache_key_fields(self):
        """获取缓存键字段"""
        return [
            "segments",  # 翻译文本
            "target_language",
            "voice_mapping"  # 音色映射
        ]

    def execute_core_logic(self):
        """核心执行逻辑"""
        segments = self.input_params["segments"]
        target_language = self.input_params["target_language"]
        user_voice_mapping = self.input_params.get("voice_mapping")

        # 阶段 1: 音色选择
        voice_mapping = self.preprocess_voice_mapping(
            segments, target_language, user_voice_mapping
        )

        # 阶段 2: 生成语音（并发）
        audio_files = self.generate_all_audios(segments, voice_mapping)

        # 阶段 3: 时长对齐
        aligned_files = self.align_all_audios(audio_files)

        # 阶段 4: 上传 MinIO
        uploaded_files = self.upload_audio_files(aligned_files)

        # 生成统计信息
        statistics = self.calculate_statistics(uploaded_files)

        return {
            "audio_files": uploaded_files,
            "voice_mapping": voice_mapping,
            "statistics": statistics
        }
```

**状态复用机制**:

```python
def get_cache_key(self):
    """
    生成缓存键

    基于：翻译文本 + 音色映射
    如果翻译或音色未变，直接返回缓存的语音文件
    """
    segments_hash = hash(
        tuple((s["id"], s["text"], s["speaker"]) for s in self.input_params["segments"])
    )
    voice_mapping_hash = hash(
        tuple(sorted(self.input_params.get("voice_mapping", {}).items()))
    )
    return f"{self.task_id}:edgetts:{segments_hash}:{voice_mapping_hash}"
```

#### 性能分析

**并发性能对比**:

| TTS 系统 | 资源 | 并发度 | 单条耗时 | 总耗时（150 条） |
|---------|------|-------|---------|----------------|
| **IndexTTS2** | GPU | 4 | 5s | ~187s（3 分钟） |
| **Edge-TTS** | CPU | 8 | 2s | ~37s（40 秒） |

**性能优势**:
- **速度快 5 倍**：40 秒 vs 3 分钟
- **无需 GPU**：可在无 GPU 机器部署
- **高并发**：CPU 并发度更高（8-16 vs 4）

**资源消耗**:
- **CPU**：中等负载（8 并发 × 2s）
- **内存**：低（每个任务 ~100MB）
- **网络**：中等（调用 Microsoft Edge-TTS API）

**部署建议**:
```yaml
# docker-compose.yml
services:
  edgetts_worker:
    image: yivideo/edgetts_service
    deploy:
      replicas: 2  # 可部署多个实例
      resources:
        limits:
          cpus: '4.0'  # 4 核 CPU
          memory: 2G
    environment:
      - CELERY_WORKER_CONCURRENCY=8
```

#### IndexTTS2 vs Edge-TTS 选择指南

**使用场景对比**:

| 场景 | 推荐 TTS | 理由 |
|------|---------|------|
| **高质量配音** | IndexTTS2 | 音质更好，可还原原音色 |
| **还原原说话人音色** | IndexTTS2 | 支持参考音克隆 |
| **快速原型验证** | Edge-TTS | 速度快 5 倍，成本低 |
| **资源受限环境** | Edge-TTS | 无需 GPU |
| **GPU 资源紧张** | Edge-TTS | 释放 GPU 给其他任务 |
| **多说话人场景** | IndexTTS2 | 音色还原一致性好 |
| **批量处理** | Edge-TTS | 并发度高，吞吐量大 |

**混合使用策略**:
```python
def select_tts_engine(quality_requirement, gpu_available):
    """
    根据质量要求和资源情况选择 TTS 引擎
    """
    if quality_requirement == "high" and gpu_available:
        return "indextts"
    elif quality_requirement == "medium" or not gpu_available:
        return "edgetts"
    else:
        # 降级策略
        logger.warning("GPU 不可用，使用 Edge-TTS 降级")
        return "edgetts"
```

#### 关键设计决策总结

**决策 1: 一次生成 + Rubberband（不使用 rate 预调整）**
- **原因**: Phase 2 翻译装词已做时长对齐，生成时长应接近目标
- **好处**: 简化流程，减少生成次数

**决策 2: Speaker → Voice 自动映射 + 用户可自定义**
- **原因**: 平衡自动化和灵活性
- **好处**: 开箱即用，同时支持高级定制

**决策 3: CPU 并发度 8-16（vs IndexTTS2 的 4）**
- **原因**: Edge-TTS 是 CPU 任务，无 GPU 资源竞争
- **好处**: 吞吐量提升 5 倍

**决策 4: 与 IndexTTS2 输出格式保持一致**
- **原因**: 下游 Phase 5 视频合并可以无缝切换
- **好处**: 降低系统复杂度，提高可维护性

### 6.5 接口规范
**待设计内容**:
- [ ] 各节点输入/输出数据结构
- [ ] 缓存键策略
- [ ] 错误处理机制

**设计文档**: 待添加

## 7. 重要发现与洞察

### 发现 1: Edge-TTS 不支持精确时长控制
**日期**: 2026-01-17
**内容**: Edge-TTS 已移除自定义 SSML 支持，无法使用 `<prosody duration="Xs">` 精确指定时长，仅能通过 `--rate` 参数调整语速。
**影响**:
- Edge-TTS 必须采用"rate 预调整 + rubberband 后置微调"混合方案
- 无法一步生成精确时长的语音

### 发现 2: IndexTTS2 和 Edge-TTS 均需后处理对齐
**日期**: 2026-01-17
**内容**: 两个 TTS 系统都不支持 SSML duration 控制，必须使用 ffmpeg rubberband 进行后处理时长对齐。
**影响**:
- 增加了处理流程复杂度
- 需要实现 rubberband 优化配置
- 必须考虑音质损失问题

### 发现 3: Rubberband 在 ±10% 范围内几乎无损
**日期**: 2026-01-17
**内容**: 根据调研，rubberband tempo=0.9-1.1（±10%）时音质损失几乎不可感知，使用 `formant=preserved` 对人声尤为重要。
**影响**:
- 翻译装词时应将时长偏差控制在 ±10% 以内
- 必须使用 `formant=preserved` 参数
- 为极端情况预留 ±20% 的可接受范围

### 发现 4: 字幕 CPS 标准对翻译装词至关重要
**日期**: 2026-01-17
**内容**: 行业标准 CPS 为 15-17（最佳）到 20-21（最大），这直接影响翻译装词的字数控制。
**影响**:
- LLM 翻译装词时必须考虑 CPS 约束
- 需要在 system prompt 中明确时长和字数的对应关系
- 不同语言（中英文）需要不同的 CPS 计算方式

### 发现 5: IndexTTS2 支持情感和音色分离控制
**日期**: 2026-01-17
**内容**: IndexTTS2 可以分别控制音色（spk_audio_prompt）和情感（emo_audio_prompt/emo_vector），为高质量语音生成提供了灵活性。
**影响**:
- 可以使用原始说话人的音频作为音色参考
- 可以独立控制情感表达
- 为未来的情感化翻译预留了扩展空间

### 发现 6: 参考音需要重复拼接机制
**日期**: 2026-01-17
**内容**: IndexTTS2 对参考音时长有要求（文档未明确），如果提取的片段时长不足，需要通过重复拼接调整。
**影响**:
- 必须实现参考音时长检测和拼接逻辑
- 同一说话人必须使用同一说话人的参考音
- 需要从原始音频的对应时间区间提取参考音

## 8. 待解决问题

### ✅ 已解决
1. ✅ 确定字幕优化的具体标准（时长、字数）- 见 2.1 字幕时长与字数标准
2. ✅ 确定时长对齐的容差范围 - ±10% 为可接受范围
3. ✅ 确认 Edge-TTS 和 IndexTTS2 的 SSML 支持情况 - 均不支持精确时长控制

### 🔄 待决策
4. [ ] 确定并发处理的窗口大小 - 需结合实际 LLM token 限制测试
5. [ ] 确定 LLM 选择策略（DEEPSEEK/GEMINI/CLAUDE）- 需成本和效果对比
6. [ ] 确定参考音的最小/最佳时长 - 需实测 IndexTTS2
7. [ ] 确定翻译装词的质量评估指标 - 需设计自动化评估方案

### 📋 待设计
8. [x] LLM 字幕优化的指令集具体格式 - 见 4.1 极简指令集规范
9. [x] LLM 翻译装词的 system prompt 具体内容 - 见 6.2 LLM 翻译装词设计
10. [x] 并发处理的重叠窗口策略 - 见 5.1 并发处理策略 和 6.2 并发处理策略
11. [ ] 各节点的详细输入/输出数据结构

## 9. 参考资料

### 项目文档
- YiVideo CLAUDE.md: `/opt/wionch/docker/yivideo/CLAUDE.md`
- 单任务 API 参考: `/opt/wionch/docker/yivideo/docs/technical/reference/SINGLE_TASK_API_REFERENCE.md`

### 外部资源
- Edge-TTS: https://github.com/rany2/edge-tts
- IndexTTS2: https://github.com/index-tts/index-tts

### 待添加
- [ ] 字幕行业标准文档
- [ ] 翻译装词研究论文
- [ ] 语音时长对齐技术文档

---

## 6.5 Phase 5: 视频合并功能设计

### 6.5.1 核心挑战

视频合并的目标是将以下元素整合为最终的 S2ST 视频：
1. TTS 生成的多个音频片段（来自 Phase 3 或 Phase 4）
2. 原视频的背景音轨（BGM、环境音）
3. 无声视频（原视频或静音版本）
4. 翻译后的字幕文件（SRT 格式）

**核心挑战**：
- 音频拼接和时间对齐
- 语音与背景音的音量平衡
- 音画同步验证
- 字幕烧录和样式控制
- 视频编码质量控制

### 6.5.2 音频拼接策略

**方案对比**：

| 方案 | 描述 | 优点 | 缺点 |
|------|------|------|------|
| **方案 A**: 分步处理 | 先拼接 TTS → 再混合背景音 | 逻辑清晰，易调试，可验证中间结果 | 需要临时文件 |
| **方案 B**: 一次性处理 | FFmpeg filter_complex 一步完成 | 内存效率高，无中间文件 | 命令复杂，难以调试 |

**推荐方案**：方案 A（分步处理）

**理由**：
1. 遵循 KISS 原则，逻辑清晰
2. 可在拼接前验证音频完整性
3. 可在混合前调整背景音音量（支持用户配置）
4. YiVideo 已有 MinIO 下载机制，下载成本可接受

**实现步骤**：

步骤 1: 生成拼接列表文件
```bash
# concat_list.txt
file '/app/tmp/task-001/audio_1.wav'
file '/app/tmp/task-001/audio_2.wav'
file '/app/tmp/task-001/audio_3.wav'
...
```

步骤 2: 拼接 TTS 音频片段
```bash
ffmpeg -f concat -safe 0 -i concat_list.txt -c copy merged_voice.wav
```

步骤 3: 混合语音和背景音
```bash
# 简单混合模式（默认）
ffmpeg -i merged_voice.wav -i background.wav \
  -filter_complex "[0:a]volume=1.0[v];[1:a]volume=0.3[b];[v][b]amix=inputs=2:duration=longest" \
  -c:a pcm_s16le final_audio.wav
```

### 6.5.3 音量平衡策略

**音量比例推荐**：
- 语音音轨：100%（volume=1.0）
- 背景音：20-40%（volume=0.2-0.4），默认 30%（0.3）

**动态音量调整（Ducking）** - 高级选项：

当有人说话时，背景音自动降低音量，提升语音清晰度。

```bash
# Ducking 模式（可选）
ffmpeg -i merged_voice.wav -i background.wav \
  -filter_complex "[1:a][0:a]sidechaincompress=threshold=0.1:ratio=3:attack=200:release=1000[bg];[0:a]volume=1.0[v];[v][bg]amix=inputs=2:duration=longest" \
  -c:a pcm_s16le final_audio.wav
```

**参数说明**：
- `threshold=0.1`: 当语音超过 -20dB 时触发压缩
- `ratio=3`: 压缩比 3:1（背景音降低到 1/3）
- `attack=200`: 200ms 响应时间
- `release=1000`: 1000ms 恢复时间

**配置选项**：
```python
{
    "voice_volume": 1.0,        # 语音音量 (0.0-2.0)
    "background_volume": 0.3,   # 背景音音量 (0.0-1.0)
    "enable_ducking": false     # 是否启用动态音量
}
```

**设计原则**：
- 默认使用简单混合（KISS 原则）
- 提供 Ducking 高级选项（满足专业需求）
- 不强制使用高级功能（YAGNI 原则）

### 6.5.4 字幕处理策略

**字幕模式对比**：

| 模式 | 描述 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **软字幕** | 独立轨道，可开关 | 灵活，文件小 | 依赖播放器，渲染不一致 | 个人使用 |
| **硬字幕** | 烧录到画面 | 兼容性强，效果一致 | 需重编码，文件大 | 分享、社交媒体 |

**推荐方案**：默认使用**硬字幕**（burned-in subtitle）

**理由**：
1. S2ST 视频通常用于分享，需要兼容所有平台
2. 翻译字幕是核心内容，不应被关闭
3. 所有播放器显示效果一致

**实现方式**：

软字幕模式：
```bash
ffmpeg -i silent_video.mp4 -i final_audio.wav -i subtitle.srt \
  -c:v libx264 -preset medium -crf 23 \
  -c:a aac -b:a 192k \
  -c:s mov_text \
  -map 0:v -map 1:a -map 2:s \
  output.mp4
```

硬字幕模式（默认）：
```bash
ffmpeg -i silent_video.mp4 -i final_audio.wav \
  -vf "subtitles=subtitle.srt:force_style='Fontsize=20,PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,Outline=2'" \
  -c:v libx264 -preset medium -crf 23 \
  -c:a aac -b:a 192k \
  output.mp4
```

**字幕样式配置**（硬字幕）：
```python
{
    "subtitle_mode": "hard",  # "hard" | "soft" | "both" | "none"
    "subtitle_style": {
        "fontsize": 20,
        "primary_colour": "&H00FFFFFF",  # 白色
        "outline_colour": "&H00000000",  # 黑色边框
        "outline": 2                     # 边框宽度
    }
}
```

**字幕格式支持**：
- 输入：SRT（来自 Phase 2 翻译装词）
- FFmpeg subtitles 滤镜支持：SRT、ASS、SSA
- 自定义样式需要使用 ASS 格式的 `force_style` 参数

### 6.5.5 视频编码策略

**编码器选择**：

| 编码器 | 类型 | 速度 | 质量 | 兼容性 | 推荐场景 |
|--------|------|------|------|--------|----------|
| **libx264** | 软件 | 中等 | 最佳 | 广泛支持 | 默认（兼容性优先） |
| **libx265** | 软件 | 慢 | 优秀 | 较新设备 | 高压缩率需求 |
| **h264_nvenc** | GPU | 快速 | 良好 | 广泛支持 | 速度优先（有 GPU） |

**质量参数（CRF - Constant Rate Factor）**：

CRF 值越小，质量越高，文件越大。

| 质量等级 | H.264 CRF | H.265 CRF | 视觉效果 |
|----------|-----------|-----------|----------|
| **高质量** | 18 | 23 | 几乎无损 |
| **中等质量** | 23 (推荐) | 28 | 视觉无损 |
| **低质量** | 28 | 33 | 轻微失真 |

**预设（Preset）**：

| 预设 | 编码速度 | 压缩率 | 适用场景 |
|------|----------|--------|----------|
| **fast** | 快速 | 较低 | 长视频（>10 分钟） |
| **medium** (推荐) | 中等 | 平衡 | 大多数场景 |
| **slow** | 慢 | 更高 | 短视频 + 质量要求高 |

**音频编码**：
- **AAC**（推荐）：`-c:a aac -b:a 192k`（兼容性最佳）
- **Opus**：`-c:a libopus -b:a 128k`（质量更好，兼容性略差）

**默认配置**：
```bash
-c:v libx264 \
-preset medium \
-crf 23 \
-c:a aac \
-b:a 192k
```

**配置参数**：
```python
{
    "video_codec": "h264",       # "h264" | "h265" | "h264_nvenc"
    "video_quality": "medium",   # "high" (crf=18) | "medium" (crf=23) | "low" (crf=28)
    "video_preset": "medium",    # "fast" | "medium" | "slow"
    "audio_bitrate": "192k"      # "128k" | "192k" | "256k"
}
```

**性能对比**（5 分钟 1080p 视频）：

| 配置 | 编码时间 | 文件大小 | 质量 |
|------|----------|----------|------|
| h264 + medium + crf23 | ~5 分钟 | ~50 MB | 优秀 |
| h264 + fast + crf23 | ~3 分钟 | ~60 MB | 良好 |
| h264_nvenc + fast + crf23 | ~30 秒 | ~55 MB | 良好 |
| h265 + medium + crf28 | ~10 分钟 | ~35 MB | 优秀 |

### 6.5.6 完整处理流程

**4 阶段处理流程**：

```
阶段 1: 音频预处理
  ├─ 下载所有 TTS 音频片段
  ├─ 下载背景音轨（如果提供）
  └─ 验证音频文件完整性

阶段 2: 音频合成
  ├─ 步骤 2.1: 拼接 TTS 音频片段
  │   ├─ 生成 concat_list.txt
  │   └─ ffmpeg concat 拼接
  └─ 步骤 2.2: 混合语音和背景音
      ├─ 简单混合模式（默认）
      └─ Ducking 模式（可选）

阶段 3: 视频合成
  ├─ 步骤 3.1: 准备无声视频
  │   └─ 如果输入视频有音轨，移除音轨
  └─ 步骤 3.2: 合并音频、视频、字幕
      ├─ 软字幕模式：-c:s mov_text
      └─ 硬字幕模式：-vf subtitles

阶段 4: 上传和清理
  ├─ 上传最终视频到 MinIO
  ├─ 提取视频元数据
  └─ 清理临时文件
```

**详细命令示例**：

阶段 1: 音频预处理
```python
# 下载音频文件
for audio_file in audio_files:
    local_path = download_from_minio(audio_file['audio_minio_url'])
    audio_file['local_path'] = local_path

    # 验证文件
    duration = get_audio_duration(local_path)
    if duration == 0:
        raise ValueError(f"Invalid audio file: {audio_file['segment_id']}")
```

阶段 2.1: 拼接 TTS 音频
```bash
# 生成拼接列表
echo "file '/app/tmp/task-001/audio_1.wav'" > concat_list.txt
echo "file '/app/tmp/task-001/audio_2.wav'" >> concat_list.txt
...

# 拼接
ffmpeg -f concat -safe 0 -i concat_list.txt -c copy merged_voice.wav
```

阶段 2.2: 混合音频
```bash
# 简单混合（默认）
ffmpeg -i merged_voice.wav -i background.wav \
  -filter_complex "[0:a]volume=1.0[v];[1:a]volume=0.3[b];[v][b]amix=inputs=2:duration=longest" \
  -c:a pcm_s16le final_audio.wav
```

阶段 3.1: 准备无声视频
```bash
# 如果输入视频有音轨，移除
ffmpeg -i input_video.mp4 -an -c:v copy silent_video.mp4
```

阶段 3.2: 合并视频（硬字幕模式）
```bash
ffmpeg -i silent_video.mp4 -i final_audio.wav \
  -vf "subtitles=subtitle.srt:force_style='Fontsize=20,PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,Outline=2'" \
  -c:v libx264 -preset medium -crf 23 \
  -c:a aac -b:a 192k \
  output.mp4
```

### 6.5.7 音画同步验证

**同步验证机制**：

```python
def validate_audio_sync(audio_files, video_duration):
    """
    验证音频与视频时长同步

    Args:
        audio_files: TTS 音频文件列表
        video_duration: 视频总时长（秒）

    Returns:
        同步验证结果字典
    """
    # 计算 TTS 音频总时长（最后一个片段的结束时间）
    tts_duration = max([seg['end'] for seg in audio_files])

    # 计算时长偏差
    deviation = abs(tts_duration - video_duration)
    deviation_percent = (deviation / video_duration) * 100

    # 质量评估
    if deviation_percent < 0.5:
        sync_quality = "excellent"  # 优秀
    elif deviation_percent < 1.0:
        sync_quality = "good"       # 良好
    elif deviation_percent < 5.0:
        sync_quality = "acceptable" # 可接受
    else:
        sync_quality = "poor"       # 差

    # 记录警告
    if deviation_percent > 1.0:
        logger.warning(
            f"Audio-video sync deviation: {deviation:.2f}s ({deviation_percent:.1f}%)"
        )

    # 超过 5% 偏差，抛出错误
    if deviation_percent > 5.0:
        raise ValueError(
            f"Audio-video sync deviation too large: {deviation:.2f}s ({deviation_percent:.1f}%)"
        )

    return {
        "tts_duration": tts_duration,
        "video_duration": video_duration,
        "deviation": deviation,
        "deviation_percent": deviation_percent,
        "sync_quality": sync_quality
    }
```

**同步质量标准**：

| 偏差范围 | 质量等级 | 处理方式 |
|----------|----------|----------|
| < 0.5% | Excellent（优秀） | 无需调整 |
| 0.5-1.0% | Good（良好） | 记录日志 |
| 1.0-5.0% | Acceptable（可接受） | 记录警告 |
| > 5.0% | Poor（差） | 抛出错误 |

**时长调整策略**（如果需要）：

如果总时长偏差 > 0.5%，调整最后一个音频片段：

```python
if deviation_percent > 0.5:
    last_audio = audio_files[-1]
    adjustment = video_duration - tts_duration

    # 计算 tempo 调整比例
    current_duration = last_audio['end'] - last_audio['start']
    tempo = current_duration / (current_duration + adjustment)

    # 使用 atempo 滤镜调整
    ffmpeg.run([
        'ffmpeg', '-i', last_audio['path'],
        '-af', f'atempo={tempo}',
        last_audio['adjusted_path']
    ])
```

**潜在同步问题及解决**：

| 问题 | 原因 | 解决方案 |
|------|------|----------|
| TTS 总时长 ≠ 视频时长 | Rubberband 累积误差 | 调整最后一个片段 |
| 帧率不匹配 | 不同来源视频 | FFmpeg `-vsync cfr` 自动同步 |
| 背景音时长 < 视频 | 背景音被裁剪 | `amix duration=longest` 自动补零 |

### 6.5.8 数据结构设计

**输入参数** (`input_params`):

```python
{
    # === 必需参数 ===
    "audio_files": [  # 来自 Phase 3/4 TTS 输出
        {
            "segment_id": 1,
            "audio_minio_url": "http://minio:9000/yivideo/task-001/audio_1.wav",
            "start": 0.0,
            "end": 2.5,
            "speaker": "SPEAKER_00"
        },
        {
            "segment_id": 2,
            "audio_minio_url": "http://minio:9000/yivideo/task-001/audio_2.wav",
            "start": 2.5,
            "end": 5.8,
            "speaker": "SPEAKER_01"
        },
        ...
    ],
    "video_minio_url": "http://minio:9000/yivideo/input/video.mp4",  # 原视频或无声视频
    "subtitle_minio_url": "http://minio:9000/yivideo/task-001/subtitle.srt",  # 翻译后字幕

    # === 可选参数 ===
    "background_audio_minio_url": "http://...",  # 背景音（可选，如无则从视频提取）

    # 音频混合配置
    "voice_volume": 1.0,  # 语音音量 (0.0-2.0)，默认 1.0
    "background_volume": 0.3,  # 背景音音量 (0.0-1.0)，默认 0.3
    "enable_ducking": false,  # 是否启用动态音量（Ducking），默认 false

    # 字幕配置
    "subtitle_mode": "hard",  # "hard" (默认) | "soft" | "both" | "none"
    "subtitle_style": {  # 仅对硬字幕生效
        "fontsize": 20,
        "primary_colour": "&H00FFFFFF",  # 白色
        "outline_colour": "&H00000000",  # 黑色边框
        "outline": 2
    },

    # 视频编码配置
    "video_codec": "h264",  # "h264" (默认) | "h265" | "h264_nvenc"
    "video_quality": "medium",  # "high" (crf=18) | "medium" (crf=23, 默认) | "low" (crf=28)
    "video_preset": "medium",  # "fast" | "medium" (默认) | "slow"
    "audio_bitrate": "192k",  # "128k" | "192k" (默认) | "256k"

    # 其他
    "remove_original_audio": true  # 是否移除原视频音轨，默认 true
}
```

**输出结果** (`output`):

```python
{
    # === 主要输出 ===
    "video_minio_url": "http://minio:9000/yivideo/task-001/final_output.mp4",
    "video_path": "/app/tmp/task-001/final_output.mp4",

    # === 视频信息 ===
    "duration": 125.5,  # 秒
    "file_size": 15728640,  # 字节（15 MB）
    "resolution": "1920x1080",
    "fps": 30.0,

    # === 编码信息 ===
    "video_codec": "h264",
    "audio_codec": "aac",
    "video_bitrate": "2000k",
    "audio_bitrate": "192k",

    # === 同步质量 ===
    "sync_validation": {
        "tts_duration": 125.3,
        "video_duration": 125.5,
        "deviation": 0.2,
        "deviation_percent": 0.16,
        "sync_quality": "excellent"
    },

    # === 处理统计 ===
    "processing_time": 45.2,  # 秒
    "audio_segments_count": 150,
    "subtitle_mode": "hard",
    "encoding_preset": "medium",

    # === 可选输出 ===
    "soft_subtitle_video_minio_url": "http://..."  # 仅当 subtitle_mode="both" 时
}
```

**数据结构规范**：
- 所有 MinIO URL 字段以 `_minio_url` 后缀命名
- 所有本地路径字段以 `_path` 后缀命名
- 遵循 YiVideo BaseNodeExecutor 的输出规范

### 6.5.9 错误处理

**错误场景及处理**：

| 错误场景 | 检测方式 | 处理方式 | 恢复策略 |
|----------|----------|----------|----------|
| 音频文件缺失/损坏 | 下载后验证大小和时长 | 记录 segment_id，抛出 ValueError | 无（需重新生成 TTS） |
| 视频音频时长不匹配（>5%） | sync_validation | 抛出 ValueError | 要求用户检查输入 |
| FFmpeg 执行失败 | subprocess 返回码 | 捕获 stderr，记录详细错误 | 关键步骤重试 1 次 |
| 字幕文件格式错误 | FFmpeg subtitles 滤镜报错 | 尝试修复 SRT 文件 | 降级为 subtitle_mode="none" |

**边界情况处理**：

| 边界情况 | 检测方式 | 处理方式 |
|----------|----------|----------|
| 无背景音 | background_audio_minio_url 为空 | 跳过音频混合步骤 |
| audio_files 为空 | validate_input() | 抛出 ValueError |
| 单个音频片段 | len(audio_files) == 1 | 跳过拼接步骤 |
| 视频分辨率 > 4K | 解析视频元数据 | 记录警告（编码慢） |
| 视频分辨率 < 360p | 解析视频元数据 | 记录警告（质量差） |

**错误示例**：

```python
class VideoMergerExecutor(BaseNodeExecutor):

    def validate_input(self):
        """验证输入参数"""
        required_fields = ["audio_files", "video_minio_url", "subtitle_minio_url"]
        for field in required_fields:
            if field not in self.input_params:
                raise ValueError(f"Missing required field: {field}")

        # 验证 audio_files 不为空
        if not self.input_params["audio_files"]:
            raise ValueError("audio_files cannot be empty")

        # 验证音频片段数据完整性
        for audio_file in self.input_params["audio_files"]:
            required_audio_fields = ["segment_id", "audio_minio_url", "start", "end"]
            for field in required_audio_fields:
                if field not in audio_file:
                    raise ValueError(
                        f"Audio file missing field '{field}': segment_id={audio_file.get('segment_id')}"
                    )

    def download_and_validate_audio(self, audio_file):
        """下载并验证音频文件"""
        try:
            local_path = self.download_from_minio(audio_file['audio_minio_url'])

            # 验证文件存在且非空
            if not os.path.exists(local_path) or os.path.getsize(local_path) == 0:
                raise ValueError(f"Invalid audio file: {audio_file['segment_id']}")

            # 验证音频时长
            duration = get_audio_duration(local_path)
            if duration == 0:
                raise ValueError(f"Audio file has zero duration: {audio_file['segment_id']}")

            return local_path

        except Exception as e:
            logger.error(f"Failed to download audio file: {audio_file['segment_id']}", exc_info=True)
            raise

    def merge_video_with_retry(self, video_path, audio_path, subtitle_path):
        """合并视频，失败时重试一次"""
        for attempt in range(2):
            try:
                return self._merge_video_audio_subtitle(video_path, audio_path, subtitle_path)
            except subprocess.CalledProcessError as e:
                logger.error(f"FFmpeg merge failed (attempt {attempt+1}): {e.stderr}", exc_info=True)
                if attempt == 1:  # 第二次失败
                    raise
                time.sleep(1)  # 重试前等待 1 秒
```

**资源清理**：

```python
def cleanup_temp_files(self):
    """
    清理临时文件
    """
    temp_dir = f"/app/tmp/{self.task_id}"

    try:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            logger.info(f"Cleaned up temp directory: {temp_dir}")
    except Exception as e:
        logger.warning(f"Failed to cleanup temp files: {e}")
        # 不抛出异常，清理失败不影响任务成功
```

**超时控制**：

```python
# 超时配置
TIMEOUT_CONFIG = {
    "audio_concat": 300,      # 音频拼接：5 分钟
    "audio_mix": 300,         # 音频混合：5 分钟
    "video_encode": lambda duration: duration * 2 * 60,  # 视频编码：视频时长 × 2
    "total": 1800             # 总超时：30 分钟
}

# 使用示例
subprocess.run(
    ffmpeg_command,
    timeout=TIMEOUT_CONFIG["audio_concat"],
    capture_output=True,
    check=True
)
```

### 6.5.10 YiVideo 架构集成

**BaseNodeExecutor 实现**：

```python
from services.common.base_node_executor import BaseNodeExecutor
from celery import Task
import subprocess
import os
import shutil

class VideoMergerExecutor(BaseNodeExecutor):
    """
    视频合并节点执行器

    功能：将 TTS 音频、背景音、视频、字幕合并为最终 S2ST 视频
    """

    def validate_input(self):
        """验证输入参数"""
        required_fields = ["audio_files", "video_minio_url", "subtitle_minio_url"]
        for field in required_fields:
            if field not in self.input_params:
                raise ValueError(f"Missing required field: {field}")

        if not self.input_params["audio_files"]:
            raise ValueError("audio_files cannot be empty")

    def get_cache_key_fields(self):
        """获取缓存键字段"""
        return [
            "audio_files",
            "video_minio_url",
            "subtitle_minio_url",
            "background_audio_minio_url",
            "voice_volume",
            "background_volume",
            "enable_ducking",
            "subtitle_mode",
            "video_codec",
            "video_quality",
            "video_preset"
        ]

    def execute_core_logic(self):
        """核心执行逻辑"""
        task_id = self.task_id
        temp_dir = f"/app/tmp/{task_id}"
        os.makedirs(temp_dir, exist_ok=True)

        try:
            # 阶段 1: 音频预处理
            logger.info(f"[{task_id}] Stage 1: Downloading audio files...")
            audio_paths = self._download_audio_files()
            background_path = self._download_background_audio()

            # 阶段 2: 音频合成
            logger.info(f"[{task_id}] Stage 2: Merging audio tracks...")
            merged_voice = self._merge_tts_audios(audio_paths)
            final_audio = self._mix_audio_tracks(merged_voice, background_path)

            # 阶段 3: 视频合成
            logger.info(f"[{task_id}] Stage 3: Merging video...")
            video_path = self._download_video()
            subtitle_path = self._download_subtitle()
            output_path = self._merge_video_audio_subtitle(
                video_path, final_audio, subtitle_path
            )

            # 阶段 4: 上传和清理
            logger.info(f"[{task_id}] Stage 4: Uploading result...")
            output_url = self._upload_to_minio(output_path)
            video_info = self._get_video_info(output_path)

            # 同步验证
            sync_validation = self._validate_audio_sync(
                self.input_params["audio_files"],
                video_info["duration"]
            )

            logger.info(f"[{task_id}] Video merge completed successfully")

            return {
                "video_minio_url": output_url,
                "video_path": output_path,
                **video_info,
                "sync_validation": sync_validation,
                "processing_time": time.time() - self.start_time,
                "audio_segments_count": len(self.input_params["audio_files"]),
                "subtitle_mode": self.input_params.get("subtitle_mode", "hard")
            }

        finally:
            # 清理临时文件
            self._cleanup_temp_files(temp_dir)

    def _merge_tts_audios(self, audio_paths):
        """拼接 TTS 音频片段"""
        concat_list_path = f"/app/tmp/{self.task_id}/concat_list.txt"

        # 生成拼接列表文件
        with open(concat_list_path, 'w') as f:
            for path in audio_paths:
                f.write(f"file '{path}'\n")

        # 拼接音频
        merged_path = f"/app/tmp/{self.task_id}/merged_voice.wav"
        subprocess.run([
            'ffmpeg', '-y',
            '-f', 'concat',
            '-safe', '0',
            '-i', concat_list_path,
            '-c', 'copy',
            merged_path
        ], check=True, capture_output=True, timeout=300)

        return merged_path

    def _mix_audio_tracks(self, voice_path, background_path):
        """混合语音和背景音"""
        if not background_path:
            return voice_path  # 无背景音，直接返回语音

        final_audio_path = f"/app/tmp/{self.task_id}/final_audio.wav"
        voice_volume = self.input_params.get("voice_volume", 1.0)
        bg_volume = self.input_params.get("background_volume", 0.3)
        enable_ducking = self.input_params.get("enable_ducking", False)

        if enable_ducking:
            # Ducking 模式
            filter_complex = (
                f"[1:a][0:a]sidechaincompress=threshold=0.1:ratio=3:attack=200:release=1000[bg];"
                f"[0:a]volume={voice_volume}[v];"
                f"[v][bg]amix=inputs=2:duration=longest"
            )
        else:
            # 简单混合模式
            filter_complex = (
                f"[0:a]volume={voice_volume}[v];"
                f"[1:a]volume={bg_volume}[b];"
                f"[v][b]amix=inputs=2:duration=longest"
            )

        subprocess.run([
            'ffmpeg', '-y',
            '-i', voice_path,
            '-i', background_path,
            '-filter_complex', filter_complex,
            '-c:a', 'pcm_s16le',
            final_audio_path
        ], check=True, capture_output=True, timeout=300)

        return final_audio_path

    def _merge_video_audio_subtitle(self, video_path, audio_path, subtitle_path):
        """合并视频、音频、字幕"""
        output_path = f"/app/tmp/{self.task_id}/final_output.mp4"

        subtitle_mode = self.input_params.get("subtitle_mode", "hard")
        video_codec = self.input_params.get("video_codec", "h264")
        video_quality = self.input_params.get("video_quality", "medium")
        video_preset = self.input_params.get("video_preset", "medium")
        audio_bitrate = self.input_params.get("audio_bitrate", "192k")

        # CRF 映射
        crf_map = {"high": 18, "medium": 23, "low": 28}
        crf = crf_map.get(video_quality, 23)

        # 编码器映射
        codec_map = {
            "h264": "libx264",
            "h265": "libx265",
            "h264_nvenc": "h264_nvenc"
        }
        codec = codec_map.get(video_codec, "libx264")

        if subtitle_mode == "hard":
            # 硬字幕模式
            subtitle_style = self.input_params.get("subtitle_style", {})
            fontsize = subtitle_style.get("fontsize", 20)
            primary_colour = subtitle_style.get("primary_colour", "&H00FFFFFF")
            outline_colour = subtitle_style.get("outline_colour", "&H00000000")
            outline = subtitle_style.get("outline", 2)

            vf_param = (
                f"subtitles={subtitle_path}:"
                f"force_style='Fontsize={fontsize},"
                f"PrimaryColour={primary_colour},"
                f"OutlineColour={outline_colour},"
                f"Outline={outline}'"
            )

            subprocess.run([
                'ffmpeg', '-y',
                '-i', video_path,
                '-i', audio_path,
                '-vf', vf_param,
                '-c:v', codec,
                '-preset', video_preset,
                '-crf', str(crf),
                '-c:a', 'aac',
                '-b:a', audio_bitrate,
                output_path
            ], check=True, capture_output=True, timeout=self._calculate_encoding_timeout())

        elif subtitle_mode == "soft":
            # 软字幕模式
            subprocess.run([
                'ffmpeg', '-y',
                '-i', video_path,
                '-i', audio_path,
                '-i', subtitle_path,
                '-c:v', codec,
                '-preset', video_preset,
                '-crf', str(crf),
                '-c:a', 'aac',
                '-b:a', audio_bitrate,
                '-c:s', 'mov_text',
                '-map', '0:v',
                '-map', '1:a',
                '-map', '2:s',
                output_path
            ], check=True, capture_output=True, timeout=self._calculate_encoding_timeout())

        elif subtitle_mode == "none":
            # 无字幕模式
            subprocess.run([
                'ffmpeg', '-y',
                '-i', video_path,
                '-i', audio_path,
                '-c:v', codec,
                '-preset', video_preset,
                '-crf', str(crf),
                '-c:a', 'aac',
                '-b:a', audio_bitrate,
                output_path
            ], check=True, capture_output=True, timeout=self._calculate_encoding_timeout())

        return output_path

    def _calculate_encoding_timeout(self):
        """计算视频编码超时时间"""
        # 默认 30 分钟，或根据视频时长 × 2
        return 1800
```

**Celery 任务定义**：

```python
# services/workers/ffmpeg_service/app/tasks.py

from celery import Task
from app.celery_app import celery_app
from .executors.video_merger_executor import VideoMergerExecutor

@celery_app.task(bind=True, name="ffmpeg.merge_s2st_video")
def merge_s2st_video(self: Task, context: dict) -> dict:
    """
    合并 S2ST 视频任务

    Args:
        context: 工作流上下文，包含 workflow_id 和 input_params

    Returns:
        执行结果字典，包含 video_minio_url 等信息
    """
    executor = VideoMergerExecutor(
        task_id=context.get("workflow_id"),
        input_params=context.get("input_params", {})
    )
    return executor.execute()
```

**性能优化建议**：

| 场景 | 配置 | 预期性能 |
|------|------|----------|
| 短视频（< 5 分钟） | preset=medium + h264 | ~5 分钟编码时间 |
| 长视频（> 10 分钟） | preset=fast + h264 | ~3 分钟编码时间 |
| 有 GPU | preset=fast + h264_nvenc | ~30 秒编码时间 |
| 高压缩率需求 | preset=slow + h265 | ~10 分钟编码时间 |

### 6.5.11 设计总结

**Phase 5 核心要点**：

1. **4 阶段处理流程**：
   - 阶段 1: 音频预处理（下载 + 验证）
   - 阶段 2: 音频合成（拼接 + 混合）
   - 阶段 3: 视频合成（合并音视频字幕）
   - 阶段 4: 上传和清理

2. **音频处理策略**：
   - 分步处理（拼接 → 混合）
   - 简单混合（默认）+ Ducking（可选）
   - 音量比例：voice=1.0, bg=0.3

3. **字幕处理策略**：
   - 默认硬字幕（兼容性最佳）
   - 支持软字幕和无字幕模式
   - 可自定义字幕样式

4. **视频编码策略**：
   - 默认 H.264 + AAC（兼容性优先）
   - 质量等级：high/medium/low（CRF 18/23/28）
   - 预设：fast/medium/slow
   - 支持 GPU 加速（h264_nvenc）

5. **音画同步验证**：
   - 容差范围：< 0.5% 优秀，< 1.0% 良好，< 5.0% 可接受
   - 超过 5% 抛出错误
   - 可调整最后一个片段实现精确对齐

6. **错误处理**：
   - 完善的输入验证
   - 音频文件完整性检查
   - FFmpeg 执行失败重试
   - 边界情况处理（无背景音、单片段等）

7. **架构集成**：
   - 遵循 BaseNodeExecutor 规范
   - 完整的缓存键定义
   - 规范的输入/输出数据结构
   - 符合 YiVideo 架构约束

**设计原则遵循**：
- ✅ **KISS**：分步处理，逻辑清晰
- ✅ **DRY**：复用 BaseNodeExecutor 和工具函数
- ✅ **YAGNI**：默认简单配置，高级功能可选
- ✅ **SOLID**：单一职责，VideoMergerExecutor 只做视频合并

---

## 7. 整体数据流和接口规范

### 7.1 核心数据结构

#### 7.1.1 Segment 标准格式

S2ST 工作流中所有字幕数据都基于 **Segment** 格式：

```json
{
  "id": 1,
  "text": "这是字幕文本",
  "start": 0.0,
  "end": 2.5,
  "speaker": "SPEAKER_00",
  "words": [
    {
      "word": "这是",
      "start": 0.0,
      "end": 0.5
    },
    {
      "word": "字幕",
      "start": 0.6,
      "end": 1.2
    },
    {
      "word": "文本",
      "start": 1.3,
      "end": 2.5
    }
  ]
}
```

**字段说明**：
- `id` (int): 唯一标识符，从 1 开始递增
- `text` (str): 完整字幕文本
- `start` (float): 开始时间（秒）
- `end` (float): 结束时间（秒）
- `speaker` (str): 说话人标识（如 "SPEAKER_00"、"SPEAKER_01"）
- `words` (list): 词级时间戳数组
  - `word` (str): 单词/字符文本
  - `start` (float): 词开始时间（秒）
  - `end` (float): 词结束时间（秒）

**数据来源**：
- 源数据：`faster_whisper.transcribe_audio` 输出的 `transcribe_data_*.json`
- 示例路径：`share/workflows/video_to_subtitle_task/nodes/faster_whisper.transcribe_audio/data/transcribe_data_video_to.json`

#### 7.1.2 完整 S2ST 工作流链

```
Stage 1: extract_audio (ffmpeg)
  ├─ Input: video_path
  └─ Output: audio_file (WAV/MP3)

Stage 2: transcribe_audio (faster_whisper)
  ├─ Input: audio_file
  └─ Output: segments (JSON Array)

Stage 3: optimize_subtitle (subtitle_optimizer) ✨ NEW
  ├─ Input: segments
  └─ Output: optimized_segments

Stage 4: translate_subtitle (subtitle_translator) ✨ NEW
  ├─ Input: optimized_segments + source_lang + target_lang
  └─ Output: translated_segments

Stage 5: generate_speech (indextts OR edgetts) ✨ NEW
  ├─ Input: translated_segments + reference_audio
  └─ Output: audio_clips (List[AudioFile])

Stage 6: merge_s2st_video (ffmpeg) ✨ NEW
  ├─ Input: original_video + audio_clips + subtitle_file
  └─ Output: final_video

Stage 7: upload_to_minio (optional)
  ├─ Input: final_video
  └─ Output: minio_url
```

---

### 7.2 节点接口规范

#### 7.2.1 subtitle_optimizer.optimize_subtitle

**功能**：基于 LLM 的字幕优化（断句、合并、拆分、纠错）

**输入接口 (input_data)**：
```json
{
  "segments_file": "minio://yivideo/task-xxx/segments.json",
  "llm_provider": "deepseek",
  "llm_model": "deepseek-chat",
  "batch_size": 150,
  "overlap_size": 10,
  "custom_instructions": "保留专业术语，避免过度合并"
}
```

**字段说明**：
- `segments_file` (str, required): MinIO URL 或本地路径，指向 Segment 数组 JSON
- `llm_provider` (str, required): LLM 提供商，可选 "deepseek" | "gemini" | "claude"
- `llm_model` (str, optional): 模型名称，默认使用 provider 推荐模型
- `batch_size` (int, optional): 并发窗口大小，默认 150 segments
- `overlap_size` (int, optional): 重叠区域大小，默认 10 segments
- `custom_instructions` (str, optional): 自定义优化指令

**输出接口 (output JSON)**：
```json
{
  "optimized_segments_file": "minio://yivideo/task-xxx/optimized_segments.json",
  "optimized_segments_minio_url": "http://minio:9000/yivideo/task-xxx/optimized_segments.json",
  "statistics": {
    "original_count": 500,
    "optimized_count": 450,
    "merged_count": 50,
    "split_count": 20,
    "corrected_count": 30
  }
}
```

**缓存键字段**：
```python
cache_key_fields = [
    "segments_file",        # 内容哈希
    "llm_provider",
    "llm_model",
    "batch_size",
    "overlap_size",
    "custom_instructions"
]
```

---

#### 7.2.2 subtitle_translator.translate_subtitle

**功能**：考虑时长对齐的字幕翻译（翻译装词）

**输入接口 (input_data)**：
```json
{
  "segments_file": "minio://yivideo/task-xxx/optimized_segments.json",
  "source_lang": "zh",
  "target_lang": "en",
  "llm_provider": "deepseek",
  "llm_model": "deepseek-chat",
  "cps_config": {
    "min_cps": 12,
    "max_cps": 20,
    "optimal_cps": 16
  },
  "batch_size": 100,
  "overlap_size": 6,
  "custom_glossary": {
    "人工智能": "AI",
    "机器学习": "Machine Learning"
  }
}
```

**字段说明**：
- `segments_file` (str, required): 优化后的 Segment 文件
- `source_lang` (str, required): 源语言代码 (ISO 639-1)
- `target_lang` (str, required): 目标语言代码
- `llm_provider` (str, required): LLM 提供商
- `llm_model` (str, optional): 模型名称
- `cps_config` (dict, optional): 字符速率约束
  - `min_cps` (int): 最小每秒字符数
  - `max_cps` (int): 最大每秒字符数
  - `optimal_cps` (int): 最优每秒字符数
- `batch_size` (int, optional): 并发窗口，默认 100
- `overlap_size` (int, optional): 上下文窗口，默认 6
- `custom_glossary` (dict, optional): 术语表（源词 → 目标词）

**输出接口 (output JSON)**：
```json
{
  "translated_segments_file": "minio://yivideo/task-xxx/translated_segments.json",
  "translated_segments_minio_url": "http://minio:9000/yivideo/task-xxx/translated_segments.json",
  "statistics": {
    "segment_count": 450,
    "avg_cps": 16.5,
    "cps_violations": 12,
    "avg_duration_match": 0.95
  }
}
```

**缓存键字段**：
```python
cache_key_fields = [
    "segments_file",        # 内容哈希
    "source_lang",
    "target_lang",
    "llm_provider",
    "llm_model",
    "cps_config",
    "batch_size",
    "overlap_size",
    "custom_glossary"
]
```

---

#### 7.2.3 indextts.generate_speech

**功能**：基于 IndexTTS2 的语音生成与时长对齐

**输入接口 (input_data)**：
```json
{
  "segments_file": "minio://yivideo/task-xxx/translated_segments.json",
  "reference_audio": "minio://yivideo/task-xxx/original_audio.wav",
  "speaker_mapping": {
    "SPEAKER_00": "voice_001",
    "SPEAKER_01": "voice_002"
  },
  "reference_duration": 5.0,
  "rubberband_threshold": 0.1,
  "max_tempo_change": 0.1,
  "concat_method": "crossfade",
  "crossfade_duration": 0.1
}
```

**字段说明**：
- `segments_file` (str, required): 翻译后的 Segment 文件
- `reference_audio` (str, required): 参考音频（用于音色克隆）
- `speaker_mapping` (dict, optional): 说话人 → 音色 ID 映射
- `reference_duration` (float, optional): 参考音时长（秒），默认 5.0
- `rubberband_threshold` (float, optional): 时长对齐阈值，默认 0.1 (±10%)
- `max_tempo_change` (float, optional): Rubberband 最大变速比，默认 0.1
- `concat_method` (str, optional): 拼接方法 "crossfade" | "direct"，默认 "crossfade"
- `crossfade_duration` (float, optional): 交叉淡化时长（秒），默认 0.1

**输出接口 (output JSON)**：
```json
{
  "audio_clips": [
    {
      "segment_id": 1,
      "audio_file": "minio://yivideo/task-xxx/audio/seg_001.wav",
      "audio_file_minio_url": "http://minio:9000/yivideo/task-xxx/audio/seg_001.wav",
      "duration": 2.5,
      "target_duration": 2.5,
      "tempo_ratio": 1.0,
      "speaker": "SPEAKER_00"
    }
  ],
  "statistics": {
    "total_segments": 450,
    "avg_tempo_ratio": 1.02,
    "max_tempo_ratio": 1.08,
    "alignment_accuracy": 0.98
  }
}
```

**缓存键字段**：
```python
cache_key_fields = [
    "segments_file",        # 内容哈希
    "reference_audio",      # 内容哈希
    "speaker_mapping",
    "reference_duration",
    "rubberband_threshold",
    "max_tempo_change",
    "concat_method",
    "crossfade_duration"
]
```

---

#### 7.2.4 edgetts.generate_speech

**功能**：基于 Edge-TTS 的快速语音生成

**输入接口 (input_data)**：
```json
{
  "segments_file": "minio://yivideo/task-xxx/translated_segments.json",
  "voice_mapping": {
    "SPEAKER_00": "zh-CN-XiaoxiaoNeural",
    "SPEAKER_01": "zh-CN-YunxiNeural"
  },
  "rubberband_threshold": 0.1,
  "max_tempo_change": 0.1,
  "pitch_shift": 0,
  "rate": "+0%",
  "volume": "+0%"
}
```

**字段说明**：
- `segments_file` (str, required): 翻译后的 Segment 文件
- `voice_mapping` (dict, required): 说话人 → Edge-TTS 音色映射
- `rubberband_threshold` (float, optional): 时长对齐阈值，默认 0.1
- `max_tempo_change` (float, optional): Rubberband 最大变速比，默认 0.1
- `pitch_shift` (int, optional): 音高偏移（半音），默认 0
- `rate` (str, optional): SSML rate 参数，默认 "+0%"
- `volume` (str, optional): SSML volume 参数，默认 "+0%"

**输出接口 (output JSON)**：
```json
{
  "audio_clips": [
    {
      "segment_id": 1,
      "audio_file": "minio://yivideo/task-xxx/audio/seg_001.wav",
      "audio_file_minio_url": "http://minio:9000/yivideo/task-xxx/audio/seg_001.wav",
      "duration": 2.5,
      "target_duration": 2.5,
      "tempo_ratio": 1.0,
      "speaker": "SPEAKER_00",
      "voice": "zh-CN-XiaoxiaoNeural"
    }
  ],
  "statistics": {
    "total_segments": 450,
    "avg_tempo_ratio": 1.01,
    "max_tempo_ratio": 1.05,
    "alignment_accuracy": 0.99
  }
}
```

**缓存键字段**：
```python
cache_key_fields = [
    "segments_file",        # 内容哈希
    "voice_mapping",
    "rubberband_threshold",
    "max_tempo_change",
    "pitch_shift",
    "rate",
    "volume"
]
```

---

#### 7.2.5 ffmpeg.merge_s2st_video

**功能**：音视频字幕合并（S2ST 最终输出）

**输入接口 (input_data)**：
```json
{
  "video_path": "minio://yivideo/task-xxx/original_video.mp4",
  "audio_clips": [
    {
      "segment_id": 1,
      "audio_file": "minio://yivideo/task-xxx/audio/seg_001.wav",
      "start": 0.0,
      "end": 2.5
    }
  ],
  "subtitle_file": "minio://yivideo/task-xxx/translated_segments.json",
  "subtitle_mode": "burned",
  "background_audio_mix": 0.2,
  "subtitle_style": {
    "font": "Arial",
    "font_size": 24,
    "primary_color": "&H00FFFFFF",
    "outline_color": "&H00000000",
    "position": "bottom"
  },
  "output_format": "mp4",
  "video_codec": "libx264",
  "audio_codec": "aac"
}
```

**字段说明**：
- `video_path` (str, required): 原始视频文件
- `audio_clips` (list, required): 语音片段列表（带时间戳）
- `subtitle_file` (str, optional): 字幕文件（Segment JSON 或 SRT）
- `subtitle_mode` (str, optional): 字幕模式
  - "burned": 硬字幕（烧录到视频）
  - "soft": 软字幕（内嵌字幕流）
  - "both": 同时生成硬字幕和软字幕
  - "none": 不添加字幕
- `background_audio_mix` (float, optional): 背景音混合比例（0.0-1.0），默认 0.2
- `subtitle_style` (dict, optional): 字幕样式配置（仅硬字幕）
- `output_format` (str, optional): 输出格式，默认 "mp4"
- `video_codec` (str, optional): 视频编码器，默认 "libx264"
- `audio_codec` (str, optional): 音频编码器，默认 "aac"

**输出接口 (output JSON)**：
```json
{
  "output_video": "minio://yivideo/task-xxx/final_video.mp4",
  "output_video_minio_url": "http://minio:9000/yivideo/task-xxx/final_video.mp4",
  "soft_subtitle_file": "minio://yivideo/task-xxx/subtitles.srt",
  "soft_subtitle_file_minio_url": "http://minio:9000/yivideo/task-xxx/subtitles.srt",
  "statistics": {
    "duration": 120.5,
    "audio_sync_accuracy": 0.99,
    "file_size_mb": 45.2
  }
}
```

**缓存键字段**：
```python
cache_key_fields = [
    "video_path",           # 内容哈希
    "audio_clips",          # 序列化后哈希
    "subtitle_file",        # 内容哈希（如果提供）
    "subtitle_mode",
    "background_audio_mix",
    "subtitle_style",
    "output_format",
    "video_codec",
    "audio_codec"
]
```

---

### 7.3 缓存键设计策略

#### 7.3.1 文件输入的哈希计算

对于所有文件输入（如 `segments_file`, `video_path`, `audio_file`），使用内容哈希而非路径：

```python
from services.common.state_manager import StateManager

def compute_cache_key_fields(self, context: dict) -> dict:
    """计算缓存键字段（包含文件内容哈希）"""
    input_data = context["input_params"]
    cache_fields = {}

    # 文件字段需要计算内容哈希
    file_fields = ["segments_file", "video_path", "reference_audio"]

    for field in self.get_cache_key_fields():
        if field in file_fields and field in input_data:
            # 下载文件并计算哈希
            file_path = self.download_if_needed(input_data[field])
            cache_fields[field] = StateManager.compute_file_hash(file_path)
        else:
            cache_fields[field] = input_data.get(field)

    return cache_fields
```

**优势**：
- 同一文件内容，即使路径不同也能命中缓存
- 避免路径变化导致的缓存失效

#### 7.3.2 复杂参数的序列化

对于字典/列表类型参数（如 `cps_config`, `speaker_mapping`, `audio_clips`），使用 JSON 序列化：

```python
import json

def serialize_cache_field(value):
    """序列化复杂参数"""
    if isinstance(value, (dict, list)):
        return json.dumps(value, sort_keys=True, ensure_ascii=False)
    return str(value)
```

**示例**：
```python
# 原始参数
cps_config = {"min_cps": 12, "max_cps": 20, "optimal_cps": 16}

# 序列化后
cache_key_field = '{"max_cps":20,"min_cps":12,"optimal_cps":16}'
```

---

### 7.4 完整 S2ST 工作流配置示例

```json
{
  "workflow_id": "s2st-translation-workflow",
  "stages": [
    {
      "stage_name": "extract_audio",
      "task_name": "ffmpeg.extract_audio",
      "input_mapping": {
        "video_path": "{{input.video_path}}"
      }
    },
    {
      "stage_name": "transcribe_audio",
      "task_name": "faster_whisper.transcribe_audio",
      "input_mapping": {
        "audio_file": "{{stages.extract_audio.audio_file}}"
      }
    },
    {
      "stage_name": "optimize_subtitle",
      "task_name": "subtitle_optimizer.optimize_subtitle",
      "input_mapping": {
        "segments_file": "{{stages.transcribe_audio.transcribe_data_file}}",
        "llm_provider": "{{input.llm_provider}}",
        "batch_size": 150,
        "overlap_size": 10
      }
    },
    {
      "stage_name": "translate_subtitle",
      "task_name": "subtitle_translator.translate_subtitle",
      "input_mapping": {
        "segments_file": "{{stages.optimize_subtitle.optimized_segments_file}}",
        "source_lang": "{{input.source_lang}}",
        "target_lang": "{{input.target_lang}}",
        "llm_provider": "{{input.llm_provider}}",
        "cps_config": {
          "min_cps": 12,
          "max_cps": 20,
          "optimal_cps": 16
        }
      }
    },
    {
      "stage_name": "generate_speech",
      "task_name": "{{input.tts_engine}}.generate_speech",
      "input_mapping": {
        "segments_file": "{{stages.translate_subtitle.translated_segments_file}}",
        "reference_audio": "{{stages.extract_audio.audio_file}}",
        "speaker_mapping": "{{input.speaker_mapping}}"
      }
    },
    {
      "stage_name": "merge_video",
      "task_name": "ffmpeg.merge_s2st_video",
      "input_mapping": {
        "video_path": "{{input.video_path}}",
        "audio_clips": "{{stages.generate_speech.audio_clips}}",
        "subtitle_file": "{{stages.translate_subtitle.translated_segments_file}}",
        "subtitle_mode": "burned",
        "background_audio_mix": 0.2
      }
    },
    {
      "stage_name": "upload_result",
      "task_name": "minio.upload_file",
      "input_mapping": {
        "file_path": "{{stages.merge_video.output_video}}",
        "bucket": "yivideo-output"
      }
    }
  ]
}
```

**工作流输入参数**：
```json
{
  "video_path": "minio://yivideo/input/demo.mp4",
  "source_lang": "zh",
  "target_lang": "en",
  "llm_provider": "deepseek",
  "tts_engine": "indextts",
  "speaker_mapping": {
    "SPEAKER_00": "voice_001",
    "SPEAKER_01": "voice_002"
  }
}
```

---

### 7.5 数据流图

```
┌─────────────────┐
│  原始视频文件   │
│  (video.mp4)    │
└────────┬────────┘
         │
         ▼
┌─────────────────────────────────┐
│ Stage 1: ffmpeg.extract_audio   │
│ Output: audio.wav               │
└────────┬────────────────────────┘
         │
         ▼
┌──────────────────────────────────────┐
│ Stage 2: faster_whisper.transcribe   │
│ Output: segments[] with timestamps   │
│         (id, text, start, end,       │
│          speaker, words[])           │
└────────┬─────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│ Stage 3: subtitle_optimizer.optimize    │
│ Input: segments[]                       │
│ LLM: 断句/合并/拆分/纠错                │
│ Output: optimized_segments[]            │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│ Stage 4: subtitle_translator.translate  │
│ Input: optimized_segments[]             │
│ LLM: 翻译装词 + 时长对齐                │
│ Output: translated_segments[]           │
│         (保留时间戳 + 新 text)          │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│ Stage 5: TTS.generate_speech            │
│ Input: translated_segments[] +          │
│        reference_audio                  │
│ Process: 参考音提取 → 生成语音 →       │
│          Rubberband 对齐                │
│ Output: audio_clips[] with timestamps   │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│ Stage 6: ffmpeg.merge_s2st_video        │
│ Input: original_video +                 │
│        audio_clips[] +                  │
│        subtitle_file                    │
│ Process: 音频拼接 → 混合背景音 →       │
│          烧录字幕 → 编码输出            │
│ Output: final_video.mp4                 │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────┐
│  最终 S2ST 视频 │
│  (output.mp4)   │
└─────────────────┘
```

**关键数据传递**：
1. `segments[]` 从 ASR → Optimizer → Translator 一直保留时间戳
2. `translated_segments[]` 包含翻译文本 + 原始时间戳
3. `audio_clips[]` 包含生成音频 + 时间戳 + tempo_ratio
4. 最终视频合并时使用 `audio_clips.start/end` 精确对齐

---

### 7.6 输入输出验证规则

#### 7.6.1 Segment 数组验证

所有处理 Segment 数组的节点必须验证：

```python
def validate_segments(segments: list) -> bool:
    """验证 Segment 数组格式"""
    required_fields = ["id", "text", "start", "end", "speaker", "words"]

    for seg in segments:
        # 检查必需字段
        if not all(field in seg for field in required_fields):
            raise ValueError(f"Segment {seg.get('id')} missing required fields")

        # 检查时间戳有效性
        if seg["start"] < 0 or seg["end"] <= seg["start"]:
            raise ValueError(f"Invalid timestamps in segment {seg['id']}")

        # 检查 words 时间戳
        for word in seg["words"]:
            if word["start"] < seg["start"] or word["end"] > seg["end"]:
                raise ValueError(f"Word timestamps out of segment range in segment {seg['id']}")

    return True
```

#### 7.6.2 音频片段验证

TTS 节点和视频合并节点必须验证 `audio_clips` 格式：

```python
def validate_audio_clips(audio_clips: list) -> bool:
    """验证音频片段列表"""
    for clip in audio_clips:
        required_fields = ["segment_id", "audio_file", "duration", "target_duration"]

        if not all(field in clip for field in required_fields):
            raise ValueError(f"Audio clip {clip.get('segment_id')} missing required fields")

        # 检查音频文件存在性
        if not os.path.exists(clip["audio_file"]) and not clip["audio_file"].startswith("minio://"):
            raise ValueError(f"Audio file not found: {clip['audio_file']}")

        # 检查时长对齐精度
        if "tempo_ratio" in clip and abs(clip["tempo_ratio"] - 1.0) > 0.15:
            logger.warning(f"Segment {clip['segment_id']} has large tempo change: {clip['tempo_ratio']}")

    return True
```

#### 7.6.3 工作流输入验证

API Gateway 必须验证工作流输入参数：

```python
def validate_s2st_workflow_input(input_params: dict) -> bool:
    """验证 S2ST 工作流输入"""
    required_fields = ["video_path", "source_lang", "target_lang", "llm_provider", "tts_engine"]

    if not all(field in input_params for field in required_fields):
        raise ValueError("Missing required workflow parameters")

    # 验证语言代码
    valid_langs = ["zh", "en", "ja", "ko", "es", "fr", "de", "ru"]
    if input_params["source_lang"] not in valid_langs:
        raise ValueError(f"Invalid source_lang: {input_params['source_lang']}")
    if input_params["target_lang"] not in valid_langs:
        raise ValueError(f"Invalid target_lang: {input_params['target_lang']}")

    # 验证 LLM 提供商
    valid_llm_providers = ["deepseek", "gemini", "claude"]
    if input_params["llm_provider"] not in valid_llm_providers:
        raise ValueError(f"Invalid llm_provider: {input_params['llm_provider']}")

    # 验证 TTS 引擎
    valid_tts_engines = ["indextts", "edgetts"]
    if input_params["tts_engine"] not in valid_tts_engines:
        raise ValueError(f"Invalid tts_engine: {input_params['tts_engine']}")

    return True
```

---

### 7.7 设计原则遵循

**KISS (保持简单)**：
- ✅ 所有节点接口统一使用 `input_data` + `output JSON` 模式
- ✅ Segment 格式简单明了，直接复用 Faster-Whisper 输出
- ✅ 缓存键基于内容哈希，逻辑清晰

**DRY (拒绝重复)**：
- ✅ 所有节点复用 `BaseNodeExecutor` 抽象
- ✅ 文件哈希计算复用 `StateManager.compute_file_hash()`
- ✅ 验证逻辑提取为独立函数

**YAGNI (拒绝过度设计)**：
- ✅ 仅定义当前 S2ST 工作流所需接口
- ✅ 高级功能（如自定义术语表）设为可选参数
- ✅ 不预留"未来可能需要"的字段

**SOLID**：
- ✅ **单一职责**：每个节点只负责一项功能（优化/翻译/TTS/合并）
- ✅ **开闭原则**：通过 `input_mapping` 动态配置，无需修改节点代码
- ✅ **依赖倒置**：所有节点依赖抽象 `BaseNodeExecutor`，不依赖具体实现

---

**设计完成日期**：2026-01-18
**设计作者**：Claude (基于 Sequential Thinking 分析)
**文档版本**：v1.0

---

## 8. LLM API 配置方案

### 8.1 Provider 选择策略

#### 8.1.1 支持的 LLM Providers

| Provider | 优势 | 劣势 | 推荐场景 |
|----------|------|------|---------|
| **DeepSeek** | • 成本低（$0.001/1M tokens）<br>• 中文理解强<br>• 速度快 | • API 稳定性相对较低<br>• 配额限制 | 大批量翻译<br>成本敏感场景 |
| **Gemini** | • 免费额度大（15 RPM）<br>• 多语言强<br>• 稳定性好 | • 中文理解稍弱 | 开发测试<br>中等规模生产 |
| **Claude** | • 指令遵循最好<br>• 输出格式最稳定 | • 成本高（$15/1M tokens）<br>• 速度较慢 | 高质量要求<br>关键场景 |

#### 8.1.2 默认策略

```yaml
default_strategy:
  primary_provider: "deepseek"
  fallback_sequence: ["deepseek", "gemini"]
  auto_fallback_enabled: true
```

**决策理由**：
- DeepSeek 成本低且中文支持好，适合 S2ST 场景
- Gemini 作为 fallback 提供高可用性
- Claude 作为可选项（用户指定时启用）

---

### 8.2 Provider 配置详情

#### 8.2.1 DeepSeek 配置

```python
DEEPSEEK_CONFIG = {
    "model": "deepseek-chat",
    "base_url": "https://api.deepseek.com/v1",
    "api_key_env": "DEEPSEEK_API_KEY",

    # Phase 1 (字幕优化) 参数
    "phase_1_params": {
        "temperature": 0.1,         # 低温度，减少随机性
        "max_tokens": 4000,         # 指令集返回，token 少
        "top_p": 0.95,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0
    },

    # Phase 2 (翻译装词) 参数
    "phase_2_params": {
        "temperature": 0.3,         # 稍高温度，允许创造性
        "max_tokens": 8000,         # 翻译文本更长
        "top_p": 0.95,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0
    },

    # Rate Limits
    "rate_limits": {
        "rpm": 60,                  # 每分钟请求数
        "tpm": 100000,              # 每分钟 token 数
        "max_concurrent": 10        # 最大并发请求
    }
}
```

**参数说明**：
- `temperature`: 0.1 确保 Phase 1 指令格式准确，0.3 允许 Phase 2 翻译灵活性
- `max_tokens`: 根据实际输出长度设置（Phase 1 指令集短，Phase 2 翻译长）
- `rpm/tpm`: 官方限制，需要客户端控制

#### 8.2.2 Gemini 配置

```python
GEMINI_CONFIG = {
    "model": "gemini-2.0-flash-exp",  # 免费且快速
    "api_key_env": "GEMINI_API_KEY",

    # Phase 1 (字幕优化) 参数
    "phase_1_params": {
        "temperature": 0.1,
        "max_output_tokens": 4000,
        "top_p": 0.95,
        "top_k": 40
    },

    # Phase 2 (翻译装词) 参数
    "phase_2_params": {
        "temperature": 0.3,
        "max_output_tokens": 8000,
        "top_p": 0.95,
        "top_k": 40
    },

    # Rate Limits (免费版)
    "rate_limits": {
        "rpm": 15,                  # 免费版限制
        "tpm": 1000000,
        "max_concurrent": 5
    }
}
```

**Gemini 特殊配置**：
- 使用 `max_output_tokens` 而非 `max_tokens`
- 支持 `top_k` 参数（Token 采样限制）
- 免费版 RPM 较低，需要更保守的并发策略

#### 8.2.3 Claude 配置（可选）

```python
CLAUDE_CONFIG = {
    "model": "claude-3-5-sonnet-20241022",
    "base_url": "https://api.anthropic.com/v1",
    "api_key_env": "CLAUDE_API_KEY",

    # Phase 1 参数
    "phase_1_params": {
        "temperature": 0.0,         # Claude 建议 0.0 用于结构化输出
        "max_tokens": 4000,
        "top_p": 0.95
    },

    # Phase 2 参数
    "phase_2_params": {
        "temperature": 0.3,
        "max_tokens": 8000,
        "top_p": 0.95
    },

    # Rate Limits
    "rate_limits": {
        "rpm": 50,
        "tpm": 40000,
        "max_concurrent": 5
    }
}
```

---

### 8.3 重试策略

#### 8.3.1 三层重试机制

**Layer 1: 单次请求重试（指数退避）**

```python
RETRY_CONFIG = {
    "max_retries": 3,
    "initial_delay": 1.0,           # 首次重试等待 1 秒
    "backoff_factor": 2.0,          # 每次翻倍（1s, 2s, 4s）
    "max_delay": 10.0,              # 最大等待 10 秒
    "jitter": True,                 # 添加随机抖动避免雷击

    # 需要重试的错误类型
    "retry_on_errors": [
        "timeout",
        "rate_limit",
        "server_error",             # 5xx
        "network_error",
        "connection_error"
    ],

    # 不重试的错误类型（立即失败）
    "no_retry_errors": [
        "invalid_api_key",
        "authentication_failed",
        "content_filtered",
        "quota_exceeded",
        "invalid_request"           # 4xx (除 429)
    ]
}
```

**退避计算逻辑**：
```python
def calculate_backoff_delay(attempt: int) -> float:
    """计算重试延迟（带抖动）"""
    delay = min(
        RETRY_CONFIG["initial_delay"] * (RETRY_CONFIG["backoff_factor"] ** attempt),
        RETRY_CONFIG["max_delay"]
    )
    if RETRY_CONFIG["jitter"]:
        delay = delay * (0.5 + random.random())  # 50%-150% 随机抖动
    return delay
```

**Layer 2: Provider 切换（Fallback）**

```python
FALLBACK_STRATEGY = {
    "enabled": True,
    "sequence": ["deepseek", "gemini"],  # DeepSeek 失败 → Gemini
    "max_fallback_attempts": 2,

    # 触发 fallback 的错误类型
    "fallback_on_errors": [
        "quota_exceeded",
        "all_retries_failed",
        "service_unavailable",
        "provider_outage"
    ],

    # Fallback 后的行为
    "stick_to_fallback": False,         # 失败后不永久切换
    "fallback_cooldown": 300.0          # 5 分钟后尝试恢复主 Provider
}
```

**Layer 3: 批次降级（Batch Size 自适应）**

```python
BATCH_DEGRADATION = {
    "enabled": True,
    "trigger_on": ["rate_limit", "timeout"],

    # 降级步骤
    "degradation_steps": [150, 100, 50, 25],  # 逐步减小 batch_size
    "min_batch_size": 10,                     # 最小批次大小

    # 恢复策略
    "recovery_wait": 60.0,                    # 降级后等待 60 秒
    "gradual_recovery": True,                 # 逐步恢复而非直接恢复
    "recovery_step_duration": 300.0           # 每 5 分钟恢复一级
}
```

---

### 8.4 并发控制和速率限制

#### 8.4.1 并发请求控制

```python
CONCURRENCY_CONFIG = {
    "deepseek": {
        "max_concurrent_requests": 10,
        "request_interval": 1.0,        # 请求间隔 1 秒（确保不超 60 RPM）
        "semaphore_timeout": 30.0,      # 等待 semaphore 最多 30 秒

        # Token Bucket 算法
        "token_bucket": {
            "capacity": 100000,         # Token bucket 容量
            "refill_rate": 1667,        # 每秒补充 token (100000/60)
            "min_tokens_required": 1000 # 最少需要 1000 tokens 才发起请求
        }
    },

    "gemini": {
        "max_concurrent_requests": 5,
        "request_interval": 4.0,        # 每 4 秒一个请求（15 RPM）
        "semaphore_timeout": 60.0,

        "token_bucket": {
            "capacity": 1000000,
            "refill_rate": 16667,       # 每秒补充 (1000000/60)
            "min_tokens_required": 1000
        }
    },

    "claude": {
        "max_concurrent_requests": 5,
        "request_interval": 1.2,        # 每 1.2 秒一个请求（50 RPM）
        "semaphore_timeout": 60.0,

        "token_bucket": {
            "capacity": 40000,
            "refill_rate": 667,
            "min_tokens_required": 1000
        }
    }
}
```

#### 8.4.2 批次调度策略

```python
SCHEDULING_STRATEGY = {
    # 跨阶段调度
    "phase_mode": "sequential",         # Phase 1 → Phase 2 顺序执行

    # 批次内调度
    "batch_mode": "concurrent",         # 批次内并发请求
    "max_batch_concurrency": None,      # 跟随 provider 的 max_concurrent

    # 重叠区域处理
    "overlap_handling": "sequential",   # 重叠区域顺序处理（避免冲突）

    # 优先级队列
    "priority": "fifo",                 # 先进先出
    "allow_priority_override": False
}
```

---

### 8.5 超时和监控

#### 8.5.1 超时配置

```python
TIMEOUT_CONFIG = {
    "connect_timeout": 10.0,            # 连接超时 10 秒
    "read_timeout": 60.0,               # 读取超时 60 秒
    "total_timeout": 120.0,             # 总超时 120 秒

    # 流式输出（S2ST 场景不需要）
    "streaming": {
        "enabled": False,
        "chunk_timeout": 5.0
    },

    # 不同阶段的超时倍率
    "phase_timeout_multiplier": {
        "phase_1": 1.0,                 # Phase 1 使用基础超时
        "phase_2": 1.5                  # Phase 2 超时 × 1.5（翻译更慢）
    }
}
```

#### 8.5.2 监控配置

```python
MONITORING_CONFIG = {
    "metrics": {
        "enabled": True,
        "track": [
            "request_count",            # 总请求数
            "success_rate",             # 成功率
            "avg_latency",              # 平均延迟
            "p95_latency",              # P95 延迟
            "p99_latency",              # P99 延迟
            "token_usage",              # Token 消耗
            "error_rate_by_type",       # 按错误类型统计
            "fallback_trigger_count",   # Fallback 触发次数
            "batch_degradation_count"   # 批次降级次数
        ],
        "aggregation_window": 60.0      # 每 60 秒聚合一次
    },

    "logging": {
        "level": "INFO",
        "log_request_body": False,      # 避免泄露敏感数据
        "log_response_body": False,
        "log_errors": True,
        "log_retries": True,
        "log_fallbacks": True,
        "log_token_usage": True
    },

    "alerting": {
        "enabled": True,
        "channels": ["log", "redis"],   # 告警渠道
        "alert_on": [
            {
                "metric": "error_rate",
                "threshold": 0.1,       # 错误率 > 10%
                "window": 300           # 5 分钟窗口
            },
            {
                "metric": "quota_remaining",
                "threshold": 0.2,       # 配额剩余 < 20%
                "provider": "all"
            },
            {
                "metric": "avg_latency",
                "threshold": 10.0,      # 平均延迟 > 10 秒
                "window": 300
            }
        ]
    }
}
```

---

### 8.6 成本估算

#### 8.6.1 Token 消耗估算

**假设场景：2 小时视频（约 1000 segments）**

**Phase 1 字幕优化**：
```
批次数 = ceil(1000 / (150 - 10)) = 8 批次

每批次输入 token:
  - system prompt: ~800 tokens
  - 150 segments × 50 tokens/segment = ~7500 tokens
  - 总输入: ~8300 tokens × 8 批次 = 66,400 tokens

每批次输出 token (指令集):
  - 平均 50 条指令 × 30 tokens/指令 = ~1500 tokens
  - 总输出: ~1500 tokens × 8 批次 = 12,000 tokens

Phase 1 总消耗: 66,400 input + 12,000 output ≈ 78,400 tokens
```

**Phase 2 翻译装词**：
```
批次数 = ceil(1000 / (100 - 6)) = 11 批次

每批次输入 token:
  - system prompt: ~1000 tokens
  - 100 segments (中文) × 60 tokens/segment = ~6000 tokens
  - 上下文 6 segments × 60 tokens = ~360 tokens
  - 总输入: ~7360 tokens × 11 批次 = 80,960 tokens

每批次输出 token (英文翻译):
  - 100 segments × 80 tokens/segment (英文更长) = ~8000 tokens
  - 总输出: ~8000 tokens × 11 批次 = 88,000 tokens

Phase 2 总消耗: 80,960 input + 88,000 output ≈ 168,960 tokens
```

**单视频总消耗**：
```
Total tokens ≈ 78,400 + 168,960 = 247,360 tokens (≈ 0.25M tokens)
```

#### 8.6.2 成本对比

| Provider | 定价（$/1M tokens） | 单视频成本 | 1000 视频/月成本 |
|----------|-------------------|-----------|----------------|
| **DeepSeek** | $0.001 | $0.00025 (≈ ¥0.0018) | $0.25 (≈ ¥1.8) |
| **Gemini** | 免费（在额度内） | $0 | $0 |
| **Claude** | $15.00 | $0.00375 (≈ ¥0.027) | $3.75 (≈ ¥27) |

**结论**：
- ✅ DeepSeek 成本极低，单视频不到 0.002 元
- ✅ Gemini 免费但有 RPM 限制（15 RPM ≈ 单日处理约 21,600 批次）
- ⚠️ Claude 成本是 DeepSeek 的 15 倍，仅推荐关键场景

#### 8.6.3 优化建议

1. **默认 DeepSeek + Gemini Fallback**：兼顾成本和可用性
2. **批次大小优化**：根据 token 消耗动态调整（避免超 max_tokens）
3. **缓存策略**：相同 segments_file + 相同参数 → 命中缓存，零成本

---

### 8.7 统一客户端抽象

#### 8.7.1 BaseLLMClient 接口

```python
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any

class BaseLLMClient(ABC):
    """LLM 客户端抽象基类"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = os.getenv(config["api_key_env"])
        if not self.api_key:
            raise ValueError(f"API key not found: {config['api_key_env']}")

    @abstractmethod
    def complete(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 4000,
        **kwargs
    ) -> str:
        """同步完成请求

        Args:
            system_prompt: 系统提示词
            user_prompt: 用户提示词
            temperature: 温度参数
            max_tokens: 最大输出 token 数
            **kwargs: 其他 provider 特定参数

        Returns:
            str: LLM 返回的文本

        Raises:
            LLMError: LLM 调用失败
        """
        pass

    @abstractmethod
    async def acomplete(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 4000,
        **kwargs
    ) -> str:
        """异步完成请求（用于并发调用）"""
        pass

    @abstractmethod
    def validate_response(self, response: str) -> bool:
        """验证响应格式

        Args:
            response: LLM 返回的文本

        Returns:
            bool: 是否格式正确
        """
        pass

    @abstractmethod
    def estimate_tokens(self, text: str) -> int:
        """估算文本 token 数

        Args:
            text: 输入文本

        Returns:
            int: 估算的 token 数
        """
        pass
```

#### 8.7.2 实现类示例

**DeepSeekClient**：
```python
from openai import OpenAI

class DeepSeekClient(BaseLLMClient):
    """DeepSeek LLM 客户端（使用 OpenAI SDK）"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=config.get("base_url", "https://api.deepseek.com/v1")
        )

    def complete(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 4000,
        **kwargs
    ) -> str:
        response = self.client.chat.completions.create(
            model=self.config["model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        return response.choices[0].message.content

    async def acomplete(self, *args, **kwargs) -> str:
        # 使用 AsyncOpenAI 实现异步调用
        pass

    def validate_response(self, response: str) -> bool:
        # 验证 JSON 格式或指令集格式
        pass

    def estimate_tokens(self, text: str) -> int:
        # 使用 tiktoken 估算
        import tiktoken
        encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")
        return len(encoder.encode(text))
```

**GeminiClient**：
```python
import google.generativeai as genai

class GeminiClient(BaseLLMClient):
    """Gemini LLM 客户端"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(config["model"])

    def complete(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.1,
        max_tokens: int = 4000,
        **kwargs
    ) -> str:
        response = self.model.generate_content(
            f"{system_prompt}\n\n{user_prompt}",
            generation_config={
                "temperature": temperature,
                "max_output_tokens": max_tokens,
                **kwargs
            }
        )
        return response.text

    # 其他方法实现...
```

#### 8.7.3 工厂模式

```python
class LLMClientFactory:
    """LLM 客户端工厂"""

    _clients = {
        "deepseek": DeepSeekClient,
        "gemini": GeminiClient,
        "claude": ClaudeClient
    }

    @staticmethod
    def create(provider: str, config: Dict[str, Any]) -> BaseLLMClient:
        """创建 LLM 客户端实例

        Args:
            provider: Provider 名称 (deepseek/gemini/claude)
            config: Provider 配置字典

        Returns:
            BaseLLMClient: 客户端实例

        Raises:
            ValueError: 未知的 provider
        """
        if provider not in LLMClientFactory._clients:
            raise ValueError(f"Unknown provider: {provider}")

        return LLMClientFactory._clients[provider](config)

    @staticmethod
    def register_client(provider: str, client_class: type):
        """注册自定义客户端（扩展点）"""
        LLMClientFactory._clients[provider] = client_class
```

**使用示例**：
```python
# 创建 DeepSeek 客户端
client = LLMClientFactory.create("deepseek", DEEPSEEK_CONFIG)

# 调用 LLM
response = client.complete(
    system_prompt="你是字幕优化专家...",
    user_prompt="请优化以下字幕：...",
    temperature=0.1,
    max_tokens=4000
)
```

---

### 8.8 设计原则遵循

**KISS (保持简单)**：
- ✅ 统一的 `BaseLLMClient` 接口，屏蔽 Provider 差异
- ✅ 配置文件化，避免硬编码
- ✅ 3 层重试策略清晰明确

**DRY (拒绝重复)**：
- ✅ 重试逻辑、超时控制、监控代码复用
- ✅ Token 估算、速率限制逻辑抽象到基类

**YAGNI (拒绝过度设计)**：
- ✅ 仅实现当前需要的 3 个 Provider
- ✅ 流式输出功能暂时禁用（S2ST 不需要）
- ✅ 不预留"未来可能支持"的 Provider

**SOLID**：
- ✅ **单一职责**：每个 Client 只负责一个 Provider 的调用
- ✅ **开闭原则**：通过 `register_client()` 支持扩展
- ✅ **依赖倒置**：所有逻辑依赖 `BaseLLMClient` 抽象

---

**设计完成日期**：2026-01-18
**设计作者**：Claude (基于 Sequential Thinking 分析)
**文档版本**：v1.0# Phase 1 字幕优化功能实施细节补充

本章节补充 Phase 1 (LLM 字幕优化) 的本地数据处理和 YiVideo 架构集成部分。

---

## 9. Phase 1: 字幕优化功能实施细节

**设计日期**: 2026-01-18
**设计目标**: 补充数据提取、重构、验证和架构集成的完整实现逻辑

### 9.1 数据提取模块

#### 核心函数设计

**extract_segments_for_llm()**

将完整的转录数据提取为精简的 LLM 输入格式,最大限度节省 token。

```python
def extract_segments_for_llm(
    segments: List[Dict],
    subtitle_standards: Dict[str, Any]
) -> Dict[str, Any]:
    """
    提取字幕数据的必要字段用于 LLM 优化

    Args:
        segments: 完整的字幕 segments (包含 id, text, start, end, words 等)
        subtitle_standards: 字幕行业标准配置
            {
                "min_duration": 1.5,
                "max_duration": 6.0,
                "optimal_cps": [15, 17],
                "max_cps": 21
            }

    Returns:
        精简的 LLM 输入数据:
        {
            "segs": [
                {
                    "i": segment_id,
                    "t": text,
                    "s": start,
                    "e": end,
                    "d": duration,
                    "wc": word_count,
                    "cps": characters_per_second
                },
                ...
            ],
            "std": subtitle_standards
        }
    """
    extracted = {
        "segs": [],
        "std": {
            "min_d": subtitle_standards["min_duration"],
            "max_d": subtitle_standards["max_duration"],
            "opt_cps": subtitle_standards["optimal_cps"],
            "max_cps": subtitle_standards["max_cps"]
        }
    }

    for seg in segments:
        # 计算时长
        duration = seg["end"] - seg["start"]

        # 计算词数 (基于 words 数组长度)
        word_count = len(seg.get("words", []))

        # 计算 CPS (每秒字符数)
        text_length = len(seg["text"])
        cps = text_length / duration if duration > 0 else 0

        extracted["segs"].append({
            "i": seg["id"],
            "t": seg["text"],  # 仅供 LLM 理解语义,不用于重构
            "s": round(seg["start"], 2),
            "e": round(seg["end"], 2),
            "d": round(duration, 2),
            "wc": word_count,
            "cps": round(cps, 1)
        })

    return extracted
```

#### 字段映射策略

| 完整字段 | 精简键 | 是否提交 LLM | 原因 |
|---------|-------|------------|------|
| `id` | `i` | ✅ | 指令集引用必需 |
| `text` | `t` | ✅ | 供 LLM 理解语义 |
| `start` | `s` | ✅ | 计算时长约束 |
| `end` | `e` | ✅ | 计算时长约束 |
| `duration` | `d` | ✅ | 方便 LLM 判断 |
| `word_count` | `wc` | ✅ | 合并/拆分参考 |
| `cps` | `cps` | ✅ | CPS 标准判断 |
| `words` | - | ❌ | 本地已有,用于重构 |
| `tokens` | - | ❌ | 无关字段 |
| `avg_logprob` | - | ❌ | 无关字段 |

#### Token 优化效果

**原始数据大小估算** (单条 segment):
```json
{
  "id": 1,
  "text": "Well, little kitty...",
  "start": 11.4,
  "end": 19.56,
  "words": [{"word": "Well,", "start": 11.4, "end": 12.24, "probability": 0.64}, ...],
  "tokens": [823, 11, 2697, 15921, 11, ...],
  "avg_logprob": -0.36,
  "compression_ratio": 1.32,
  "no_speech_prob": 0.14
}
```
约 300-500 tokens (含 words 数组)

**精简后数据大小** (单条 segment):
```json
{"i":1,"t":"Well, little kitty...","s":11.4,"e":19.56,"d":8.16,"wc":9,"cps":16.5}
```
约 80-120 tokens

**节省比例**: 60-75% token 减少

---

### 9.2 数据重构模块

#### 核心流程设计

**apply_instruction_set()**

执行 LLM 返回的指令集,重构字幕数据。

```python
def apply_instruction_set(
    segments: List[Dict],
    instructions: List[Dict]
) -> List[Dict]:
    """
    应用 LLM 返回的指令集,重构字幕数据

    Args:
        segments: 原始字幕 segments (包含完整 words 数组)
        instructions: LLM 返回的指令集
            [
                {"t":"r", "i":1, "f":5, "e":6, "w":"kitty"},
                {"t":"m", "i":2, "f":0, "e":2, "to":1},
                ...
            ]

    Returns:
        重构后的 segments (保留完整数据结构)

    Processing Steps:
        1. 验证指令集
        2. 执行指令集 (顺序执行)
        3. 重新编号所有 segments
        4. 返回重构结果
    """
    # 步骤 1: 验证指令集
    validate_instruction_set(segments, instructions)

    # 步骤 2: 创建 segment 副本 (避免修改原数据)
    working_segments = copy.deepcopy(segments)

    # 步骤 3: 顺序执行指令
    for instruction in instructions:
        op_type = instruction["t"]

        if op_type == "r":  # Replace
            working_segments = apply_replace(working_segments, instruction)
        elif op_type == "m":  # Move
            working_segments = apply_move(working_segments, instruction)
        elif op_type == "g":  # Merge (Group)
            working_segments = apply_merge(working_segments, instruction)
        elif op_type == "s":  # Split
            working_segments = apply_split(working_segments, instruction)
        else:
            raise ValueError(f"Unknown operation type: {op_type}")

    # 步骤 4: 重新编号
    working_segments = renumber_segments(working_segments)

    return working_segments
```

#### 四种操作的具体实现

**1. Replace 操作 (替换/删除/插入)**

```python
def apply_replace(segments: List[Dict], instruction: Dict) -> List[Dict]:
    """
    替换操作: 替换 segment i 的 words[f:e] 为新文本 w

    Instruction:
        {"t":"r", "i":1, "f":5, "e":6, "w":"kitty"}

    处理逻辑:
        1. 找到 segment i
        2. 提取 words[f:e]
        3. 估算新词的时间戳
        4. 替换 words 数组
        5. 重新生成 text
    """
    seg_id = instruction["i"]
    from_idx = instruction["f"]
    end_idx = instruction["e"]
    new_text = instruction["w"]

    # 查找目标 segment
    segment = find_segment_by_id(segments, seg_id)
    if not segment:
        raise ValueError(f"Segment {seg_id} not found")

    # 提取被替换的 words
    old_words = segment["words"][from_idx:end_idx]

    # 计算时间范围
    if old_words:
        start_time = old_words[0]["start"]
        end_time = old_words[-1]["end"]
        duration = end_time - start_time
    else:
        # 插入操作 (f == e)
        if from_idx > 0:
            start_time = segment["words"][from_idx - 1]["end"]
        else:
            start_time = segment["start"]
        duration = 0
        end_time = start_time

    # 生成新的 words 数组
    if new_text:
        new_words = generate_word_timestamps(new_text, start_time, end_time)
    else:
        new_words = []  # 删除操作

    # 替换 words 数组
    segment["words"] = (
        segment["words"][:from_idx] +
        new_words +
        segment["words"][end_idx:]
    )

    # 重新生成 text
    segment["text"] = " ".join(w["word"] for w in segment["words"])

    # 保持 segment 的 start/end 不变 (边界由移动/合并操作调整)

    return segments


def generate_word_timestamps(text: str, start_time: float, end_time: float) -> List[Dict]:
    """
    为新文本估算词级时间戳

    Args:
        text: 新文本 (如 "kitty")
        start_time: 起始时间
        end_time: 结束时间

    Returns:
        词级时间戳数组
    """
    words = text.split()
    duration = end_time - start_time

    if not words:
        return []

    # 平均分配时间
    time_per_word = duration / len(words)

    result = []
    for i, word in enumerate(words):
        word_start = start_time + i * time_per_word
        word_end = word_start + time_per_word

        result.append({
            "word": word,
            "start": round(word_start, 2),
            "end": round(word_end, 2),
            "probability": 0.8  # 默认置信度
        })

    return result
```

**2. Move 操作 (移动词)**

```python
def apply_move(segments: List[Dict], instruction: Dict) -> List[Dict]:
    """
    移动操作: 将 segment i 的 words[f:e] 移动到 segment to

    Instruction:
        {"t":"m", "i":2, "f":0, "e":2, "to":1}

    处理逻辑:
        1. 从 segment i 提取 words[f:e]
        2. 删除 segment i 的 words[f:e]
        3. 插入到 segment to (开头或结尾,根据 to 与 i 的关系)
        4. 重新计算两个 segment 的 start/end
        5. 重新生成 text
    """
    src_id = instruction["i"]
    from_idx = instruction["f"]
    end_idx = instruction["e"]
    dst_id = instruction["to"]

    # 查找源和目标 segment
    src_seg = find_segment_by_id(segments, src_id)
    dst_seg = find_segment_by_id(segments, dst_id)

    if not src_seg or not dst_seg:
        raise ValueError(f"Segment not found: {src_id} or {dst_id}")

    # 提取要移动的 words
    moving_words = src_seg["words"][from_idx:end_idx]

    # 从源 segment 删除
    src_seg["words"] = (
        src_seg["words"][:from_idx] +
        src_seg["words"][end_idx:]
    )

    # 插入到目标 segment
    if dst_id > src_id:
        # 移动到后面的 segment,插入到开头
        dst_seg["words"] = moving_words + dst_seg["words"]
    else:
        # 移动到前面的 segment,插入到结尾
        dst_seg["words"] = dst_seg["words"] + moving_words

    # 重新计算时间戳和 text
    recalculate_segment(src_seg)
    recalculate_segment(dst_seg)

    return segments


def recalculate_segment(segment: Dict):
    """
    重新计算 segment 的 start/end/text

    Args:
        segment: 要重新计算的 segment (in-place 修改)
    """
    if not segment["words"]:
        # 空 segment,标记为删除
        segment["_delete"] = True
        return

    # 重新计算 start/end
    segment["start"] = segment["words"][0]["start"]
    segment["end"] = segment["words"][-1]["end"]

    # 重新生成 text
    segment["text"] = " ".join(w["word"] for w in segment["words"])
```

**3. Merge 操作 (合并)**

```python
def apply_merge(segments: List[Dict], instruction: Dict) -> List[Dict]:
    """
    合并操作: 合并多个 segments

    Instruction:
        {"t":"g", "i":[3, 4, 5]}

    处理逻辑:
        1. 提取所有要合并的 segments
        2. 合并 words 数组
        3. 新 segment 的 start = 第一个 segment 的 start
        4. 新 segment 的 end = 最后一个 segment 的 end
        5. 重新生成 text
        6. 标记其他 segments 为删除
        7. 新 segment 保留第一个 ID
    """
    seg_ids = instruction["i"]

    if len(seg_ids) < 2:
        raise ValueError("Merge requires at least 2 segments")

    # 查找所有 segments
    merge_segs = [find_segment_by_id(segments, sid) for sid in seg_ids]

    if not all(merge_segs):
        raise ValueError(f"Some segments not found: {seg_ids}")

    # 按时间顺序排序
    merge_segs.sort(key=lambda s: s["start"])

    # 创建新 segment
    new_segment = {
        "id": merge_segs[0]["id"],  # 保留第一个 ID
        "start": merge_segs[0]["start"],
        "end": merge_segs[-1]["end"],
        "words": [],
        "speaker": merge_segs[0].get("speaker", "SPEAKER_00")
    }

    # 合并 words 数组
    for seg in merge_segs:
        new_segment["words"].extend(seg["words"])

    # 重新生成 text
    new_segment["text"] = " ".join(w["word"] for w in new_segment["words"])

    # 标记其他 segments 为删除
    for seg in merge_segs[1:]:
        seg["_delete"] = True

    # 替换第一个 segment
    merge_segs[0].update(new_segment)

    return segments
```

**4. Split 操作 (拆分)**

```python
def apply_split(segments: List[Dict], instruction: Dict) -> List[Dict]:
    """
    拆分操作: 在词索引 p 处拆分 segment

    Instruction:
        {"t":"s", "i":5, "p":[8, 16]}

    处理逻辑:
        1. 在词索引 8 和 16 处拆分
        2. 生成 3 个新 segments:
           - Segment A: words[0:8]
           - Segment B: words[8:16]
           - Segment C: words[16:]
        3. 每个新 segment 重新计算 start/end
        4. 重新生成 text
        5. 原 segment 替换为第一个,其余添加到列表
        6. 最后统一重新编号
    """
    seg_id = instruction["i"]
    positions = instruction["p"]

    # 查找目标 segment
    segment = find_segment_by_id(segments, seg_id)
    if not segment:
        raise ValueError(f"Segment {seg_id} not found")

    # 构建拆分边界 [0, p1, p2, ..., len(words)]
    boundaries = [0] + sorted(positions) + [len(segment["words"])]

    # 生成新 segments
    new_segments = []
    for i in range(len(boundaries) - 1):
        start_idx = boundaries[i]
        end_idx = boundaries[i + 1]

        if start_idx >= end_idx:
            continue  # 跳过空片段

        words_slice = segment["words"][start_idx:end_idx]

        new_seg = {
            "id": segment["id"] if i == 0 else None,  # 第一个保留原 ID
            "start": words_slice[0]["start"],
            "end": words_slice[-1]["end"],
            "words": words_slice,
            "text": " ".join(w["word"] for w in words_slice),
            "speaker": segment.get("speaker", "SPEAKER_00")
        }

        new_segments.append(new_seg)

    # 替换原 segment
    seg_index = segments.index(segment)
    segments[seg_index] = new_segments[0]

    # 插入其余新 segments
    for new_seg in new_segments[1:]:
        segments.insert(seg_index + 1, new_seg)
        seg_index += 1

    return segments
```

#### ID 映射和重新编号

```python
def renumber_segments(segments: List[Dict]) -> List[Dict]:
    """
    重新编号所有 segments

    处理逻辑:
        1. 移除标记为删除的 segments
        2. 按 start 时间排序
        3. 重新分配 ID (1, 2, 3, ...)
    """
    # 移除已删除的 segments
    active_segments = [s for s in segments if not s.get("_delete", False)]

    # 按时间排序
    active_segments.sort(key=lambda s: s["start"])

    # 重新编号
    for i, seg in enumerate(active_segments, start=1):
        seg["id"] = i

    return active_segments
```

#### 工具函数

```python
def find_segment_by_id(segments: List[Dict], seg_id: int) -> Dict:
    """
    根据 ID 查找 segment

    Args:
        segments: segments 列表
        seg_id: segment ID

    Returns:
        找到的 segment,如果不存在返回 None
    """
    for seg in segments:
        if seg["id"] == seg_id:
            return seg
    return None
```

---

### 9.3 指令集验证机制

#### 核心函数设计

**validate_instruction_set()**

验证 LLM 返回的指令集是否合法。

```python
def validate_instruction_set(segments: List[Dict], instructions: List[Dict]):
    """
    验证指令集的合法性

    Args:
        segments: 原始 segments
        instructions: LLM 返回的指令集

    Raises:
        ValueError: 如果指令集不合法

    验证规则:
        1. 所有引用的 segment ID 必须存在
        2. 词索引范围必须合法 (0 <= idx < len(words))
        3. 合并操作的 segments 必须连续或相邻
        4. 拆分点不能在边界 (0 或 len(words))
        5. 不允许同一 segment 被多次操作 (除非是顺序依赖)
    """
    segment_ids = {s["id"] for s in segments}
    operated_segments = set()

    for idx, instruction in enumerate(instructions):
        op_type = instruction["t"]

        # 规则 1: ID 存在性检查
        if op_type in ["r", "m", "s"]:
            seg_id = instruction["i"]
            if seg_id not in segment_ids:
                raise ValueError(f"Instruction {idx}: Segment {seg_id} not found")

        elif op_type == "g":
            seg_ids = instruction["i"]
            for sid in seg_ids:
                if sid not in segment_ids:
                    raise ValueError(f"Instruction {idx}: Segment {sid} not found")

        # 规则 2: 词索引范围检查
        if op_type in ["r", "m"]:
            seg_id = instruction["i"]
            from_idx = instruction["f"]
            end_idx = instruction["e"]

            segment = find_segment_by_id(segments, seg_id)
            word_count = len(segment["words"])

            if not (0 <= from_idx <= end_idx <= word_count):
                raise ValueError(
                    f"Instruction {idx}: Invalid word range [{from_idx}:{end_idx}] "
                    f"for segment {seg_id} (word_count={word_count})"
                )

        # 规则 3: 合并 segments 连续性检查 (可选,根据需求)
        if op_type == "g":
            seg_ids = sorted(instruction["i"])
            # 可以添加连续性检查逻辑

        # 规则 4: 拆分点边界检查
        if op_type == "s":
            seg_id = instruction["i"]
            positions = instruction["p"]
            segment = find_segment_by_id(segments, seg_id)
            word_count = len(segment["words"])

            for pos in positions:
                if pos <= 0 or pos >= word_count:
                    raise ValueError(
                        f"Instruction {idx}: Invalid split position {pos} "
                        f"(must be in range 1-{word_count-1})"
                    )

        # 规则 5: 重复操作检查 (简化版,不考虑顺序依赖)
        # 注: 实际可能允许顺序依赖,此处仅示例
        if op_type != "g":
            seg_id = instruction["i"]
            if seg_id in operated_segments:
                # 警告: segment 被多次操作 (可能合法,取决于设计)
                pass
            operated_segments.add(seg_id)
```

#### 错误信息格式

**验证错误示例**:

```python
# 示例 1: Segment 不存在
ValueError: "Instruction 3: Segment 99 not found"

# 示例 2: 词索引超出范围
ValueError: "Instruction 5: Invalid word range [10:15] for segment 3 (word_count=12)"

# 示例 3: 拆分点在边界
ValueError: "Instruction 7: Invalid split position 0 (must be in range 1-18)"
```

#### 容错策略

```python
def validate_instruction_set_safe(
    segments: List[Dict],
    instructions: List[Dict]
) -> Tuple[bool, List[str]]:
    """
    容错版本的指令集验证

    Args:
        segments: 原始 segments
        instructions: LLM 返回的指令集

    Returns:
        (is_valid, errors):
            - is_valid: 是否完全合法
            - errors: 错误信息列表 (警告/错误)
    """
    errors = []

    try:
        validate_instruction_set(segments, instructions)
        return (True, [])
    except ValueError as e:
        errors.append(str(e))

    # 如果有错误,记录但不抛出
    return (False, errors)
```

---

### 9.4 YiVideo 架构集成

#### BaseNodeExecutor 实现

**SubtitleOptimizerExecutor**

```python
from services.common.base_node_executor import BaseNodeExecutor
from services.common.context import WorkflowContext
import copy


class SubtitleOptimizerExecutor(BaseNodeExecutor):
    """
    字幕优化节点执行器

    职责:
        1. 提取字幕数据
        2. 调用 LLM 生成优化指令集
        3. 验证和执行指令集
        4. 重构字幕数据
    """

    def validate_input(self):
        """验证输入参数"""
        required_fields = ["segments"]

        for field in required_fields:
            if field not in self.input_params:
                raise ValueError(f"Missing required field: {field}")

        # 验证 segments 格式
        segments = self.input_params["segments"]
        if not isinstance(segments, list) or len(segments) == 0:
            raise ValueError("segments must be a non-empty list")

        # 验证每个 segment 包含必要字段
        required_seg_fields = ["id", "text", "start", "end", "words"]
        for seg in segments:
            for field in required_seg_fields:
                if field not in seg:
                    raise ValueError(f"Segment {seg.get('id', '?')} missing field: {field}")

    def get_cache_key_fields(self) -> List[str]:
        """
        获取缓存键字段

        缓存策略:
            - 基于 segments 的内容哈希
            - 基于字幕标准配置
            - 如果输入相同,直接返回缓存结果
        """
        return [
            "segments",  # 字幕内容
            "subtitle_standards"  # 行业标准配置
        ]

    def execute_core_logic(self) -> Dict[str, Any]:
        """
        核心执行逻辑

        执行流程:
            1. 提取字幕数据
            2. 分批提交 LLM (并发处理)
            3. 合并指令集
            4. 验证指令集
            5. 执行指令集
            6. 返回优化后的字幕
        """
        segments = self.input_params["segments"]
        subtitle_standards = self.input_params.get("subtitle_standards", DEFAULT_STANDARDS)

        # 阶段 1: 提取数据
        extracted_data = extract_segments_for_llm(segments, subtitle_standards)

        # 阶段 2: 分批提交 LLM
        all_instructions = []
        batches = split_into_batches(
            extracted_data["segs"],
            window_size=150,
            overlap=10
        )

        for batch in batches:
            batch_data = {
                "segs": batch["segments"],
                "std": extracted_data["std"]
            }

            # 调用 LLM
            instructions = self.call_llm_for_optimization(batch_data, batch["context"])

            # 过滤重叠区域的指令 (仅保留主体区域)
            filtered_instructions = filter_overlap_instructions(
                instructions,
                batch["main_start_id"],
                batch["main_end_id"]
            )

            all_instructions.extend(filtered_instructions)

        # 阶段 3: 验证指令集
        is_valid, errors = validate_instruction_set_safe(segments, all_instructions)

        if not is_valid:
            logger.warning(f"指令集验证失败: {errors}")
            # 根据策略决定是否继续 (此处选择继续,但记录警告)

        # 阶段 4: 执行指令集
        optimized_segments = apply_instruction_set(
            copy.deepcopy(segments),  # 避免修改原数据
            all_instructions
        )

        # 阶段 5: 返回结果
        return {
            "segments": optimized_segments,
            "optimization_stats": {
                "original_count": len(segments),
                "optimized_count": len(optimized_segments),
                "instructions_count": len(all_instructions),
                "validation_errors": errors
            }
        }

    def call_llm_for_optimization(
        self,
        batch_data: Dict,
        context: Dict
    ) -> List[Dict]:
        """
        调用 LLM 进行字幕优化

        Args:
            batch_data: 批次数据 (精简格式)
            context: 上下文信息 (前后字幕)

        Returns:
            指令集列表
        """
        # 构建 User Prompt
        user_prompt = self.build_user_prompt(batch_data, context)

        # 调用 LLM API
        llm_client = LLMClientFactory.create("deepseek", DEEPSEEK_CONFIG)

        response = llm_client.complete(
            system_prompt=SUBTITLE_OPTIMIZATION_SYSTEM_PROMPT,  # 见 findings.md 4.2
            user_prompt=user_prompt,
            temperature=0.1,
            max_tokens=4000
        )

        # 解析 JSON 响应
        try:
            instructions = json.loads(response)
            return instructions
        except json.JSONDecodeError as e:
            logger.error(f"LLM 返回的 JSON 格式错误: {e}")
            return []  # 返回空指令集

    def build_user_prompt(self, batch_data: Dict, context: Dict) -> str:
        """
        构建 User Prompt

        Args:
            batch_data: 批次数据
            context: 上下文 (包含 previous_segments 和 next_segments)

        Returns:
            User Prompt 字符串
        """
        # 参考 findings.md 4.3 节的模板
        prompt = f"""请分析以下字幕数据并返回优化指令集：

```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

要求：
1. 识别所有需要优化的问题（断句、纠错、时长、CPS）
2. 返回JSON格式的指令数组
3. 如果没有问题，返回空数组 []
"""
        return prompt


# 默认字幕标准
DEFAULT_STANDARDS = {
    "min_duration": 1.5,
    "max_duration": 6.0,
    "optimal_cps": [15, 17],
    "max_cps": 21
}
```

#### Celery 任务定义

```python
from celery import Task
from services.common.celery_app import celery_app


@celery_app.task(bind=True)
def subtitle_optimizer_task(self: Task, context: dict) -> dict:
    """
    字幕优化 Celery 任务

    Args:
        context: WorkflowContext 字典

    Returns:
        更新后的 WorkflowContext
    """
    executor = SubtitleOptimizerExecutor(context)
    return executor.execute()
```

#### 输入/输出数据结构 (JSON Schema)

**输入数据结构**:

```json
{
  "task_name": "subtitle.optimize",
  "task_id": "task-001",
  "callback": "http://localhost:5678/webhook",
  "input_data": {
    "segments": [
      {
        "id": 1,
        "text": "Well, little kitty, if you really want to learn",
        "start": 11.4,
        "end": 19.56,
        "speaker": "SPEAKER_00",
        "words": [
          {"word": "Well,", "start": 11.4, "end": 12.24, "probability": 0.64},
          {"word": "little", "start": 12.24, "end": 12.68, "probability": 0.82},
          ...
        ]
      },
      ...
    ],
    "subtitle_standards": {
      "min_duration": 1.5,
      "max_duration": 6.0,
      "optimal_cps": [15, 17],
      "max_cps": 21
    }
  }
}
```

**输出数据结构**:

```json
{
  "segments": [
    {
      "id": 1,
      "text": "Well, little kitty, if you really want to learn how to catch flies,",
      "start": 11.4,
      "end": 19.56,
      "speaker": "SPEAKER_00",
      "words": [
        {"word": "Well,", "start": 11.4, "end": 12.24, "probability": 0.64},
        ...
      ]
    },
    ...
  ],
  "optimization_stats": {
    "original_count": 150,
    "optimized_count": 142,
    "instructions_count": 18,
    "validation_errors": []
  }
}
```

#### 错误处理策略

```python
class SubtitleOptimizerExecutor(BaseNodeExecutor):
    # ... (前面的代码)

    def execute_core_logic(self) -> Dict[str, Any]:
        """核心执行逻辑 (含错误处理)"""
        try:
            # 主流程 (见上面)
            ...

        except Exception as e:
            logger.error(f"字幕优化失败: {e}", exc_info=True)

            # 返回原始 segments (降级策略)
            return {
                "segments": self.input_params["segments"],
                "optimization_stats": {
                    "original_count": len(self.input_params["segments"]),
                    "optimized_count": len(self.input_params["segments"]),
                    "instructions_count": 0,
                    "validation_errors": [str(e)],
                    "fallback": True
                }
            }
```

**错误类型和处理策略**:

| 错误类型 | 处理策略 | 影响 |
|---------|---------|------|
| **LLM API 调用失败** | 重试 3 次 → 降级返回原始数据 | 字幕未优化 |
| **指令集验证失败** | 记录警告 → 尝试执行有效部分 | 部分优化 |
| **指令集执行异常** | 回滚到原始数据 → 记录错误 | 字幕未优化 |
| **输入数据格式错误** | 抛出 ValueError → 任务失败 | 阻塞工作流 |

---

### 9.5 性能优化与配置

#### 并发控制策略

**窗口分割逻辑**:

```python
def split_into_batches(
    segments: List[Dict],
    window_size: int = 150,
    overlap: int = 10
) -> List[Dict]:
    """
    将 segments 分割为重叠窗口批次

    Args:
        segments: 所有 segments
        window_size: 窗口大小 (主体区域)
        overlap: 重叠区域大小 (前后各 overlap/2)

    Returns:
        批次列表:
        [
            {
                "segments": [...],  # 当前批次的所有 segments (含重叠)
                "main_start_id": 1,  # 主体区域起始 ID
                "main_end_id": 150,  # 主体区域结束 ID
                "context": {
                    "previous_segments": [...],  # 前文参考
                    "next_segments": [...]       # 后文参考
                }
            },
            ...
        ]
    """
    batches = []
    total = len(segments)
    overlap_half = overlap // 2

    i = 0
    while i < total:
        # 主体区域
        main_start = i
        main_end = min(i + window_size, total)

        # 重叠区域
        batch_start = max(0, main_start - overlap_half)
        batch_end = min(total, main_end + overlap_half)

        # 提取 segments
        batch_segments = segments[batch_start:batch_end]
        main_start_id = segments[main_start]["id"]
        main_end_id = segments[main_end - 1]["id"]

        # 上下文
        previous_segments = segments[batch_start:main_start] if main_start > batch_start else []
        next_segments = segments[main_end:batch_end] if main_end < batch_end else []

        batches.append({
            "segments": batch_segments,
            "main_start_id": main_start_id,
            "main_end_id": main_end_id,
            "context": {
                "previous_segments": previous_segments,
                "next_segments": next_segments
            }
        })

        i = main_end

    return batches
```

**重叠区域指令过滤**:

```python
def filter_overlap_instructions(
    instructions: List[Dict],
    main_start_id: int,
    main_end_id: int
) -> List[Dict]:
    """
    过滤重叠区域的指令,仅保留主体区域的指令

    Args:
        instructions: LLM 返回的指令集
        main_start_id: 主体区域起始 ID
        main_end_id: 主体区域结束 ID

    Returns:
        过滤后的指令集
    """
    filtered = []

    for instruction in instructions:
        op_type = instruction["t"]

        # 判断指令是否在主体区域
        if op_type in ["r", "m", "s"]:
            seg_id = instruction["i"]
            if main_start_id <= seg_id <= main_end_id:
                filtered.append(instruction)

        elif op_type == "g":
            seg_ids = instruction["i"]
            # 如果所有 ID 都在主体区域,保留
            if all(main_start_id <= sid <= main_end_id for sid in seg_ids):
                filtered.append(instruction)

    return filtered
```

#### LLM 调用超时和重试

```python
from services.common.llm_client import LLMClientFactory, LLMClientError


class SubtitleOptimizerExecutor(BaseNodeExecutor):
    # ... (前面的代码)

    def call_llm_for_optimization(
        self,
        batch_data: Dict,
        context: Dict,
        max_retries: int = 3
    ) -> List[Dict]:
        """
        调用 LLM 进行字幕优化 (含重试)

        Args:
            batch_data: 批次数据
            context: 上下文
            max_retries: 最大重试次数

        Returns:
            指令集列表
        """
        user_prompt = self.build_user_prompt(batch_data, context)

        # 重试逻辑
        for attempt in range(max_retries):
            try:
                # 创建 LLM 客户端 (参考 findings.md 第 8 章)
                llm_client = LLMClientFactory.create("deepseek", DEEPSEEK_CONFIG)

                response = llm_client.complete(
                    system_prompt=SUBTITLE_OPTIMIZATION_SYSTEM_PROMPT,
                    user_prompt=user_prompt,
                    temperature=0.1,
                    max_tokens=4000,
                    timeout=60  # 60 秒超时
                )

                # 解析 JSON
                instructions = json.loads(response)
                return instructions

            except (LLMClientError, json.JSONDecodeError) as e:
                logger.warning(f"LLM 调用失败 (尝试 {attempt + 1}/{max_retries}): {e}")

                if attempt == max_retries - 1:
                    # 最后一次重试失败,返回空指令集
                    logger.error("LLM 调用最终失败,返回空指令集")
                    return []

                # 指数退避
                time.sleep(2 ** attempt)

        return []
```

#### 配置参数

```yaml
# config.yml
subtitle_optimizer:
  # 窗口配置
  window_size: 150          # 批次窗口大小 (segments)
  overlap_size: 10          # 重叠区域大小

  # LLM 配置
  llm_provider: "deepseek"  # 默认 Provider
  llm_timeout: 60           # 超时时间 (秒)
  max_retries: 3            # 最大重试次数

  # 字幕标准
  subtitle_standards:
    min_duration: 1.5       # 最小时长 (秒)
    max_duration: 6.0       # 最大时长 (秒)
    optimal_cps: [15, 17]   # 理想 CPS 范围
    max_cps: 21             # 最大 CPS

  # 性能配置
  enable_cache: true        # 启用状态复用
  cache_ttl: 3600           # 缓存过期时间 (秒)
```

---

### 9.6 设计原则遵循

**KISS (保持简单)**:
- ✅ 数据提取: 简单的字典映射和计算
- ✅ 数据重构: 直接操作 Python 列表和字典
- ✅ 指令集验证: 基础的条件检查
- ✅ 不引入复杂框架或抽象

**DRY (拒绝重复)**:
- ✅ 时间戳计算逻辑提取为 `recalculate_segment()`
- ✅ segment 查找逻辑提取为 `find_segment_by_id()`
- ✅ 重新编号逻辑统一在 `renumber_segments()`

**YAGNI (拒绝过度设计)**:
- ✅ 只实现当前需要的 4 种操作 (m/r/g/s)
- ✅ 不预留"扩展接口"或"插件系统"
- ✅ 不引入不必要的抽象层

**SOLID**:
- ✅ **单一职责**: SubtitleOptimizerExecutor 仅负责字幕优化
- ✅ **开闭原则**: 通过配置文件扩展字幕标准,无需修改代码
- ✅ **依赖倒置**: 依赖 BaseNodeExecutor 抽象,而非具体实现

---

**设计完成日期**: 2026-01-18
**设计作者**: Claude (基于 Sequential Thinking 分析)
**文档版本**: v1.0 (Phase 1 实施细节补充)
