# ğŸ—ï¸ YiVideoå­—å¹•å…³é”®å¸§æå–åŠŸèƒ½ - ä»£ç æ–½å·¥æ–‡æ¡£

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-01-07  
**é‡å¤§æ›´æ–°**: å…³é”®å¸§é€»è¾‘é‡æ„ (ä»"äº‹ä»¶é©±åŠ¨"æ”¹ä¸º"å…³é”®å¸§é©±åŠ¨")
**é€‚ç”¨é¡¹ç›®**: YiVideoå­—å¹•æå–ç³»ç»Ÿ  
**åŸºäºæ–‡æ¡£**: SUBTITLE_KEYFRAME_EXTRACTION.md

---

## ğŸ“‹ **æ–½å·¥æ¦‚è¿°**

### **é‡å¤§æ¶æ„è°ƒæ•´**
âš ï¸ **æœ¬æ¬¡æ–½å·¥æ¶‰åŠæ ¸å¿ƒç®—æ³•çš„é‡æ„**ï¼Œå°†åŸæœ‰çš„"äº‹ä»¶é©±åŠ¨"æ”¹ä¸º"å…³é”®å¸§é©±åŠ¨"æ¨¡å¼ï¼š

**åŸæœ‰é€»è¾‘**:
```
é€å¸§æ¯”å¯¹ â†’ æ£€æµ‹äº‹ä»¶(å‡ºç°/æ¶ˆå¤±/å˜åŒ–) â†’ OCRè¯†åˆ« â†’ æ„å»ºæ®µè½
```

**æ–°ç‰ˆé€»è¾‘**:
```
ç¬¬ä¸€å¸§=å…³é”®å¸§ â†’ é€å¸§ç›¸ä¼¼åº¦æ¯”å¯¹ â†’ ç›¸ä¼¼åº¦<90%=æ–°å…³é”®å¸§ â†’ OCRè¯†åˆ« â†’ æ„å»ºæ®µè½
```

### **å…³é”®æŠ€æœ¯æ”¹è¿›**
1. **è¡Œä¸šæ ‡å‡†é˜ˆå€¼**: åŸºäºDr. Neal Krawetzæ ‡å‡†ï¼Œé»˜è®¤90%ç›¸ä¼¼åº¦é˜ˆå€¼
2. **ç¬¬ä¸€å¸§å¼ºåˆ¶**: ç¬¬ä¸€å¸§æ— æ¡ä»¶ä½œä¸ºå…³é”®å¸§
3. **ç›¸ä¼¼åº¦ä¼˜å…ˆ**: ä¼˜å…ˆè®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼Œè€ŒéçŠ¶æ€è½¬æ¢
4. **ç›´è§‚é…ç½®**: ä½¿ç”¨ç™¾åˆ†æ¯”é˜ˆå€¼æ›¿ä»£æ±‰æ˜è·ç¦»

---

## ğŸš¨ **ç¬¬é›¶é˜¶æ®µï¼šå…³é”®å¸§é€»è¾‘é‡æ„** (ä¼˜å…ˆçº§ï¼šğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥)

### **ä»»åŠ¡0.1: åˆ›å»ºæ–°çš„å…³é”®å¸§æ£€æµ‹å™¨**

**ç›®æ ‡**: å®Œå…¨é‡æ„å…³é”®å¸§æ£€æµ‹é€»è¾‘ï¼Œä»"äº‹ä»¶é©±åŠ¨"æ”¹ä¸º"å…³é”®å¸§é©±åŠ¨"

**æ–°å¢æ–‡ä»¶**: `services/workers/paddleocr_service/app/modules/keyframe_detector.py`

```python
import torch
import numpy as np
import cv2
from typing import List, Tuple, Dict
from .decoder import GPUDecoder

class KeyFrameDetector:
    """
    å…³é”®å¸§æ£€æµ‹å™¨ - é‡æ„ç‰ˆæœ¬
    åŸºäºç›¸ä¼¼åº¦çš„å…³é”®å¸§æ£€æµ‹ï¼Œæ›¿ä»£åŸæœ‰çš„äº‹ä»¶æ£€æµ‹ç³»ç»Ÿ
    """
    
    def __init__(self, config):
        self.config = config
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # dHashé…ç½®
        self.hash_size = config.get('dhash_size', 8)
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½® (æ–°å¢)
        self.similarity_threshold = config.get('similarity_threshold', 0.90)  # 90%é»˜è®¤
        
        # ä»ç›¸ä¼¼åº¦æ¢ç®—æ±‰æ˜è·ç¦»é˜ˆå€¼
        max_bits = self.hash_size * self.hash_size
        self.hamming_threshold = int((1 - self.similarity_threshold) * max_bits)
        
        print(f"æ¨¡å—: å…³é”®å¸§æ£€æµ‹å™¨å·²åŠ è½½ - ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold:.0%}, "
              f"æ±‰æ˜é˜ˆå€¼: {self.hamming_threshold}")

    def detect_keyframes(self, video_path: str, decoder: GPUDecoder, 
                        subtitle_area: Tuple[int, int, int, int]) -> List[int]:
        """
        æ£€æµ‹è§†é¢‘ä¸­æ‰€æœ‰å…³é”®å¸§
        
        å®ç°é€»è¾‘:
        1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        2. é€å¸§æ¯”å¯¹: 1vs0, 2vs1, 3vs2...
        3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            subtitle_area: å­—å¹•åŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            
        Returns:
            å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ [0, 45, 89, 156, ...]
        """
        print("ğŸ” å¼€å§‹å…³é”®å¸§æ£€æµ‹...")
        x1, y1, x2, y2 = subtitle_area

        # 1. æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„ç‰¹å¾
        all_hashes, all_stds = self._compute_frame_features(video_path, decoder, (x1, y1, x2, y2))
        print(f"ğŸ“Š å®Œæˆç‰¹å¾è®¡ç®—: {len(all_hashes)} å¸§")

        # 2. ä½¿ç”¨å¤§æ´¥æ³•ç¡®å®šç©ºç™½å¸§é˜ˆå€¼
        blank_threshold = self._get_otsu_threshold(all_stds)
        print(f"ğŸ¯ ç©ºç™½å¸§é˜ˆå€¼: {blank_threshold:.4f}")

        # 3. å…³é”®å¸§é€å¸§æ£€æµ‹
        keyframes = self._detect_keyframes_sequential(all_hashes, all_stds, blank_threshold)
        
        print(f"âœ… æ£€æµ‹åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        return keyframes
    
    def _detect_keyframes_sequential(self, hashes: List[np.ndarray], 
                                   stds: np.ndarray, blank_threshold: float) -> List[int]:
        """
        æŒ‰ç…§æ–°é€»è¾‘è¿›è¡Œå…³é”®å¸§æ£€æµ‹
        """
        keyframes = []
        
        # 1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        keyframes.append(0)
        print(f"ğŸ“Œ å…³é”®å¸§ 0: é»˜è®¤ç¬¬ä¸€å¸§")
        
        print(f"ğŸ”„ æ­£åœ¨åˆ†æ {len(hashes)} å¸§çš„ç›¸ä¼¼åº¦...")
        
        # 2. ä»ç¬¬1å¸§å¼€å§‹é€å¸§æ¯”å¯¹
        for curr_frame in range(1, len(hashes)):
            prev_frame = curr_frame - 1
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self._calculate_similarity(
                hashes[prev_frame], hashes[curr_frame],
                stds[prev_frame], stds[curr_frame], 
                blank_threshold
            )
            
            # 3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
            if similarity < self.similarity_threshold:
                keyframes.append(curr_frame)
                print(f"ğŸ“Œ å…³é”®å¸§ {curr_frame}: ç›¸ä¼¼åº¦ {similarity:.1%}")
            
            # è¿›åº¦æ˜¾ç¤º
            if curr_frame % 1000 == 0:
                progress = (curr_frame / len(hashes)) * 100
                print(f"  ğŸ” æ£€æµ‹è¿›åº¦: {curr_frame}/{len(hashes)} ({progress:.1f}%), "
                      f"å·²æ‰¾åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        
        return keyframes
    
    def _calculate_similarity(self, hash1: np.ndarray, hash2: np.ndarray,
                            std1: float, std2: float, blank_threshold: float) -> float:
        """
        è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        ç›¸ä¼¼åº¦è®¡ç®—è§„åˆ™:
        - ç©ºç™½å¸§ vs ç©ºç™½å¸§: 100%
        - ç©ºç™½å¸§ vs å†…å®¹å¸§: 0%  
        - å†…å®¹å¸§ vs å†…å®¹å¸§: åŸºäºdHashçš„æ±‰æ˜è·ç¦»
        """
        # åˆ¤æ–­å¸§ç±»å‹
        is_blank1 = std1 < blank_threshold
        is_blank2 = std2 < blank_threshold
        
        # Case 1: ä¸¤å¸§éƒ½æ˜¯ç©ºç™½å¸§ â†’ ç›¸ä¼¼åº¦100%
        if is_blank1 and is_blank2:
            return 1.0
        
        # Case 2: ä¸€ä¸ªç©ºç™½ä¸€ä¸ªéç©ºç™½ â†’ ç›¸ä¼¼åº¦0% (å®Œå…¨ä¸åŒ)
        if is_blank1 != is_blank2:
            return 0.0
        
        # Case 3: ä¸¤å¸§éƒ½æœ‰å†…å®¹ â†’ åŸºäºdHashè®¡ç®—ç›¸ä¼¼åº¦
        hamming_distance = np.count_nonzero(hash1 != hash2)
        max_possible_distance = hash1.size  # 64 for 8x8 dHash
        
        # ç›¸ä¼¼åº¦ = 1 - (æ±‰æ˜è·ç¦» / æœ€å¤§å¯èƒ½è·ç¦»)
        similarity = 1.0 - (hamming_distance / max_possible_distance)
        
        return similarity
    
    def _compute_frame_features(self, video_path: str, decoder: GPUDecoder, 
                               crop_rect: Tuple[int, int, int, int]) -> Tuple[List[np.ndarray], np.ndarray]:
        """
        æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„dHashå’Œæ ‡å‡†å·®
        å¤ç”¨åŸæœ‰çš„GPUæ‰¹é‡è®¡ç®—é€»è¾‘
        """
        all_hashes = []
        all_stds = []
        x1, y1, x2, y2 = crop_rect

        frame_count = 0
        batch_count = 0
        
        print("ğŸ”„ æ­£åœ¨è®¡ç®—è§†é¢‘ç‰¹å¾...")
        
        for batch_tensor, _ in decoder.decode(video_path):
            # è£å‰ªå­—å¹•åŒºåŸŸ
            cropped_batch = batch_tensor[:, :, y1:y2, x1:x2]

            # --- åœ¨GPUä¸Šæ‰¹é‡è®¡ç®— --- #
            # 1. è®¡ç®—æ ‡å‡†å·®
            stds = torch.std(cropped_batch.float().view(cropped_batch.size(0), -1), dim=1)
            all_stds.extend(stds.cpu().numpy())

            # 2. è®¡ç®—dHash
            grayscale_batch = cropped_batch.float().mean(dim=1, keepdim=True)
            resized_batch = torch.nn.functional.interpolate(
                grayscale_batch, 
                size=(self.hash_size, self.hash_size + 1), 
                mode='bilinear', align_corners=False
            )
            diff = resized_batch[:, :, :, 1:] > resized_batch[:, :, :, :-1]
            hashes_np = diff.cpu().numpy().astype(np.uint8).reshape(diff.shape[0], -1)
            all_hashes.extend(hashes_np)
            
            frame_count += batch_tensor.size(0)
            batch_count += 1
            
            # æ¯50ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if batch_count % 50 == 0:
                print(f"  ğŸ“Š å·²å¤„ç† {frame_count} å¸§...")
            
        print(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: å…±å¤„ç† {frame_count} å¸§")
        return all_hashes, np.array(all_stds)
    
    def _get_otsu_threshold(self, stds: np.ndarray) -> float:
        """ä½¿ç”¨å¤§æ´¥æ³•è®¡ç®—æœ€ä½³ç©ºç™½å¸§é˜ˆå€¼"""
        if stds.max() == stds.min(): 
            return 0.0
        
        stds_normalized = (255 * (stds - stds.min()) / (stds.max() - stds.min())).astype(np.uint8)
        threshold_otsu, _ = cv2.threshold(stds_normalized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        original_threshold = threshold_otsu / 255 * (stds.max() - stds.min()) + stds.min()
        return float(original_threshold)

    def generate_subtitle_segments(self, keyframes: List[int], 
                                 fps: float, total_frames: int) -> List[Dict]:
        """
        ä»å…³é”®å¸§åˆ—è¡¨ç”Ÿæˆå­—å¹•æ®µè½
        æ¯ä¸¤ä¸ªè¿ç»­å…³é”®å¸§ä¹‹é—´å½¢æˆä¸€ä¸ªæ®µè½
        """
        segments = []
        
        for i in range(len(keyframes)):
            start_frame = keyframes[i]
            
            # ç¡®å®šç»“æŸå¸§
            if i + 1 < len(keyframes):
                end_frame = keyframes[i + 1] - 1  # ä¸‹ä¸€å…³é”®å¸§çš„å‰ä¸€å¸§
            else:
                end_frame = total_frames - 1  # è§†é¢‘çš„æœ€åä¸€å¸§
            
            # è®¡ç®—æ—¶é—´æˆ³
            start_time = start_frame / fps
            end_time = end_frame / fps
            
            segments.append({
                'key_frame': start_frame,      # ğŸ†• å…³é”®å¸§ä¿¡æ¯
                'start_frame': start_frame,
                'end_frame': end_frame, 
                'start_time': start_time,
                'end_time': end_time,
                'duration': end_time - start_time
            })
        
        return segments
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ–°çš„KeyFrameDetectorç±»å®Œå…¨å®ç°
- [ ] ç›¸ä¼¼åº¦è®¡ç®—ç¬¦åˆè¡Œä¸šæ ‡å‡†
- [ ] ç¬¬ä¸€å¸§å¼ºåˆ¶è®¾ä¸ºå…³é”®å¸§
- [ ] æ”¯æŒå¯é…ç½®çš„ç›¸ä¼¼åº¦é˜ˆå€¼
- [ ] ç”ŸæˆåŒ…å«å…³é”®å¸§ä¿¡æ¯çš„æ®µè½æ•°æ®

### **ä»»åŠ¡0.2: æ›´æ–°ä¸»å¤„ç†é€»è¾‘**

**ä¿®æ”¹æ–‡ä»¶**: `services/workers/paddleocr_service/app/logic.py`

**ä¿®æ”¹å†…å®¹**: å°†change_detectoræ›¿æ¢ä¸ºkeyframe_detector

```python
# ğŸš« æ—§ç‰ˆæœ¬å·²æ›¿æ¢: from app.modules.change_detector import ChangeDetector, ChangeType

# æ–°å¢å¯¼å…¥
from app.modules.keyframe_detector import KeyFrameDetector

def extract_subtitles_from_video(video_path: str, config: Dict) -> List[Dict[str, Any]]:
    """é›†æˆæ–°çš„å…³é”®å¸§æ£€æµ‹é€»è¾‘"""
    
    # 1. åˆå§‹åŒ–æ¨¡å—
    decoder = GPUDecoder(config.get('decoder', {}))
    area_detector = SubtitleAreaDetector(config.get('area_detector', {}))
    keyframe_detector = KeyFrameDetector(config.get('keyframe_detector', {}))  # ğŸ†• æ–°æ£€æµ‹å™¨
    ocr_engine = MultiProcessOCREngine(config.get('ocr', {}))
    postprocessor = SubtitlePostprocessor(config.get('postprocessor', {}))
    
    # 2. è·å–è§†é¢‘å…ƒæ•°æ®
    fps, total_frames = _get_video_metadata(video_path)
    
    # 3. æ™ºèƒ½å­—å¹•åŒºåŸŸæ£€æµ‹
    subtitle_area = area_detector.detect(video_path, decoder)
    if subtitle_area is None:
        return []
    
    # 4. å…³é”®å¸§æ£€æµ‹ (æ–°é€»è¾‘)
    keyframes = keyframe_detector.detect_keyframes(video_path, decoder, subtitle_area)
    
    # 5. ç”Ÿæˆæ®µè½ä¿¡æ¯ (æ–°é€»è¾‘) 
    segments = keyframe_detector.generate_subtitle_segments(keyframes, fps, total_frames)
    
    # 6. OCRè¯†åˆ« (éœ€è¦é€‚é…æ–°çš„è¾“å…¥æ ¼å¼)
    ocr_results = ocr_engine.recognize_keyframes(video_path, decoder, keyframes, subtitle_area, total_frames)
    
    # 7. åå¤„ç† (éœ€è¦é€‚é…æ–°çš„æ•°æ®ç»“æ„)
    final_subtitles = postprocessor.format_from_keyframes(segments, ocr_results, fps)
    
    return final_subtitles
```

### **ä»»åŠ¡0.3: é€‚é…OCRå¤„ç†é€»è¾‘**

**ä¿®æ”¹æ–‡ä»¶**: `services/workers/paddleocr_service/app/modules/ocr.py`

**æ–°å¢æ–¹æ³•**: æ”¯æŒåŸºäºå…³é”®å¸§åˆ—è¡¨çš„OCRå¤„ç†

```python
def recognize_keyframes(self, video_path: str, decoder: GPUDecoder,
                       keyframes: List[int], subtitle_area: Tuple[int, int, int, int],
                       total_frames: int) -> Dict[int, Tuple[str, Any]]:
    """
    åŸºäºå…³é”®å¸§åˆ—è¡¨è¿›è¡ŒOCRè¯†åˆ«
    æ›¿ä»£åŸæœ‰çš„åŸºäºäº‹ä»¶çš„è¯†åˆ«æ–¹å¼
    
    Args:
        keyframes: å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ [0, 45, 89, ...]
        
    Returns:
        OCRç»“æœæ˜ å°„ {å…³é”®å¸§ç´¢å¼•: (æ–‡æœ¬, bbox)}
    """
    if not keyframes:
        return {}
    
    print(f"ğŸ” å¼€å§‹å¯¹ {len(keyframes)} ä¸ªå…³é”®å¸§è¿›è¡ŒOCRè¯†åˆ«...")
    
    # ç”Ÿæˆç²¾å‡†é‡‡æ ·ä»»åŠ¡
    worker_tasks = []
    for frame_idx in keyframes:
        worker_tasks.append((frame_idx, video_path, subtitle_area))
    
    # æ‰§è¡ŒOCRè¯†åˆ« (å¤ç”¨ç°æœ‰çš„å¤šè¿›ç¨‹é€»è¾‘)
    ocr_results_map = {}
    # ... ç°æœ‰çš„å¤šè¿›ç¨‹å¤„ç†é€»è¾‘ ...
    
    return ocr_results_map
```

### **ä»»åŠ¡0.4: é€‚é…åå¤„ç†é€»è¾‘**

**ä¿®æ”¹æ–‡ä»¶**: `services/workers/paddleocr_service/app/modules/postprocessor.py`

**æ–°å¢æ–¹æ³•**: æ”¯æŒåŸºäºå…³é”®å¸§æ•°æ®çš„åå¤„ç†

```python
def format_from_keyframes(self, segments: List[Dict], 
                         ocr_results: Dict[int, Tuple[str, Any]], 
                         fps: float) -> List[Dict[str, Any]]:
    """
    åŸºäºå…³é”®å¸§æ®µè½å’ŒOCRç»“æœç”Ÿæˆæœ€ç»ˆå­—å¹•
    
    Args:
        segments: å…³é”®å¸§æ®µè½åˆ—è¡¨ï¼ŒåŒ…å«key_frame, start_frame, end_frameç­‰ä¿¡æ¯
        ocr_results: OCRè¯†åˆ«ç»“æœ {å…³é”®å¸§ç´¢å¼•: (æ–‡æœ¬, bbox)}
        fps: è§†é¢‘å¸§ç‡
        
    Returns:
        æ ‡å‡†åŒ–çš„å­—å¹•åˆ—è¡¨ï¼ŒåŒ…å«keyFrameå’ŒframeRangeå­—æ®µ
    """
    final_subtitles = []
    subtitle_id = 1
    
    for segment in segments:
        key_frame = segment['key_frame']
        
        # è·å–OCRç»“æœ
        if key_frame in ocr_results:
            text, bbox = ocr_results[key_frame]
            
            if text and text.strip():
                # è®¡ç®—æŒç»­æ—¶é—´
                duration = segment['duration']
                
                # è¿‡æ»¤è¿‡çŸ­çš„æ®µè½
                if duration >= self.min_duration_seconds:
                    final_subtitles.append({
                        'id': subtitle_id,
                        'startTime': round(segment['start_time'], 3),
                        'endTime': round(segment['end_time'], 3),
                        'keyFrame': key_frame,  # ğŸ†• å…³é”®å¸§ä¿¡æ¯
                        'frameRange': [segment['start_frame'], segment['end_frame']],  # ğŸ†• å¸§èŒƒå›´
                        'text': text.strip(),
                        'bbox': bbox if bbox else []
                    })
                    subtitle_id += 1
    
    return final_subtitles
```

### **ä»»åŠ¡0.5: æ›´æ–°é…ç½®æ–‡ä»¶**

**ä¿®æ”¹æ–‡ä»¶**: `config.yml`

```yaml
# 3. å…³é”®å¸§æ£€æµ‹å™¨é…ç½® (æ›¿ä»£åŸæœ‰çš„change_detector)
keyframe_detector:
  # dHashè®¡ç®—å°ºå¯¸
  dhash_size: 8
  # ç›¸ä¼¼åº¦é˜ˆå€¼ (åŸºäºè¡Œä¸šæ ‡å‡†)
  similarity_threshold: 0.90  # 90%é»˜è®¤é˜ˆå€¼
  
  # é¢„è®¾é…ç½®é€‰é¡¹
  preset:
    high_precision: 0.95    # é«˜ç²¾åº¦ (æ±‰æ˜è·ç¦» â‰¤ 3)
    medium_precision: 0.90  # ä¸­ç²¾åº¦ (æ±‰æ˜è·ç¦» â‰¤ 6) - é»˜è®¤
    low_precision: 0.85     # ä½ç²¾åº¦ (æ±‰æ˜è·ç¦» â‰¤ 10)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] é…ç½®æ–‡ä»¶æ­£ç¡®æ›´æ–°
- [ ] æ”¯æŒé¢„è®¾çš„ç›¸ä¼¼åº¦é…ç½®
- [ ] å‘åå…¼å®¹æ€§ä¿æŒ

---

## ğŸ¯ **ç¬¬ä¸€é˜¶æ®µï¼šåŠŸèƒ½éªŒè¯å’Œä¼˜åŒ–** (ä¼˜å…ˆçº§ï¼šğŸ”¥ğŸ”¥ğŸ”¥)

### **ä»»åŠ¡1.1: JSONè¾“å‡ºæ ¼å¼æ ‡å‡†åŒ–**

**ç›®æ ‡**: åœ¨JSONè¾“å‡ºä¸­æ·»åŠ ç¼ºå¤±çš„ `keyFrame` å’Œ `frameRange` å­—æ®µ

**å½±å“æ–‡ä»¶**:
- `services/workers/paddleocr_service/app/modules/postprocessor.py`

**å®ç°ç»†èŠ‚**:

#### **æ­¥éª¤1: ä¿®æ”¹æ•°æ®ä¼ é€’ç»“æ„**

**æ–‡ä»¶**: `postprocessor.py`
**ä½ç½®**: `format()` æ–¹æ³• (ç¬¬17è¡Œ)

```python
# åŸå§‹æ–¹æ³•ç­¾å
def format(self, ocr_results: Dict[int, Tuple[str, Any]], video_fps: float, total_frames: int) -> List[Dict[str, Any]]:

# æ–°å¢æ–¹æ³•ç­¾å - éœ€è¦å…³é”®å¸§å’Œæ®µè½ä¿¡æ¯
def format_from_keyframes(self, segments: List[Dict], ocr_results: Dict[int, Tuple[str, Any]], 
                         video_fps: float) -> List[Dict[str, Any]]:
```

**ä¿®æ”¹åŸå› **: åŸºäºå…³é”®å¸§é©±åŠ¨æ¶æ„ï¼Œéœ€è¦æ®µè½ä¿¡æ¯æ¥ç”ŸæˆkeyFrameå’ŒframeRange

#### **æ­¥éª¤2: å¢å¼º_build_segmentsæ–¹æ³•**

**ä½ç½®**: `postprocessor.py` ç¬¬44è¡Œ

```python
def _build_segments_from_keyframes(self, keyframes: List[int], 
                                  ocr_results: Dict[int, Tuple[str, Any]], 
                                  total_frames: int) -> List[Dict]:
    """
    åŸºäºå…³é”®å¸§æ„å»ºæ—¶é—´æ®µï¼Œè®°å½•å…³é”®å¸§å’Œå¸§èŒƒå›´ä¿¡æ¯
    """
    if not keyframes:
        return []

    segments = []
    
    for i, keyframe in enumerate(keyframes):
        if keyframe in ocr_results:
            text, bbox = ocr_results[keyframe]
            
            # ç¡®å®šç»“æŸå¸§
            if i + 1 < len(keyframes):
                end_frame = keyframes[i + 1] - 1
            else:
                end_frame = total_frames - 1
            
            segments.append({
                'start_frame': keyframe,
                'end_frame': end_frame,
                'key_frame': keyframe,  # å…³é”®å¸§å°±æ˜¯æ®µè½èµ·å§‹å¸§
                'text': text,
                'bbox': bbox
            })
    
    return segments
```

#### **æ­¥éª¤3: æ›´æ–°_clean_and_format_segmentsæ–¹æ³•**

**ä½ç½®**: `postprocessor.py` ç¬¬83è¡Œ

```python
def _clean_and_format_segments(self, segments: List[Dict], fps: float) -> List[Dict]:
    """
    è¿‡æ»¤æ— æ•ˆæ®µè½å¹¶è½¬æ¢æ ¼å¼ï¼Œæ·»åŠ keyFrameå’ŒframeRangeå­—æ®µ
    """
    cleaned_subtitles = []
    subtitle_id = 1

    for seg in segments:
        if not seg.get('text') or not seg['text'].strip():
            continue

        start_time = seg['start_frame'] / fps
        end_time = seg['end_frame'] / fps
        duration = end_time - start_time

        if duration < self.min_duration_seconds:
            continue
        
        # å¤„ç†è¾¹ç•Œæ¡†æ ¼å¼ (ä¿æŒå››ä¸ªé¡¶ç‚¹æ ¼å¼)
        bbox = seg.get('bbox')
        if bbox and isinstance(bbox, tuple) and len(bbox) == 4:
            # è¾“å…¥æ˜¯(x1, y1, x2, y2)ï¼Œè½¬æ¢ä¸ºå››ä¸ªé¡¶ç‚¹
            x1, y1, x2, y2 = bbox
            formatted_bbox = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
        elif bbox and isinstance(bbox, list):
            # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
            formatted_bbox = bbox
        else:
            formatted_bbox = []

        cleaned_subtitles.append({
            'id': subtitle_id,
            'startTime': round(start_time, 3),
            'endTime': round(end_time, 3),
            'keyFrame': seg['key_frame'],  # ğŸ†• æ–°å¢å­—æ®µ
            'frameRange': [seg['start_frame'], seg['end_frame']],  # ğŸ†• æ–°å¢å­—æ®µ
            'text': seg['text'],
            'bbox': formatted_bbox
        })
        subtitle_id += 1
        
    return cleaned_subtitles
```

#### **æ­¥éª¤4: æ›´æ–°è°ƒç”¨é“¾**

**æ–‡ä»¶**: `logic.py` 
**ä½ç½®**: ç¬¬67è¡Œ

```python
# åŸå§‹è°ƒç”¨
final_subtitles = postprocessor.format_from_keyframes(segments, ocr_results, fps)

# æ–°è°ƒç”¨æ–¹å¼ - åŸºäºå…³é”®å¸§æ¶æ„
segments = keyframe_detector.generate_subtitle_segments(keyframes, fps, total_frames)
final_subtitles = postprocessor.format_from_keyframes(segments, ocr_results, fps)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] JSONè¾“å‡ºåŒ…å«å®Œæ•´çš„ `keyFrame` å’Œ `frameRange` å­—æ®µ
- [ ] `bbox` ä¿æŒå››ä¸ªé¡¶ç‚¹åæ ‡æ ¼å¼
- [ ] å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡
- [ ] ä½¿ç”¨ `debug_run.py -i /app/videos/223.mp4` éªŒè¯è¾“å‡ºæ ¼å¼

---

## ğŸš€ **ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½ä¼˜åŒ–åŠŸèƒ½** (ä¼˜å…ˆçº§ï¼šğŸ”¥ğŸ”¥)

### **ä»»åŠ¡2.1: æ®µè½èšåˆåŠŸèƒ½**

**ç›®æ ‡**: å®ç°æ™ºèƒ½æ®µè½èšåˆï¼Œé¿å…è¿‡åº¦åˆ†å‰²å­—å¹•

**æ–°å¢æ–‡ä»¶**: `services/workers/paddleocr_service/app/modules/segment_builder.py`

```python
# å®Œæ•´çš„æ®µè½èšåˆå™¨å®ç°
import numpy as np
from typing import List, Dict, Tuple

class SubtitleSegmentBuilder:
    """
    å­—å¹•æ®µè½æ„å»ºå™¨ - å®ç°æ™ºèƒ½æ®µè½èšåˆ
    åŸºäºå…³é”®å¸§é©±åŠ¨æ¶æ„çš„æ®µè½èšåˆ
    """
    
    def __init__(self, config):
        self.config = config
        # æ®µè½èšåˆå‚æ•°
        self.max_gap_seconds = config.get('max_gap_seconds', 1.0)  # æœ€å¤§é—´éš”
        self.min_segment_duration = config.get('min_segment_duration', 0.5)  # æœ€å°æ®µè½é•¿åº¦
        self.similarity_threshold = config.get('similarity_threshold', 0.7)  # æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼
        
    def build_segments(self, keyframes: List[int], 
                      ocr_results: Dict[int, Tuple[str, Any]], 
                      frame_rate: float, total_frames: int) -> List[Dict]:
        """
        åŸºäºå…³é”®å¸§æ„å»ºæ™ºèƒ½èšåˆçš„å­—å¹•æ®µè½
        
        Args:
            keyframes: å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
            ocr_results: OCRè¯†åˆ«ç»“æœ
            frame_rate: è§†é¢‘å¸§ç‡
            total_frames: æ€»å¸§æ•°
            
        Returns:
            èšåˆåçš„æ®µè½åˆ—è¡¨
        """
        # 1. åˆæ­¥æ„å»ºåŸå§‹æ®µè½
        raw_segments = self._build_raw_segments_from_keyframes(keyframes, ocr_results, total_frames)
        
        # 2. åº”ç”¨èšåˆè§„åˆ™
        merged_segments = self._apply_merge_rules(raw_segments, frame_rate)
        
        # 3. è´¨é‡è¿‡æ»¤
        final_segments = self._filter_by_quality(merged_segments, frame_rate)
        
        return final_segments
    
    def _build_raw_segments_from_keyframes(self, keyframes: List[int], 
                                          ocr_results: Dict[int, Tuple[str, Any]], 
                                          total_frames: int) -> List[Dict]:
        """åŸºäºå…³é”®å¸§æ„å»ºåŸå§‹æ®µè½"""
        segments = []
        
        for i, keyframe in enumerate(keyframes):
            if keyframe in ocr_results:
                text, bbox = ocr_results[keyframe]
                
                # ç¡®å®šç»“æŸå¸§
                if i + 1 < len(keyframes):
                    end_frame = keyframes[i + 1] - 1
                else:
                    end_frame = total_frames - 1
                
                segments.append({
                    'start_frame': keyframe,
                    'end_frame': end_frame,
                    'key_frame': keyframe,
                    'text': text,
                    'bbox': bbox,
                    'confidence': self._calculate_confidence(text)
                })
        
        return segments
    
    def _apply_merge_rules(self, segments: List[Dict], frame_rate: float) -> List[Dict]:
        """åº”ç”¨æ®µè½åˆå¹¶è§„åˆ™"""
        if len(segments) <= 1:
            return segments
            
        merged = []
        current_segment = segments[0].copy()
        
        for i in range(1, len(segments)):
            next_segment = segments[i]
            
            # è®¡ç®—æ—¶é—´é—´éš”
            gap_frames = next_segment['start_frame'] - current_segment['end_frame']
            gap_seconds = gap_frames / frame_rate
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
            should_merge = (
                gap_seconds <= self.max_gap_seconds and
                self._texts_are_related(current_segment['text'], next_segment['text'])
            )
            
            if should_merge:
                # åˆå¹¶æ®µè½
                current_segment['end_frame'] = next_segment['end_frame']
                current_segment['text'] = f"{current_segment['text']} {next_segment['text']}"
                # ä¿æŒç½®ä¿¡åº¦æ›´é«˜çš„è¾¹ç•Œæ¡†
                if next_segment.get('confidence', 0) > current_segment.get('confidence', 0):
                    current_segment['bbox'] = next_segment['bbox']
                    current_segment['key_frame'] = next_segment['key_frame']
            else:
                # ä¸åˆå¹¶ï¼Œä¿å­˜å½“å‰æ®µè½ï¼Œå¼€å§‹æ–°æ®µè½
                merged.append(current_segment)
                current_segment = next_segment.copy()
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
        merged.append(current_segment)
        return merged
    
    def _texts_are_related(self, text1: str, text2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦ç›¸å…³ï¼ˆç®€å•å®ç°ï¼‰"""
        if not text1 or not text2:
            return False
            
        # ç®€å•çš„è¯æ±‡é‡å æ£€æŸ¥
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return False
            
        overlap = len(words1.intersection(words2))
        total = len(words1.union(words2))
        
        return (overlap / total) >= self.similarity_threshold
    
    def _calculate_confidence(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç½®ä¿¡åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
        if not text:
            return 0.0
        
        # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå­—ç¬¦è´¨é‡çš„ç®€å•ç½®ä¿¡åº¦
        length_score = min(len(text.strip()) / 20.0, 1.0)  # é•¿åº¦å¾—åˆ†
        char_score = sum(1 for c in text if c.isalnum()) / len(text) if text else 0  # å­—ç¬¦è´¨é‡
        
        return (length_score + char_score) / 2.0
    
    def _filter_by_quality(self, segments: List[Dict], frame_rate: float) -> List[Dict]:
        """åŸºäºè´¨é‡è¿‡æ»¤æ®µè½"""
        filtered = []
        
        for segment in segments:
            duration = (segment['end_frame'] - segment['start_frame']) / frame_rate
            
            # è¿‡æ»¤æ¡ä»¶
            if (duration >= self.min_segment_duration and 
                segment.get('text', '').strip() and
                len(segment['text'].strip()) >= 2):
                
                filtered.append(segment)
        
        return filtered
```

**é›†æˆåˆ°postprocessor.py**:

```python
# åœ¨postprocessor.pyä¸­å¼•å…¥æ®µè½æ„å»ºå™¨
from .segment_builder import SubtitleSegmentBuilder

class SubtitlePostprocessor:
    def __init__(self, config):
        # ... ç°æœ‰ä»£ç  ...
        
        # ğŸ†• æ·»åŠ æ®µè½æ„å»ºå™¨
        self.segment_builder = SubtitleSegmentBuilder(config.get('segment_builder', {}))
    
    def format(self, ocr_results: Dict[int, Tuple[str, Any, ChangeType]], 
               change_events: List[Tuple[int, ChangeType]], 
               video_fps: float, total_frames: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ™ºèƒ½æ®µè½æ„å»ºå™¨é‡æ„åå¤„ç†é€»è¾‘"""
        if not ocr_results:
            return []
        
        print("å¼€å§‹æ™ºèƒ½æ®µè½æ„å»ºå’Œåå¤„ç†...")

        # ä½¿ç”¨æ™ºèƒ½æ®µè½æ„å»ºå™¨
        segments = self.segment_builder.build_segments(keyframes, ocr_results, video_fps, total_frames)
        print(f"æ™ºèƒ½èšåˆåæ„å»º {len(segments)} ä¸ªæ®µè½ã€‚")

        # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
        final_subtitles = self._convert_to_final_format(segments, video_fps)
        print(f"æœ€ç»ˆè¾“å‡º {len(final_subtitles)} æ¡å­—å¹•ã€‚")

        return final_subtitles
```

### **ä»»åŠ¡2.2: æ™ºèƒ½å¸§é€‰æ‹©ä¼˜åŒ–**

**ç›®æ ‡**: åœ¨æ®µè½ä¸­é€‰æ‹©è´¨é‡æœ€é«˜çš„å¸§è¿›è¡ŒOCRè¯†åˆ«

**ä¿®æ”¹æ–‡ä»¶**: `services/workers/paddleocr_service/app/modules/keyframe_detector.py`

**æ–°å¢æ–¹æ³•**:

```python
def select_optimal_frame(self, frame_range: Tuple[int, int], 
                        quality_scores: np.ndarray) -> int:
    """
    åœ¨æ®µè½ä¸­é€‰æ‹©è´¨é‡æœ€é«˜çš„å¸§
    åŸºäºæ–‡æ¡£ä¼˜åŒ–å»ºè®®å®ç°
    
    Args:
        frame_range: å¸§èŒƒå›´ (start_frame, end_frame)
        quality_scores: è´¨é‡åˆ†æ•°æ•°ç»„ (é€šå¸¸ä½¿ç”¨æ ‡å‡†å·®)
        
    Returns:
        æœ€ä¼˜å¸§çš„ç´¢å¼•
    """
    start_frame, end_frame = frame_range
    
    # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
    if start_frame >= end_frame or start_frame < 0:
        return start_frame
    
    # è·³è¿‡æ¸å˜æ•ˆæœå¸§ï¼ˆå¼€å¤´å’Œç»“å°¾å„2å¸§ï¼‰
    stable_start = start_frame + 2
    stable_end = end_frame - 2
    
    # å¦‚æœèŒƒå›´å¤ªå°ï¼Œä½¿ç”¨å®Œæ•´èŒƒå›´
    if stable_end <= stable_start:
        stable_start = start_frame
        stable_end = end_frame
    
    # ç¡®ä¿ç´¢å¼•åœ¨è´¨é‡åˆ†æ•°æ•°ç»„èŒƒå›´å†…
    max_index = len(quality_scores) - 1
    stable_start = min(stable_start, max_index)
    stable_end = min(stable_end, max_index)
    
    if stable_start > stable_end:
        return start_frame
    
    # åœ¨ç¨³å®šèŒƒå›´å†…é€‰æ‹©æ ‡å‡†å·®æœ€å¤§çš„å¸§
    stable_range = quality_scores[stable_start:stable_end + 1]
    if len(stable_range) == 0:
        return start_frame
        
    relative_best_idx = np.argmax(stable_range)
    absolute_best_idx = stable_start + relative_best_idx
    
    return absolute_best_idx

def get_frame_quality_scores(self, video_path: str, decoder, subtitle_area: Tuple[int, int, int, int]) -> np.ndarray:
    """
    è·å–æ‰€æœ‰å¸§çš„è´¨é‡åˆ†æ•°ï¼ˆæ ‡å‡†å·®ï¼‰
    ä¸ºæ™ºèƒ½å¸§é€‰æ‹©æä¾›æ•°æ®æ”¯æŒ
    """
    # å¤ç”¨ç°æœ‰çš„_compute_frame_featuresæ–¹æ³•
    all_hashes, all_stds = self._compute_frame_features(video_path, decoder, subtitle_area)
    return all_stds
```

**é›†æˆåˆ°OCRå¤„ç†æµç¨‹**:

```python
# åœ¨OCRå¼•æ“ä¸­ä½¿ç”¨æ™ºèƒ½å¸§é€‰æ‹©
# ä¿®æ”¹recognizeæ–¹æ³•ä»¥æ”¯æŒå¸§é€‰æ‹©ä¼˜åŒ–
def recognize_with_optimization(self, video_path: str, decoder: GPUDecoder, 
                              change_events: List[Tuple[int, ChangeType]], 
                              subtitle_area: Tuple[int, int, int, int], 
                              total_frames: int) -> Dict[int, Tuple[str, Any, ChangeType]]:
    """
    å¸¦æœ‰æ™ºèƒ½å¸§é€‰æ‹©çš„OCRè¯†åˆ«
    """
    # 1. è·å–è´¨é‡åˆ†æ•°
    quality_scores = self.keyframe_detector.get_frame_quality_scores(
        video_path, decoder, subtitle_area
    )
    
    # 2. å¯¹æ¯ä¸ªæ®µè½é€‰æ‹©æœ€ä¼˜å¸§
    optimized_keyframes = []
    for i, keyframe in enumerate(keyframes):
        if i + 1 < len(keyframes):
            # ç¡®å®šæ®µè½èŒƒå›´
            frame_range = (keyframe, keyframes[i + 1] - 1)
        else:
            frame_range = (keyframe, total_frames - 1)
        
        # é€‰æ‹©æœ€ä¼˜å¸§
        optimal_frame = self.keyframe_detector.select_optimal_frame(frame_range, quality_scores)
        optimized_keyframes.append(optimal_frame)
    
    # 3. ä½¿ç”¨ä¼˜åŒ–åçš„å…³é”®å¸§è¿›è¡ŒOCR
    return self.recognize_keyframes(video_path, decoder, optimized_keyframes, subtitle_area, total_frames)
```

---

## ğŸ”§ **ç¬¬ä¸‰é˜¶æ®µï¼šå·¥å…·å’Œè°ƒè¯•åŠŸèƒ½** (ä¼˜å…ˆçº§ï¼šğŸ”¥)

### **ä»»åŠ¡3.1: æ€§èƒ½ç›‘æ§ç³»ç»Ÿ**

**æ–°å¢æ–‡ä»¶**: `services/workers/paddleocr_service/app/utils/performance_monitor.py`

```python
import time
import psutil
import GPUtil
import numpy as np
from typing import Dict, List, Any
from dataclasses import dataclass, field
from contextlib import contextmanager

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    # å¤„ç†æ—¶é—´æŒ‡æ ‡
    total_processing_time: float = 0.0
    area_detection_time: float = 0.0
    change_detection_time: float = 0.0
    ocr_processing_time: float = 0.0
    postprocessing_time: float = 0.0
    
    # OCRè°ƒç”¨ç»Ÿè®¡
    total_frames: int = 0
    ocr_calls: int = 0
    ocr_reduction_ratio: float = 0.0
    
    # ç³»ç»Ÿèµ„æº
    peak_memory_usage: float = 0.0
    average_gpu_utilization: float = 0.0
    gpu_memory_used: float = 0.0
    
    # è´¨é‡æŒ‡æ ‡
    successful_recognitions: int = 0
    failed_recognitions: int = 0
    success_rate: float = 0.0
    
    # æ€§èƒ½æå‡æŒ‡æ ‡
    theoretical_processing_time: float = 0.0  # å…¨å¸§å¤„ç†é¢„ä¼°æ—¶é—´
    actual_speedup: float = 0.0

class PerformanceMonitor:
    """
    æ€§èƒ½ç›‘æ§å™¨ - éªŒè¯æ–‡æ¡£ä¸­å£°ç§°çš„æ€§èƒ½æŒ‡æ ‡
    å®ç°æ–‡æ¡£ä¸­æåˆ°çš„æ€§èƒ½æå‡æµ‹é‡
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.metrics = PerformanceMetrics()
        self.start_time = None
        self.gpu_samples = []
        self.memory_samples = []
        
    @contextmanager
    def measure_stage(self, stage_name: str):
        """æµ‹é‡ç‰¹å®šé˜¶æ®µçš„æ‰§è¡Œæ—¶é—´"""
        start_time = time.time()
        try:
            yield
        finally:
            elapsed = time.time() - start_time
            setattr(self.metrics, f"{stage_name}_time", elapsed)
            print(f"â±ï¸ {stage_name}é˜¶æ®µè€—æ—¶: {elapsed:.2f}ç§’")
    
    def start_monitoring(self):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        self.start_time = time.time()
        self._sample_system_resources()
        print("ğŸš€ æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """ç»“æŸç›‘æ§å¹¶è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        if self.start_time:
            self.metrics.total_processing_time = time.time() - self.start_time
        
        self._calculate_final_metrics()
        self._sample_system_resources()  # æœ€åä¸€æ¬¡é‡‡æ ·
        print("ğŸ“Š æ€§èƒ½ç›‘æ§å·²å®Œæˆ")
        
        return self.get_performance_report()
    
    def track_ocr_calls(self, total_frames: int, actual_ocr_calls: int):
        """ç»Ÿè®¡OCRè°ƒç”¨æ¬¡æ•°"""
        self.metrics.total_frames = total_frames
        self.metrics.ocr_calls = actual_ocr_calls
        
        if total_frames > 0:
            self.metrics.ocr_reduction_ratio = (1 - actual_ocr_calls / total_frames) * 100
        
        # ä¼°ç®—ç†è®ºå¤„ç†æ—¶é—´ï¼ˆå‡è®¾æ¯å¸§OCRè€—æ—¶0.1ç§’ï¼‰
        self.metrics.theoretical_processing_time = total_frames * 0.1
    
    def track_ocr_results(self, successful: int, failed: int):
        """ç»Ÿè®¡OCRè¯†åˆ«ç»“æœ"""
        self.metrics.successful_recognitions = successful
        self.metrics.failed_recognitions = failed
        
        total = successful + failed
        if total > 0:
            self.metrics.success_rate = (successful / total) * 100
    
    def _sample_system_resources(self):
        """é‡‡æ ·ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            # å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.memory_samples.append(memory_mb)
            self.metrics.peak_memory_usage = max(self.memory_samples)
            
            # GPUä½¿ç”¨ç‡
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # å‡è®¾ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
                self.gpu_samples.append(gpu.load * 100)
                self.metrics.gpu_memory_used = gpu.memoryUsed
                
        except Exception as e:
            print(f"âš ï¸ èµ„æºç›‘æ§é‡‡æ ·å¤±è´¥: {e}")
    
    def _calculate_final_metrics(self):
        """è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡"""
        if self.gpu_samples:
            self.metrics.average_gpu_utilization = np.mean(self.gpu_samples)
        
        # è®¡ç®—å®é™…åŠ é€Ÿæ¯”
        if self.metrics.theoretical_processing_time > 0:
            self.metrics.actual_speedup = (
                self.metrics.theoretical_processing_time / 
                self.metrics.total_processing_time
            )
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        return {
            "ğŸ“Š å¤„ç†æ€§èƒ½": {
                "æ€»å¤„ç†æ—¶é—´": f"{self.metrics.total_processing_time:.2f}ç§’",
                "åŒºåŸŸæ£€æµ‹è€—æ—¶": f"{self.metrics.area_detection_time:.2f}ç§’",
                "å˜åŒ–æ£€æµ‹è€—æ—¶": f"{self.metrics.change_detection_time:.2f}ç§’",
                "OCRè¯†åˆ«è€—æ—¶": f"{self.metrics.ocr_processing_time:.2f}ç§’",
                "åå¤„ç†è€—æ—¶": f"{self.metrics.postprocessing_time:.2f}ç§’"
            },
            "ğŸš€ æ•ˆç‡æå‡": {
                "æ€»å¸§æ•°": self.metrics.total_frames,
                "å®é™…OCRè°ƒç”¨": self.metrics.ocr_calls,
                "è°ƒç”¨å‡å°‘ç‡": f"{self.metrics.ocr_reduction_ratio:.1f}%",
                "ç†è®ºå¤„ç†æ—¶é—´": f"{self.metrics.theoretical_processing_time:.2f}ç§’",
                "å®é™…åŠ é€Ÿæ¯”": f"{self.metrics.actual_speedup:.1f}x"
            },
            "ğŸ’¾ ç³»ç»Ÿèµ„æº": {
                "å³°å€¼å†…å­˜ä½¿ç”¨": f"{self.metrics.peak_memory_usage:.1f}MB",
                "å¹³å‡GPUä½¿ç”¨ç‡": f"{self.metrics.average_gpu_utilization:.1f}%",
                "GPUæ˜¾å­˜ä½¿ç”¨": f"{self.metrics.gpu_memory_used:.1f}MB"
            },
            "âœ… è¯†åˆ«è´¨é‡": {
                "æˆåŠŸè¯†åˆ«": self.metrics.successful_recognitions,
                "è¯†åˆ«å¤±è´¥": self.metrics.failed_recognitions,
                "æˆåŠŸç‡": f"{self.metrics.success_rate:.1f}%"
            }
        }
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½æ€»ç»“"""
        report = self.get_performance_report()
        
        print("\n" + "="*60)
        print("ğŸ“ˆ YiVideoå­—å¹•æå–æ€§èƒ½æŠ¥å‘Š")
        print("="*60)
        
        for category, metrics in report.items():
            print(f"\n{category}:")
            for key, value in metrics.items():
                print(f"  {key}: {value}")
        
        # éªŒè¯æ–‡æ¡£å£°ç§°çš„æ€§èƒ½æå‡
        print(f"\nğŸ¯ æ–‡æ¡£éªŒè¯:")
        print(f"  OCRè°ƒç”¨å‡å°‘: {self.metrics.ocr_reduction_ratio:.1f}% (æ–‡æ¡£å£°ç§°: >95%)")
        print(f"  å¤„ç†é€Ÿåº¦æå‡: {self.metrics.actual_speedup:.1f}x (æ–‡æ¡£å£°ç§°: 50x)")
        print(f"  GPUåˆ©ç”¨ç‡: {self.metrics.average_gpu_utilization:.1f}% (æ–‡æ¡£å£°ç§°: 40%+)")
```

**é›†æˆåˆ°ä¸»æµç¨‹**:

```python
# åœ¨logic.pyä¸­é›†æˆæ€§èƒ½ç›‘æ§
from app.utils.performance_monitor import PerformanceMonitor

def extract_subtitles_from_video(video_path: str, config: Dict) -> List[Dict[str, Any]]:
    """é›†æˆæ€§èƒ½ç›‘æ§çš„å­—å¹•æå–å‡½æ•°"""
    
    # ğŸ†• å¯åŠ¨æ€§èƒ½ç›‘æ§
    monitor = PerformanceMonitor(config.get('performance_monitor', {}))
    monitor.start_monitoring()
    
    try:
        # 1. åˆå§‹åŒ–æ¨¡å—
        decoder = GPUDecoder(config.get('decoder', {}))
        area_detector = SubtitleAreaDetector(config.get('area_detector', {}))
        keyframe_detector = KeyFrameDetector(config.get('keyframe_detector', {}))  # ğŸ†• æ–°æ£€æµ‹å™¨
        ocr_engine = MultiProcessOCREngine(config.get('ocr', {}))
        postprocessor = SubtitlePostprocessor(config.get('postprocessor', {}))
        
        # 2. è·å–è§†é¢‘å…ƒæ•°æ®
        fps, total_frames = _get_video_metadata(video_path)
        
        # 3. æ™ºèƒ½å­—å¹•åŒºåŸŸæ£€æµ‹
        with monitor.measure_stage("area_detection"):
            subtitle_area = area_detector.detect(video_path, decoder)
            if subtitle_area is None:
                return []
        
        # 4. å…³é”®å¸§æ£€æµ‹ (æ–°é€»è¾‘)
        with monitor.measure_stage("keyframe_detection"):  # ğŸ†• æ›´æ–°stageåç§°
            keyframes = keyframe_detector.detect_keyframes(video_path, decoder, subtitle_area)  # ğŸ†• æ–°æ–¹æ³•
        
        # 5. OCRè¯†åˆ«
        with monitor.measure_stage("ocr_processing"):
            ocr_results = ocr_engine.recognize_keyframes(video_path, decoder, keyframes, subtitle_area, total_frames)  # ğŸ†• æ–°æ–¹æ³•
            
            # ç»Ÿè®¡OCRè°ƒç”¨
            monitor.track_ocr_calls(total_frames, len(keyframes))  # ğŸ†• æ›´æ–°ä¸ºå…³é”®å¸§æ•°é‡
            success_count = len([r for r in ocr_results.values() if r[0].strip()])
            monitor.track_ocr_results(success_count, len(ocr_results) - success_count)
        
        # 6. åå¤„ç†
        with monitor.measure_stage("postprocessing"):
            # ğŸ†• ç”Ÿæˆæ®µè½ä¿¡æ¯
            segments = keyframe_detector.generate_subtitle_segments(keyframes, fps, total_frames)
            final_subtitles = postprocessor.format_from_keyframes(segments, ocr_results, fps)  # ğŸ†• æ–°æ–¹æ³•
        
        return final_subtitles
        
    finally:
        # ğŸ†• è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        monitor.stop_monitoring()
        monitor.print_performance_summary()
```

### **ä»»åŠ¡3.2: è°ƒè¯•åˆ†æå·¥å…·**

**æ–°å¢æ–‡ä»¶**: `services/workers/paddleocr_service/app/utils/debug_analyzer.py`

```python
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
from typing import List, Tuple, Dict, Any
from ..modules.keyframe_detector import KeyFrameDetector  # ğŸ†• æ–°çš„æ£€æµ‹å™¨

class DebugAnalyzer:
    """
    è°ƒè¯•åˆ†æå·¥å…· - å®ç°æ–‡æ¡£ä¸­çš„è°ƒè¯•æ–¹æ³•
    æä¾›å¯è§†åŒ–åˆ†æå’Œè´¨é‡è¯Šæ–­åŠŸèƒ½
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.debug_dir = config.get('debug_dir', './debug_output')
        os.makedirs(self.debug_dir, exist_ok=True)
        
    def analyze_detection_quality(self, video_path: str, detector: KeyFrameDetector,  # ğŸ†• æ›´æ–°å‚æ•°ç±»å‹ 
                                 decoder, subtitle_area: Tuple[int, int, int, int]) -> Dict[str, Any]:
        """
        åˆ†ææ£€æµ‹è´¨é‡çš„è°ƒè¯•å·¥å…·
        å®ç°æ–‡æ¡£ç¬¬381-392è¡Œçš„è°ƒè¯•æ–¹æ³•
        """
        print("ğŸ” å¼€å§‹æ£€æµ‹è´¨é‡åˆ†æ...")
        
        # 1. è·å–æ‰€æœ‰å¸§çš„æŒ‡æ ‡æ•°æ®
        all_hashes, all_stds = detector._compute_frame_features(
            video_path, decoder, subtitle_area
        )
        
        # 2. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = self._calculate_statistics(all_stds)
        
        # 3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualization(all_stds, stats)
        
        # 4. åˆ†æé˜ˆå€¼æ•ˆæœ
        threshold_analysis = self._analyze_threshold_effects(all_stds, all_hashes)
        
        report = {
            "ç»Ÿè®¡æ•°æ®": stats,
            "é˜ˆå€¼åˆ†æ": threshold_analysis,
            "å»ºè®®": self._generate_recommendations(stats, threshold_analysis)
        }
        
        print("âœ… æ£€æµ‹è´¨é‡åˆ†æå®Œæˆ")
        return report
    
    def _calculate_statistics(self, stds: np.ndarray) -> Dict[str, Any]:
        """è®¡ç®—æ ‡å‡†å·®ç»Ÿè®¡æ•°æ®"""
        return {
            "æ€»å¸§æ•°": len(stds),
            "å¹³å‡æ ‡å‡†å·®": np.mean(stds),
            "æ ‡å‡†å·®ä¸­ä½æ•°": np.median(stds),
            "æ ‡å‡†å·®èŒƒå›´": f"{np.min(stds):.4f} - {np.max(stds):.4f}",
            "æ ‡å‡†å·®æ ‡å‡†å·®": np.std(stds),  # å˜åŒ–ç¨‹åº¦
            "å¤§æ´¥é˜ˆå€¼": self._calculate_otsu_threshold(stds)
        }
    
    def _calculate_otsu_threshold(self, stds: np.ndarray) -> float:
        """ä½¿ç”¨å¤§æ´¥æ³•è®¡ç®—é˜ˆå€¼"""
        stds_normalized = (255 * (stds - stds.min()) / (stds.max() - stds.min())).astype(np.uint8)
        threshold_otsu, _ = cv2.threshold(stds_normalized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return threshold_otsu / 255 * (stds.max() - stds.min()) + stds.min()
    
    def _generate_visualization(self, stds: np.ndarray, stats: Dict[str, Any]):
        """ç”Ÿæˆæ ‡å‡†å·®åˆ†å¸ƒç›´æ–¹å›¾"""
        plt.figure(figsize=(12, 8))
        
        # å­å›¾1: æ ‡å‡†å·®æ—¶é—´åºåˆ—
        plt.subplot(2, 2, 1)
        plt.plot(stds)
        plt.title("æ ‡å‡†å·®æ—¶é—´åºåˆ—")
        plt.xlabel("å¸§ç¼–å·")
        plt.ylabel("åƒç´ æ ‡å‡†å·®")
        plt.axhline(y=stats["å¤§æ´¥é˜ˆå€¼"], color='r', linestyle='--', label=f'å¤§æ´¥é˜ˆå€¼: {stats["å¤§æ´¥é˜ˆå€¼"]:.4f}')
        plt.legend()
        
        # å­å›¾2: æ ‡å‡†å·®åˆ†å¸ƒç›´æ–¹å›¾
        plt.subplot(2, 2, 2)
        plt.hist(stds, bins=50, alpha=0.7, edgecolor='black')
        plt.title("åƒç´ æ ‡å‡†å·®åˆ†å¸ƒ")
        plt.xlabel("åƒç´ æ ‡å‡†å·®")
        plt.ylabel("å¸§æ•°")
        plt.axvline(x=stats["å¤§æ´¥é˜ˆå€¼"], color='r', linestyle='--', label=f'å¤§æ´¥é˜ˆå€¼: {stats["å¤§æ´¥é˜ˆå€¼"]:.4f}')
        plt.legend()
        
        # å­å›¾3: ç´¯ç§¯åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        sorted_stds = np.sort(stds)
        cumulative = np.arange(1, len(sorted_stds) + 1) / len(sorted_stds)
        plt.plot(sorted_stds, cumulative)
        plt.title("æ ‡å‡†å·®ç´¯ç§¯åˆ†å¸ƒ")
        plt.xlabel("åƒç´ æ ‡å‡†å·®")
        plt.ylabel("ç´¯ç§¯æ¦‚ç‡")
        plt.axvline(x=stats["å¤§æ´¥é˜ˆå€¼"], color='r', linestyle='--', label=f'å¤§æ´¥é˜ˆå€¼: {stats["å¤§æ´¥é˜ˆå€¼"]:.4f}')
        plt.legend()
        
        # å­å›¾4: ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
        plt.subplot(2, 2, 4)
        plt.axis('off')
        info_text = "\n".join([f"{k}: {v}" for k, v in stats.items()])
        plt.text(0.1, 0.9, info_text, transform=plt.gca().transAxes, 
                verticalalignment='top', fontsize=10, fontfamily='monospace')
        plt.title("ç»Ÿè®¡ä¿¡æ¯")
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.debug_dir, 'detection_quality_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {self.debug_dir}/detection_quality_analysis.png")
    
    def _analyze_threshold_effects(self, stds: np.ndarray, hashes: List[np.ndarray]) -> Dict[str, Any]:
        """åˆ†æä¸åŒé˜ˆå€¼å¯¹æ£€æµ‹æ•ˆæœçš„å½±å“"""
        otsu_threshold = self._calculate_otsu_threshold(stds)
        
        # æµ‹è¯•ä¸åŒçš„æ±‰æ˜è·ç¦»é˜ˆå€¼
        hamming_thresholds = [1, 2, 3, 4, 5, 6, 7]
        threshold_results = {}
        
        for threshold in hamming_thresholds:
            change_count = self._count_changes_with_threshold(hashes, threshold)
            blank_frames = np.sum(stds < otsu_threshold)
            content_frames = len(stds) - blank_frames
            
            threshold_results[threshold] = {
                "æ£€æµ‹åˆ°å˜åŒ–æ•°": change_count,
                "ç©ºç™½å¸§æ•°": int(blank_frames),
                "å†…å®¹å¸§æ•°": int(content_frames),
                "å˜åŒ–å¯†åº¦": change_count / len(stds) if len(stds) > 0 else 0
            }
        
        return threshold_results
    
    def _count_changes_with_threshold(self, hashes: List[np.ndarray], threshold: int) -> int:
        """ä½¿ç”¨æŒ‡å®šé˜ˆå€¼è®¡ç®—å˜åŒ–æ¬¡æ•°"""
        if len(hashes) < 2:
            return 0
        
        changes = 0
        for i in range(1, len(hashes)):
            hamming_distance = np.sum(hashes[i] != hashes[i-1])
            if hamming_distance > threshold:
                changes += 1
        
        return changes
    
    def _generate_recommendations(self, stats: Dict[str, Any], 
                                threshold_analysis: Dict[str, Any]) -> List[str]:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆè°ƒæ•´å»ºè®®"""
        recommendations = []
        
        # åˆ†æå˜åŒ–å¯†åº¦ï¼Œç»™å‡ºé˜ˆå€¼è°ƒæ•´å»ºè®®
        densities = [result["å˜åŒ–å¯†åº¦"] for result in threshold_analysis.values()]
        avg_density = np.mean(densities)
        
        if avg_density > 0.1:  # å˜åŒ–è¿‡äºé¢‘ç¹
            recommendations.append("âš ï¸ æ£€æµ‹åˆ°è¿‡å¤šå˜åŒ–ç‚¹ï¼Œå»ºè®®å¢å¤§ hamming_threshold (3â†’4æˆ–5)")
        elif avg_density < 0.01:  # å˜åŒ–å¤ªå°‘
            recommendations.append("âš ï¸ æ£€æµ‹åˆ°å˜åŒ–ç‚¹è¿‡å°‘ï¼Œå»ºè®®å‡å° hamming_threshold (3â†’2æˆ–1)")
        else:
            recommendations.append("âœ… å½“å‰æ£€æµ‹çµæ•åº¦é€‚ä¸­")
        
        # åŸºäºæ ‡å‡†å·®åˆ†å¸ƒç»™å‡ºå»ºè®®
        if stats["æ ‡å‡†å·®æ ‡å‡†å·®"] > 20:  # å˜åŒ–å¾ˆå¤§
            recommendations.append("ğŸ’¡ è§†é¢‘å†…å®¹å˜åŒ–è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´åŒºåŸŸæ£€æµ‹å‚æ•°")
        
        # åŸºäºé˜ˆå€¼åˆ†æç»™å‡ºå…·ä½“å»ºè®®
        best_threshold = min(threshold_analysis.keys(), 
                           key=lambda k: abs(threshold_analysis[k]["å˜åŒ–å¯†åº¦"] - 0.05))
        recommendations.append(f"ğŸ¯ æ¨è hamming_threshold è®¾ç½®ä¸º: {best_threshold}")
        
        return recommendations
    
    def visualize_keyframes(self, stds: np.ndarray, 
                           keyframes: List[int], 
                           video_path: str):
        """å¯è§†åŒ–å…³é”®å¸§æ£€æµ‹ç»“æœ"""
        plt.figure(figsize=(15, 8))
        
        # ç»˜åˆ¶æ ‡å‡†å·®æ›²çº¿
        frames = np.arange(len(stds))
        plt.plot(frames, stds, 'b-', alpha=0.6, label='åƒç´ æ ‡å‡†å·®')
        
        # æ ‡è®°å…³é”®å¸§
        for keyframe_idx in keyframes:
            if keyframe_idx < len(stds):
                plt.axvline(x=keyframe_idx, color='red', 
                           alpha=0.8, linewidth=2)
                plt.annotate(f'å…³é”®å¸§ {keyframe_idx}', 
                           xy=(keyframe_idx, stds[keyframe_idx]),
                           xytext=(keyframe_idx, stds[keyframe_idx] + 10),
                           rotation=90, fontsize=8,
                           arrowprops=dict(arrowstyle='->', color='red'))
        
        plt.title(f"å…³é”®å¸§æ£€æµ‹å¯è§†åŒ– - {os.path.basename(video_path)}")
        plt.xlabel("å¸§ç¼–å·")
        plt.ylabel("åƒç´ æ ‡å‡†å·®")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        output_path = os.path.join(self.debug_dir, 'keyframes_visualization.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å…³é”®å¸§å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_path}")
    
    def generate_debug_report(self, video_path: str, analysis_results: Dict[str, Any], 
                            performance_metrics: Dict[str, Any]):
        """ç”Ÿæˆå®Œæ•´çš„è°ƒè¯•æŠ¥å‘Š"""
        report_path = os.path.join(self.debug_dir, 'debug_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# YiVideoè°ƒè¯•æŠ¥å‘Š\n\n")
            f.write(f"**è§†é¢‘æ–‡ä»¶**: {video_path}\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ” æ£€æµ‹è´¨é‡åˆ†æ\n\n")
            for key, value in analysis_results["ç»Ÿè®¡æ•°æ®"].items():
                f.write(f"- **{key}**: {value}\n")
            
            f.write("\n## âš™ï¸ é˜ˆå€¼åˆ†æ\n\n")
            f.write("| æ±‰æ˜é˜ˆå€¼ | å˜åŒ–ç‚¹æ•° | ç©ºç™½å¸§ | å†…å®¹å¸§ | å˜åŒ–å¯†åº¦ |\n")
            f.write("|----------|----------|--------|--------|----------|\n")
            for threshold, result in analysis_results["é˜ˆå€¼åˆ†æ"].items():
                f.write(f"| {threshold} | {result['æ£€æµ‹åˆ°å˜åŒ–æ•°']} | {result['ç©ºç™½å¸§æ•°']} | "
                       f"{result['å†…å®¹å¸§æ•°']} | {result['å˜åŒ–å¯†åº¦']:.4f} |\n")
            
            f.write("\n## ğŸ’¡ è°ƒæ•´å»ºè®®\n\n")
            for recommendation in analysis_results["å»ºè®®"]:
                f.write(f"- {recommendation}\n")
            
            f.write(f"\n## ğŸ“Š æ€§èƒ½æŒ‡æ ‡\n\n")
            for category, metrics in performance_metrics.items():
                f.write(f"### {category}\n\n")
                for key, value in metrics.items():
                    f.write(f"- **{key}**: {value}\n")
                f.write("\n")
        
        print(f"ğŸ“‹ è°ƒè¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
```

---

## ğŸ§ª **ç¬¬å››é˜¶æ®µï¼šéªŒæ”¶å’Œæµ‹è¯•** (ä¼˜å…ˆçº§ï¼šâš¡)

### **ä»»åŠ¡4.1: ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶**

**æ–°å¢æ–‡ä»¶**: `services/workers/paddleocr_service/test_enhanced_features.py`

```python
#!/usr/bin/env python3
"""
YiVideoå¢å¼ºåŠŸèƒ½ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶
éªŒè¯æ‰€æœ‰æ–°å¢åŠŸèƒ½çš„æ­£ç¡®æ€§
"""

import os
import sys
import yaml
import json
import time
import tempfile
from typing import Dict, List, Any

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥æ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from logic import extract_subtitles_from_video
from utils.performance_monitor import PerformanceMonitor
from utils.debug_analyzer import DebugAnalyzer

def load_test_config() -> Dict:
    """åŠ è½½æµ‹è¯•é…ç½®"""
    config_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'config.yml')
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æ·»åŠ æµ‹è¯•ç‰¹å®šé…ç½®
    config['performance_monitor'] = {'enabled': True}
    config['debug_analyzer'] = {'debug_dir': './test_debug_output'}
    config['segment_builder'] = {
        'max_gap_seconds': 1.0,
        'min_segment_duration': 0.5,
        'similarity_threshold': 0.7
    }
    
    return config

def test_json_format_enhancement(video_path: str, config: Dict) -> bool:
    """æµ‹è¯•JSONæ ¼å¼å¢å¼ºåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•JSONæ ¼å¼å¢å¼º...")
    
    try:
        subtitles = extract_subtitles_from_video(video_path, config)
        
        if not subtitles:
            print("âŒ æ²¡æœ‰æå–åˆ°å­—å¹•")
            return False
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['id', 'startTime', 'endTime', 'text', 'bbox', 'keyFrame', 'frameRange']
        
        for subtitle in subtitles:
            missing_fields = [field for field in required_fields if field not in subtitle]
            if missing_fields:
                print(f"âŒ å­—å¹•æ¡ç›®ç¼ºå°‘å­—æ®µ: {missing_fields}")
                return False
            
            # éªŒè¯å­—æ®µç±»å‹
            if not isinstance(subtitle['keyFrame'], int):
                print(f"âŒ keyFrameå­—æ®µç±»å‹é”™è¯¯: {type(subtitle['keyFrame'])}")
                return False
            
            if not isinstance(subtitle['frameRange'], list) or len(subtitle['frameRange']) != 2:
                print(f"âŒ frameRangeå­—æ®µæ ¼å¼é”™è¯¯: {subtitle['frameRange']}")
                return False
            
            if not isinstance(subtitle['bbox'], list):
                print(f"âŒ bboxå­—æ®µæ ¼å¼é”™è¯¯: {type(subtitle['bbox'])}")
                return False
        
        print(f"âœ… JSONæ ¼å¼æµ‹è¯•é€šè¿‡ï¼Œæå–åˆ°{len(subtitles)}æ¡å­—å¹•")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_output = os.path.join(os.path.dirname(video_path), 'test_enhanced_output.json')
        with open(test_output, 'w', encoding='utf-8') as f:
            json.dump(subtitles, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_output}")
        return True
        
    except Exception as e:
        print(f"âŒ JSONæ ¼å¼æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_monitoring(video_path: str, config: Dict) -> bool:
    """æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ€§èƒ½ç›‘æ§...")
    
    try:
        monitor = PerformanceMonitor(config.get('performance_monitor', {}))
        monitor.start_monitoring()
        
        # æ‰§è¡Œå­—å¹•æå–
        subtitles = extract_subtitles_from_video(video_path, config)
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = monitor.stop_monitoring()
        
        # éªŒè¯æ€§èƒ½æŠ¥å‘Šå†…å®¹
        required_sections = ['ğŸ“Š å¤„ç†æ€§èƒ½', 'ğŸš€ æ•ˆç‡æå‡', 'ğŸ’¾ ç³»ç»Ÿèµ„æº', 'âœ… è¯†åˆ«è´¨é‡']
        for section in required_sections:
            if section not in report:
                print(f"âŒ æ€§èƒ½æŠ¥å‘Šç¼ºå°‘ç« èŠ‚: {section}")
                return False
        
        # æ‰“å°æ€§èƒ½æ€»ç»“
        monitor.print_performance_summary()
        
        print("âœ… æ€§èƒ½ç›‘æ§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_debug_analyzer(video_path: str, config: Dict) -> bool:
    """æµ‹è¯•è°ƒè¯•åˆ†æå·¥å…·"""
    print("\nğŸ§ª æµ‹è¯•è°ƒè¯•åˆ†æå·¥å…·...")
    
    try:
        from modules.keyframe_detector import KeyFrameDetector  # ğŸ†• æ–°çš„æ£€æµ‹å™¨
        from modules.decoder import GPUDecoder
        from modules.area_detector import SubtitleAreaDetector
        
        # åˆå§‹åŒ–ç»„ä»¶
        decoder = GPUDecoder(config.get('decoder', {}))
        area_detector = SubtitleAreaDetector(config.get('area_detector', {}))
        keyframe_detector = KeyFrameDetector(config.get('keyframe_detector', {}))  # ğŸ†• æ–°æ£€æµ‹å™¨
        analyzer = DebugAnalyzer(config.get('debug_analyzer', {}))
        
        # æ£€æµ‹å­—å¹•åŒºåŸŸ
        subtitle_area = area_detector.detect(video_path, decoder)
        if subtitle_area is None:
            print("âŒ æ— æ³•æ£€æµ‹åˆ°å­—å¹•åŒºåŸŸ")
            return False
        
        # è¿è¡Œè´¨é‡åˆ†æ
        analysis_results = analyzer.analyze_detection_quality(
            video_path, keyframe_detector, decoder, subtitle_area
        )
        
        # éªŒè¯åˆ†æç»“æœ
        required_keys = ['ç»Ÿè®¡æ•°æ®', 'é˜ˆå€¼åˆ†æ', 'å»ºè®®']
        for key in required_keys:
            if key not in analysis_results:
                print(f"âŒ åˆ†æç»“æœç¼ºå°‘é”®: {key}")
                return False
        
        print("âœ… è°ƒè¯•åˆ†æå·¥å…·æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•åˆ†æå·¥å…·æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_integration_test(video_path: str) -> bool:
    """è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹YiVideoå¢å¼ºåŠŸèƒ½é›†æˆæµ‹è¯•")
    print(f"ğŸ“¹ æµ‹è¯•è§†é¢‘: {video_path}")
    
    if not os.path.exists(video_path):
        print(f"âŒ æµ‹è¯•è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return False
    
    # åŠ è½½é…ç½®
    config = load_test_config()
    
    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    test_results = {}
    
    # æµ‹è¯•1: JSONæ ¼å¼å¢å¼º
    test_results['json_format'] = test_json_format_enhancement(video_path, config)
    
    # æµ‹è¯•2: æ€§èƒ½ç›‘æ§
    test_results['performance_monitoring'] = test_performance_monitoring(video_path, config)
    
    # æµ‹è¯•3: è°ƒè¯•åˆ†æ
    test_results['debug_analyzer'] = test_debug_analyzer(video_path, config)
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*60)
    
    all_passed = True
    for test_name, passed in test_results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:20}: {status}")
        if not passed:
            all_passed = False
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡' if all_passed else 'âŒ å­˜åœ¨æµ‹è¯•å¤±è´¥'}")
    
    return all_passed

if __name__ == '__main__':
    # é»˜è®¤æµ‹è¯•è§†é¢‘è·¯å¾„
    test_video = '/app/videos/223.mp4'
    
    if len(sys.argv) > 1:
        test_video = sys.argv[1]
    
    success = run_integration_test(test_video)
    sys.exit(0 if success else 1)
```

### **ä»»åŠ¡4.2: é…ç½®æ–‡ä»¶æ›´æ–°**

**ä¿®æ”¹æ–‡ä»¶**: `config.yml`

```yaml
# æ–°å¢çš„å¢å¼ºåŠŸèƒ½é…ç½®
# åœ¨ç°æœ‰é…ç½®åŸºç¡€ä¸Šè¿½åŠ ä»¥ä¸‹å†…å®¹

# 6. æ®µè½èšåˆå™¨é…ç½®
segment_builder:
  # æ®µè½é—´æœ€å¤§é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´ä¸è¿›è¡Œèšåˆ
  max_gap_seconds: 1.0
  # æ®µè½æœ€å°æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
  min_segment_duration: 0.5
  # æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºç›¸å…³å†…å®¹
  similarity_threshold: 0.7

# 7. æ€§èƒ½ç›‘æ§é…ç½®
performance_monitor:
  # æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
  enabled: true
  # ç³»ç»Ÿèµ„æºé‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
  sample_interval: 1.0
  # æ˜¯å¦ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
  detailed_report: true

# 8. è°ƒè¯•åˆ†æå™¨é…ç½®
debug_analyzer:
  # è°ƒè¯•è¾“å‡ºç›®å½•
  debug_dir: "./debug_output"
  # æ˜¯å¦ä¿å­˜å¯è§†åŒ–å›¾è¡¨
  save_visualizations: true
  # æ˜¯å¦ç”Ÿæˆè°ƒè¯•æŠ¥å‘Š
  generate_report: true
  # å›¾è¡¨DPIè®¾ç½®
  plot_dpi: 300

# 9. å¢å¼ºåŠŸèƒ½å¼€å…³
enhanced_features:
  # æ˜¯å¦å¯ç”¨æ™ºèƒ½æ®µè½èšåˆ
  enable_segment_building: true
  # æ˜¯å¦å¯ç”¨æ™ºèƒ½å¸§é€‰æ‹©
  enable_optimal_frame_selection: true
  # æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
  enable_performance_monitoring: true
  # æ˜¯å¦å¯ç”¨è°ƒè¯•åˆ†æ
  enable_debug_analysis: false  # é»˜è®¤å…³é—­ï¼Œé¿å…å½±å“æ€§èƒ½
```

---

## ğŸ“‹ **å®æ–½è®¡åˆ’**

### **ç¬¬ä¸€é˜¶æ®µå®æ–½æ¸…å•** (1-2å‘¨)

1. **JSONæ ¼å¼å¢å¼º** âœ…
   - [ ] ä¿®æ”¹ `postprocessor.py` çš„ `format()` æ–¹æ³•ç­¾å
   - [ ] å®ç° `_build_segments()` æ–¹æ³•å¢å¼º
   - [ ] å®ç° `_clean_and_format_segments()` æ–¹æ³•æ›´æ–°
   - [ ] æ›´æ–° `logic.py` è°ƒç”¨é“¾
   - [ ] æ‰§è¡Œ `test_enhanced_features.py` éªŒè¯

2. **é…ç½®æ–‡ä»¶æ›´æ–°** âœ…
   - [ ] æ›´æ–° `config.yml` æ·»åŠ æ–°é…ç½®é¡¹
   - [ ] æµ‹è¯•é…ç½®åŠ è½½å’ŒéªŒè¯

### **ç¬¬äºŒé˜¶æ®µå®æ–½æ¸…å•** (2-4å‘¨)

3. **æ™ºèƒ½ä¼˜åŒ–åŠŸèƒ½** ğŸ“‹
   - [ ] åˆ›å»º `segment_builder.py` å®Œæ•´å®ç°
   - [ ] é›†æˆæ®µè½èšåˆå™¨åˆ° `postprocessor.py`
   - [ ] å®ç° `keyframe_detector.py` ä¸­çš„æ™ºèƒ½å¸§é€‰æ‹©
   - [ ] æµ‹è¯•ä¼˜åŒ–æ•ˆæœå’Œæ€§èƒ½æå‡

### **ç¬¬ä¸‰é˜¶æ®µå®æ–½æ¸…å•** (4-6å‘¨)

4. **ç›‘æ§å’Œè°ƒè¯•å·¥å…·** ğŸ”§
   - [ ] åˆ›å»º `performance_monitor.py` å®Œæ•´å®ç°
   - [ ] åˆ›å»º `debug_analyzer.py` å®Œæ•´å®ç°
   - [ ] é›†æˆåˆ°ä¸»æµç¨‹ `logic.py`
   - [ ] åˆ›å»ºæµ‹è¯•å¥—ä»¶ `test_enhanced_features.py`

### **éªŒæ”¶æ ‡å‡†** âœ…

- [ ] **åŠŸèƒ½å®Œæ•´æ€§**: æ‰€æœ‰æ–°åŠŸèƒ½æŒ‰æ–‡æ¡£è¦æ±‚å®ç°
- [ ] **JSONæ ¼å¼**: 100%ç¬¦åˆå¢å¼ºåçš„æ ¼å¼è§„èŒƒ
- [ ] **æ€§èƒ½ç›‘æ§**: èƒ½å¤Ÿå‡†ç¡®æµ‹é‡å’ŒæŠ¥å‘Šæ€§èƒ½æŒ‡æ ‡  
- [ ] **è°ƒè¯•å·¥å…·**: æä¾›å®Œæ•´çš„å¯è§†åŒ–å’Œåˆ†æåŠŸèƒ½
- [ ] **å‘åå…¼å®¹**: ä¸å½±å“ç°æœ‰åŠŸèƒ½çš„æ­£å¸¸è¿è¡Œ
- [ ] **æµ‹è¯•è¦†ç›–**: æ‰€æœ‰æ–°åŠŸèƒ½éƒ½æœ‰å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹

### **æ–½å·¥æ³¨æ„äº‹é¡¹** âš ï¸

1. **ä»£ç è´¨é‡**: æ‰€æœ‰æ–°å¢ä»£ç å¿…é¡»åŒ…å«å®Œæ•´çš„ä¸­æ–‡æ³¨é‡Š
2. **é”™è¯¯å¤„ç†**: æ·»åŠ é€‚å½“çš„å¼‚å¸¸å¤„ç†å’Œå›é€€æœºåˆ¶  
3. **æ€§èƒ½ä¼˜åŒ–**: ç¡®ä¿æ–°åŠŸèƒ½ä¸ä¼šæ˜¾è‘—å½±å“å¤„ç†é€Ÿåº¦
4. **å‘åå…¼å®¹**: ä¿æŒä¸ç°æœ‰æ¥å£å’Œæ•°æ®æ ¼å¼çš„å…¼å®¹æ€§
5. **æµ‹è¯•éªŒè¯**: æ¯ä¸ªé˜¶æ®µå®Œæˆåéƒ½è¦è¿›è¡Œå……åˆ†çš„åŠŸèƒ½æµ‹è¯•

---

**æ–‡æ¡£ç»“æŸ**  
*æœ€åæ›´æ–°: 2025-01-07*