services:
  # --- Message Broker ---
  # redis:
  #   image: "redis"
  #   container_name: redis
  #   restart: always
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - wionch

  # --- API Gateway (The Brain) ---
  api_gateway:
    container_name: api_gateway
    build:
      context: .
      dockerfile: ./services/api_gateway/Dockerfile
    ports:
      - "8788:80" # Map host port 8788 to container port 80
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./tests:/app/tests
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
    networks:
      - wionch

  # --- AI Worker Services ---

  ffmpeg_service:
    container_name: ffmpeg_service
    build:
      context: .
      dockerfile: ./services/workers/ffmpeg_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "ffmpeg_queue"]
    environment:
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - wionch

  paddleocr_service:
    container_name: paddleocr_service
    build:
      context: .
      dockerfile: ./services/workers/paddleocr_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
      - paddle_models_volume:/root/.paddlex/official_models
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["/opt/venv/bin/celery", "-A", "app.tasks.celery_app", "worker", "-l", "info", "-Q", "paddleocr_queue"]
    environment:
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - wionch


  # --- Faster Whisper Service (WhisperX拆分) ---
  faster_whisper_service:
    container_name: faster_whisper_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/faster_whisper_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
      - huggingface_cache_volume:/app/.cache
      - transformers_cache_volume:/root/.cache
    restart: on-failure
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "services.workers.faster_whisper_service.app.celery_app", "worker", "-l", "info", "-Q", "faster_whisper_queue"]
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/app/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=0
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ZHIPU_API_KEY=${ZHIPU_API_KEY}
      - VOLCENGINE_API_KEY=${VOLCENGINE_API_KEY}
      - OPENAI_COMPATIBLE_API_KEY=${OPENAI_COMPATIBLE_API_KEY}
      - TRANSFORMERS_CACHE=/root/.cache/transformers
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - wionch

  # --- Pyannote Audio Service (WhisperX拆分) ---
  pyannote_audio_service:
    container_name: pyannote_audio_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/pyannote_audio_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
      - huggingface_cache_volume:/app/.cache
      - transformers_cache_volume:/root/.cache
    restart: unless-stopped
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "services.workers.pyannote_audio_service.app.celery_app", "worker", "-l", "info", "-Q", "pyannote_audio_queue"]
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/app/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=0
      - TRANSFORMERS_CACHE=/root/.cache/transformers
      - PYANNOTEAI_API_KEY=${PYANNOTEAI_API_KEY}
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - PYANNOTE_SKIP_DEPENDENCY_CHECK=0
      - TORCH_CUDA_ARCH_LIST="8.6"
      - CUDA_LAUNCH_BLOCKING=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - wionch

  # --- Audio Separator Service ---
  audio_separator_service:
    container_name: audio_separator_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/audio_separator_service/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - audio_separator_models_volume:/models
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
      - huggingface_cache_volume:/app/.cache
      - transformers_cache_volume:/root/.cache
    restart: on-failure
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "services.workers.audio_separator_service.app.celery_app", "worker", "-l", "info", "-Q", "audio_separator_queue"]
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - wionch

  # --- WService (字幕AI优化服务) ---
  wservice:
    container_name: wservice
    build:
      context: .
      dockerfile: ./services/workers/wservice/Dockerfile
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "services.workers.wservice.app.celery_app", "worker", "-l", "info", "-Q", "wservice_queue"]
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      # AI服务相关环境变量
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - ZHIPU_API_KEY=${ZHIPU_API_KEY}
      - VOLCENGINE_API_KEY=${VOLCENGINE_API_KEY}
      - OPENAI_COMPATIBLE_API_KEY=${OPENAI_COMPATIBLE_API_KEY}
    networks:
      - wionch

  # --- IndexTTS Service (IndexTTS2 文本转语音服务) ---
  indextts_service:
    container_name: indextts_service
    runtime: nvidia
    build:
      context: .
      dockerfile: ./services/workers/indextts_service/Dockerfile
    ports:
      - "7860:7860"  # IndexTTS2 WebUI端口映射
    volumes:
      - ./services:/app/services
      - ./videos:/app/videos
      - ./locks:/app/locks
      - ./tmp:/app/tmp
      - ./share:/share
      - ./config.yml:/app/config.yml
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ~/.ssh:/root/.ssh
      - ~/.gemini:/root/.gemini
      - indextts_models_volume:/models/indextts
      - huggingface_cache_volume:/app/.cache/huggingface
      - transformers_cache_volume:/app/.cache/transformers
      - indextts_cache_volume:/app/.cache/indextts
      - torch_cache_volume:/app/.cache/torch
    restart: always
    user: "0:0"
    entrypoint: ""
    command: ["celery", "-A", "services.workers.indextts_service.app.celery_app", "worker", "-l", "info", "-Q", "indextts_queue", "--concurrency=1"]
    environment:
      # 基础环境变量
      - PYTHONPATH=/app
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}

      # GPU 配置
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0

      # IndexTTS2 专用环境变量
      - INDEX_TTS_MODEL_DIR=/models/indextts
      - INDEX_TTS_CHECKPOINTS_DIR=/models/indextts/checkpoints

      # 缓存配置
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/transformers
      - TORCH_HOME=/app/.cache/torch

      # Hugging Face 配置
      - HF_HUB_ENABLE_HF_TRANSFER=0

      # IndexTTS2 性能优化配置
      - INDEX_TTS_USE_FP16=true
      - INDEX_TTS_USE_DEEPSPEED=false
      - INDEX_TTS_USE_CUDA_KERNEL=false
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          memory: 4G
        limits:
          memory: 8G
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; print('GPU Available:' if torch.cuda.is_available() else 'GPU Not Available')"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s
    networks:
      - wionch


# 定义所有卷
volumes:
  npm_cache: {}
  paddle_models_volume: {}
  huggingface_cache_volume: {}
  transformers_cache_volume: {}
  audio_separator_models_volume: {}
  indextts_models_volume: {}
  indextts_cache_volume: {}
  torch_cache_volume: {}
  serena_cache_volume: {}

networks:
  wionch:
    external: true