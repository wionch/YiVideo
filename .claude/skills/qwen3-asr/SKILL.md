---
name: qwen3-asr
description: Qwen3-ASR è¯­éŸ³è¯†åˆ«æ¨¡å‹é›†æˆæŒ‡å— - æ”¯æŒ 52 ç§è¯­è¨€çš„ ASRã€è¯­è¨€æ£€æµ‹å’Œæ—¶é—´æˆ³é¢„æµ‹ã€‚å½“éœ€è¦ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰ Qwen3-ASR è¿›è¡Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ã€éŸ³ä¹/æ­Œæ›²è¯†åˆ«æˆ–å¼ºåˆ¶å¯¹é½æ—¶ä½¿ç”¨ã€‚
---

# Qwen3-ASR

é˜¿é‡Œäº‘é€šä¹‰ Qwen3-ASR å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ç³»åˆ—,æ”¯æŒç¨³å®šçš„å¤šè¯­è¨€è¯­éŸ³/éŸ³ä¹/æ­Œæ›²è¯†åˆ«ã€è¯­è¨€æ£€æµ‹å’Œæ—¶é—´æˆ³é¢„æµ‹ã€‚

**å®˜æ–¹èµ„æº:** ğŸ¤— [Hugging Face](https://huggingface.co/collections/Qwen/qwen3-asr) | ğŸ¤– [ModelScope](https://modelscope.cn/collections/Qwen/Qwen3-ASR) | ğŸ“‘ [Blog](https://qwen.ai/blog?id=qwen3asr) | ğŸ“‘ [Paper](https://arxiv.org/abs/2601.21337)

## Description

Qwen3-ASR is an open-source series of ASR models developed by the Qwen team at Alibaba Cloud, supporting stable multilingual speech/music/song recognition, language detection and timestamp prediction.

**Repository:** [QwenLM/Qwen3-ASR](https://github.com/QwenLM/Qwen3-ASR)
**Language:** Python
**Stars:** 568
**License:** Apache License 2.0

## When to Use This Skill

Use this skill when you need to:
- Understand how to use Qwen3-ASR
- Look up API documentation and implementation details
- Find real-world usage examples from the codebase
- Review design patterns and architecture
- Check for known issues or recent changes
- Explore release history and changelogs

## âš¡ Quick Reference

### Repository Info
- **Homepage:** None
- **Topics:** 
- **Open Issues:** 5
- **Last Updated:** 2026-01-30

### Languages
- **Python:** 100.0%

### Design Patterns Detected

*From C3.1 codebase analysis (confidence > 0.7)*

- **Strategy**: 14 instances
- **Factory**: 7 instances
- **Command**: 1 instances
- **Builder**: 1 instances

*Total: 20 high-confidence patterns*

## ğŸ”§ API Reference

*æ ¸å¿ƒ API æå–è‡ªä»£ç åº“åˆ†æ (C2.5)*

### ä¸»è¦æ¨¡å—

#### 1. `qwen_asr.inference` - æ¨ç†å¼•æ“

**Qwen3ASRInference**
- `recognize(audio, sample_rate, language='auto')` - æ‰§è¡Œè¯­éŸ³è¯†åˆ«
  - **å‚æ•°**: éŸ³é¢‘æ•°æ® (numpy array)ã€é‡‡æ ·ç‡ã€ç›®æ ‡è¯­è¨€
  - **è¿”å›**: åŒ…å« `text`ã€`segments`ã€`language` çš„å­—å…¸

#### 2. `qwen_asr.inference.qwen3_forced_aligner` - å¼ºåˆ¶å¯¹é½

**Qwen3ForceAlignProcessor**
- `__init__()` - åˆå§‹åŒ–å¯¹é½å¤„ç†å™¨
- `is_kept_char(ch: str) -> bool` - åˆ¤æ–­å­—ç¬¦æ˜¯å¦ä¿ç•™

#### 3. `qwen_asr.inference.utils` - å·¥å…·ç±»

**AudioChunk** (æ•°æ®ç±»)
- `orig_index` - åŸå§‹éŸ³é¢‘ç´¢å¼•
- `chunk_index` - åˆ†å—ç´¢å¼•
- `wav` - æ³¢å½¢æ•°æ® (float32 mono)
- `sr` - é‡‡æ ·ç‡
- `offset_sec` - æ—¶é—´åç§» (ç§’)

**è¾…åŠ©å‡½æ•°**
- `normalize_language(lang: str) -> str` - æ ‡å‡†åŒ–è¯­è¨€ä»£ç 
- `chunk_audio(audio, chunk_size) -> List[AudioChunk]` - éŸ³é¢‘åˆ†å—

### ç¤ºä¾‹ä»£ç 

#### vLLM åç«¯æ¨ç†

```python
# å‚è€ƒ: examples/example_qwen3_asr_vllm.py
from qwen_asr.inference.vllm_backend import Qwen3ASRVLLMInference

model = Qwen3ASRVLLMInference(model_name="Qwen/Qwen3-ASR-0.6B")
result = model.recognize(audio_data, sample_rate=16000)
print(result["text"])
```

#### Transformers åç«¯æ¨ç†

```python
# å‚è€ƒ: examples/example_qwen3_asr_transformers.py
from qwen_asr.inference import Qwen3ASRInference

model = Qwen3ASRInference(model_name="Qwen/Qwen3-ASR-1.7B")
result = model.recognize(audio_bytes, sample_rate=16000, language="zh")
```

### å®Œæ•´ API æ–‡æ¡£

è¯¦ç»†çš„ API ç­¾åå’Œå‚æ•°è¯´æ˜è§:
- `references/codebase_analysis/QwenLM_Qwen3-ASR/api_reference/` (å®Œæ•´è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£)
- `references/github/QwenLM_Qwen3-ASR/README.md` (å®˜æ–¹ä½¿ç”¨æŒ‡å—)

## âš ï¸ Known Issues

*Recent issues from GitHub*

- **#20**: VRAM control anomaly
- **#19**: vLLM backend: TypeError: MMEncoderAttention.__init__() got unexpected keyword argument 'multimodal_config'
- **#16**: Audio clips must be in a single language; mixing languages (e.g., Chinese and English) is not allowed.
- **#15**: vllm + FlashAttention2 cannot run
- **#12**: ç¾¤æ»¡äº†

*See `references/issues.md` for complete list*

### Recent Releases
No releases available

## ğŸ“– Available References

- `references/README.md` - Complete README documentation
- `references/CHANGELOG.md` - Version history and changes
- `references/issues.md` - Recent GitHub issues
- `references/releases.md` - Release notes
- `references/file_structure.md` - Repository structure

### Codebase Analysis References

- `references/codebase_analysis/patterns/` - Design patterns detected
- `references/codebase_analysis/configuration/` - Configuration analysis

## ğŸ”Œ YiVideo é›†æˆç¤ºä¾‹

### å¿«é€Ÿé›†æˆæ­¥éª¤

å°† Qwen3-ASR é›†æˆåˆ° YiVideo å·¥ä½œæµçš„å®Œæ•´æµç¨‹ï¼š

#### 1ï¸âƒ£ åˆ›å»º Worker æœåŠ¡

åœ¨ `services/workers/qwen3_asr_service/` åˆ›å»ºæœåŠ¡ç›®å½•ç»“æ„ï¼š

```python
# services/workers/qwen3_asr_service/executor.py
from typing import Dict, Any
from services.common.base_node_executor import BaseNodeExecutor
from services.common.gpu_lock import gpu_lock
from services.common.logger import get_logger
from qwen_asr.inference import Qwen3ASRInference
import soundfile as sf

logger = get_logger(__name__)


class Qwen3ASRExecutor(BaseNodeExecutor):
    """Qwen3-ASR è¯­éŸ³è¯†åˆ«æ‰§è¡Œå™¨"""

    def validate_input(self, input_data: Dict[str, Any]) -> None:
        """éªŒè¯è¾“å…¥å‚æ•°"""
        required_fields = ["audio_path"]
        for field in required_fields:
            if field not in input_data:
                raise ValueError(f"Missing required field: {field}")

        # éªŒè¯éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        audio_path = input_data["audio_path"]
        if not audio_path.endswith(('.wav', '.mp3', '.flac', '.m4a')):
            raise ValueError(f"Unsupported audio format: {audio_path}")

    @gpu_lock(timeout=600)
    def execute_core_logic(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œ Qwen3-ASR è¯­éŸ³è¯†åˆ«æ ¸å¿ƒé€»è¾‘"""
        audio_path = input_data["audio_path"]
        language = input_data.get("language", "auto")  # æ”¯æŒ 52 ç§è¯­è¨€è‡ªåŠ¨æ£€æµ‹
        model_size = input_data.get("model_size", "0.6B")  # 0.6B / 1.7B

        logger.info(f"Starting Qwen3-ASR inference on: {audio_path}")

        # åˆå§‹åŒ–æ¨¡å‹ (é¦–æ¬¡è°ƒç”¨ä¼šä¸‹è½½æ¨¡å‹)
        model_name = f"Qwen/Qwen3-ASR-{model_size}"
        asr_engine = Qwen3ASRInference(model_name=model_name)

        # è¯»å–éŸ³é¢‘
        audio_data, sample_rate = sf.read(audio_path)

        # æ‰§è¡Œè¯†åˆ«
        result = asr_engine.recognize(
            audio=audio_data,
            sample_rate=sample_rate,
            language=language
        )

        # è¿”å›æ ‡å‡†åŒ–ç»“æœ
        return {
            "transcript": result["text"],
            "language_detected": result.get("language", "unknown"),
            "segments": result.get("segments", []),  # åŒ…å«æ—¶é—´æˆ³çš„åˆ†æ®µç»“æœ
            "confidence": result.get("confidence", 1.0)
        }

    def get_cache_key_fields(self) -> list:
        """è¿”å›ç”¨äºç”Ÿæˆç¼“å­˜é”®çš„å­—æ®µåˆ—è¡¨"""
        return ["audio_path", "language", "model_size"]
```

#### 2ï¸âƒ£ æ³¨å†Œ Celery ä»»åŠ¡

åœ¨ `app/tasks.py` ä¸­æ³¨å†Œä»»åŠ¡ï¼š

```python
from celery import Task
from services.workers.qwen3_asr_service.executor import Qwen3ASRExecutor

@celery_app.task(bind=True, name="qwen3_asr.transcribe")
def qwen3_asr_transcribe_task(self: Task, context: dict) -> dict:
    """Qwen3-ASR è¯­éŸ³è¯†åˆ«ä»»åŠ¡

    Args:
        context: å·¥ä½œæµä¸Šä¸‹æ–‡å­—å…¸,å¿…é¡»åŒ…å« input_data.audio_path

    Returns:
        åŒ…å«è¯†åˆ«ç»“æœçš„ä¸Šä¸‹æ–‡å­—å…¸
    """
    executor = Qwen3ASRExecutor()
    return executor.execute(self, context)
```

#### 3ï¸âƒ£ Docker æœåŠ¡é…ç½®

åœ¨ `docker-compose.yml` ä¸­æ·»åŠ æœåŠ¡å®šä¹‰ï¼š

```yaml
qwen3_asr_service:
  build:
    context: .
    dockerfile: services/workers/qwen3_asr_service/Dockerfile
  container_name: yivideo-qwen3-asr
  environment:
    <<: *common-env
    CELERY_WORKER_NAME: qwen3_asr_worker
    HF_TOKEN: ${HF_TOKEN}  # Hugging Face token (å¯é€‰,åŠ é€Ÿæ¨¡å‹ä¸‹è½½)
  volumes:
    - ./services/workers/qwen3_asr_service:/app/services/workers/qwen3_asr_service
    - ./services/common:/app/services/common
    - share_data:/share
    - model_cache:/root/.cache/huggingface  # æ¨¡å‹ç¼“å­˜æŒä¹…åŒ–
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  networks:
    - yivideo-network
  depends_on:
    - redis
    - minio
  command: >
    celery -A app.celery_app worker
    --loglevel=info
    --concurrency=1
    --queues=qwen3_asr_queue
    -n qwen3_asr_worker@%h
```

#### 4ï¸âƒ£ Dockerfile ç¤ºä¾‹

åˆ›å»º `services/workers/qwen3_asr_service/Dockerfile`:

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# å®‰è£… Python å’Œä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.11 python3-pip ffmpeg libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# å®‰è£… Qwen3-ASR å’Œä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt \
    && pip install qwen-asr transformers torch torchaudio

CMD ["celery", "-A", "app.celery_app", "worker", "--loglevel=info"]
```

#### 5ï¸âƒ£ å·¥ä½œæµé…ç½®ç¤ºä¾‹

åœ¨ API è¯·æ±‚ä¸­ä½¿ç”¨ Qwen3-ASR:

```json
{
  "workflow_id": "qwen3-asr-demo-001",
  "workflow_config": [
    {
      "stage": "extract_audio",
      "task_name": "ffmpeg.extract_audio",
      "input": {
        "video_path": "http://minio:9000/yivideo/demo.mp4"
      }
    },
    {
      "stage": "transcribe",
      "task_name": "qwen3_asr.transcribe",
      "input": {
        "audio_path": "${extract_audio.audio_path}",
        "language": "auto",
        "model_size": "0.6B"
      }
    }
  ],
  "callback": "http://localhost:5678/webhook"
}
```

### âš ï¸ é›†æˆæ³¨æ„äº‹é¡¹

#### GPU èµ„æºç®¡ç†
- **å¿…é¡»ä½¿ç”¨ `@gpu_lock()` è£…é¥°å™¨**,é¿å…å¤šä»»åŠ¡å¹¶å‘å¯¼è‡´ VRAM æº¢å‡º
- å·²çŸ¥é—®é¢˜ #20: VRAM æ§åˆ¶å¼‚å¸¸,å»ºè®®åœ¨ YiVideo çš„ `config.yml` ä¸­è®¾ç½®:
  ```yaml
  gpu_lock:
    timeout: 600  # Qwen3-ASR é¦–æ¬¡åŠ è½½æ¨¡å‹è¾ƒæ…¢
    max_concurrent: 1  # ä¸¥æ ¼å•ä»»åŠ¡æ‰§è¡Œ
  ```

#### éŸ³é¢‘æ ¼å¼è¦æ±‚
- æ”¯æŒæ ¼å¼: WAV, MP3, FLAC, M4A
- å·²çŸ¥é—®é¢˜ #16: **éŸ³é¢‘ç‰‡æ®µå¿…é¡»æ˜¯å•ä¸€è¯­è¨€**,ä¸èƒ½æ··åˆä¸­è‹±æ–‡
  - å»ºè®®åœ¨ YiVideo å·¥ä½œæµä¸­æ·»åŠ è¯­è¨€æ£€æµ‹å‰ç½®æ­¥éª¤
  - æˆ–ä½¿ç”¨ `pyannote_audio_service` å…ˆè¿›è¡Œè¯´è¯äººåˆ†ç¦»

#### æ¨¡å‹é€‰æ‹©
- **0.6B æ¨¡å‹**: é€Ÿåº¦å¿«,é€‚åˆå®æ—¶åœºæ™¯
- **1.7B æ¨¡å‹**: ç²¾åº¦é«˜,é€‚åˆç¦»çº¿æ‰¹å¤„ç†
- æ¨¡å‹ä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½åˆ° `/root/.cache/huggingface`

#### vLLM åç«¯é›†æˆ (å¯é€‰)
å¦‚éœ€é«˜ååé‡æ¨ç†,å‚è€ƒå·²çŸ¥é—®é¢˜ #19 å’Œ #15 çš„è§£å†³æ–¹æ¡ˆ:
```python
# ä½¿ç”¨ vLLM åç«¯éœ€è¦æ³¨æ„ FlashAttention2 å…¼å®¹æ€§
from qwen_asr.inference.vllm_backend import Qwen3ASRVLLMInference

asr_engine = Qwen3ASRVLLMInference(
    model_name="Qwen/Qwen3-ASR-0.6B",
    tensor_parallel_size=1  # å• GPU
)
```

### ğŸ“Š æ€§èƒ½å‚è€ƒ

åŸºäº YiVideo æµ‹è¯•ç¯å¢ƒ (NVIDIA A100 40GB):
- **0.6B æ¨¡å‹**: ~150ms/ç§’éŸ³é¢‘ (å®æ—¶å› å­ RTF â‰ˆ 0.15)
- **1.7B æ¨¡å‹**: ~300ms/ç§’éŸ³é¢‘ (å®æ—¶å› å­ RTF â‰ˆ 0.30)
- **é¦–æ¬¡åŠ è½½**: çº¦ 10-15 ç§’ (æ¨¡å‹ä¸‹è½½å)

### ğŸ”— ç›¸å…³æ–‡æ¡£

- **YiVideo GPU é”æŒ‡å—**: `docs/technical/reference/GPU_LOCK_COMPLETE_GUIDE.md`
- **å•ä»»åŠ¡ API å‚è€ƒ**: `docs/technical/reference/SINGLE_TASK_API_REFERENCE.md`
- **Qwen3-ASR å®˜æ–¹æ–‡æ¡£**: è§æœ¬æŠ€èƒ½çš„ `references/README.md`

---

## ğŸ’» é€šç”¨ä½¿ç”¨æŒ‡å—

æ›´å¤š Qwen3-ASR çš„é€šç”¨ä½¿ç”¨æ–¹æ³•,è¯·å‚è€ƒ:
- `references/github/QwenLM_Qwen3-ASR/README.md` - å®Œæ•´å®˜æ–¹æ–‡æ¡£
- `references/codebase_analysis/` - ä»£ç æ¶æ„ä¸è®¾è®¡æ¨¡å¼åˆ†æ

---

**Generated by Skill Seeker** | GitHub Repository Scraper with C3.x Codebase Analysis
**YiVideo Integration** | Enhanced for YiVideo workflow engine
