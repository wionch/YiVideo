"""
WService å­—å¹•åˆ†å¥æ‰§è¡Œå™¨ã€‚

åŸºäº PySBD + LangChain å®ç°è¯­ä¹‰åˆ†å¥å’Œå­—ç¬¦é™åˆ¶ï¼Œæ”¯æŒè¯çº§æ—¶é—´æˆ³æ˜ å°„ã€‚
"""

import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict

from services.common.base_node_executor import BaseNodeExecutor
from services.common.logger import get_logger
from services.common.parameter_resolver import get_param_with_fallback
from services.common.path_builder import build_node_output_path, ensure_directory
from services.common.file_service import get_file_service

logger = get_logger(__name__)


@dataclass
class SubtitleSegment:
    """å­—å¹•ç‰‡æ®µæ•°æ®æ¨¡å‹"""
    id: int  # ç‰‡æ®µå”¯ä¸€æ ‡è¯†ï¼ˆä» 1 å¼€å§‹ï¼‰
    text: str
    start: Optional[float] = None
    end: Optional[float] = None
    duration: Optional[float] = None
    word_count: Optional[int] = None
    char_count: Optional[int] = None
    words: Optional[List[Dict[str, Any]]] = None  # è¯çº§æ—¶é—´æˆ³åˆ—è¡¨


class ASRDataLoader:
    """ASR æ•°æ®å¤„ç†è¾…åŠ©ç±»"""

    @staticmethod
    def build_char_to_timestamp_map(text: str, timestamps: List[Dict]) -> Dict[int, Dict]:
        """
        æ„å»ºå­—ç¬¦ä½ç½®åˆ°æ—¶é—´æˆ³çš„æ˜ å°„ã€‚

        Args:
            text: å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…å«æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
            timestamps: è¯çº§æ—¶é—´æˆ³åˆ—è¡¨ [{text, start, end}, ...]

        Returns:
            {char_index: {word, start, end, word_index}, ...}
        """
        char_map = {}
        text_idx = 0

        for word_idx, ts in enumerate(timestamps):
            word = ts["text"]
            start = ts["start"]
            end = ts["end"]

            # è·³è¿‡å‰å¯¼ç©ºæ ¼å’Œæ ‡ç‚¹ï¼ˆä½†ä¸è·³è¿‡æ’‡å·ï¼‰
            while text_idx < len(text) and text[text_idx] in ' \t\n.,!?;:"':
                text_idx += 1

            if text_idx >= len(text):
                break

            word_len = len(word)

            # å°è¯•ç›´æ¥åŒ¹é…
            if text_idx + word_len <= len(text) and \
               text[text_idx:text_idx+word_len].lower() == word.lower():
                for i in range(text_idx, text_idx + word_len):
                    char_map[i] = {
                        "word": word,
                        "start": start,
                        "end": end,
                        "word_index": word_idx
                    }
                text_idx += word_len
            else:
                # åŒ¹é…å¤±è´¥ï¼Œå°è¯•åœ¨é™„è¿‘èŒƒå›´å†…æŸ¥æ‰¾
                search_end = min(text_idx + 100, len(text))
                search_text = text[text_idx:search_end]
                found_offset = search_text.lower().find(word.lower())

                if found_offset != -1:
                    found_pos = text_idx + found_offset
                    for i in range(found_pos, found_pos + word_len):
                        char_map[i] = {
                            "word": word,
                            "start": start,
                            "end": end,
                            "word_index": word_idx
                        }
                    text_idx = found_pos + word_len

        return char_map


class PySBDLangChainSubtitleSegmenter:
    """PySBD + LangChain å­—å¹•åˆ†å¥å™¨"""

    def __init__(self, max_chars: int = 42, language: str = "en"):
        """
        åˆå§‹åŒ–åˆ†å¥å™¨ã€‚

        Args:
            max_chars: å•è¡Œå­—å¹•æœ€å¤§å­—ç¬¦æ•°
            language: è¯­è¨€ä»£ç ï¼ˆæ”¯æŒ PySBD çš„ 23 ç§è¯­è¨€ï¼‰
        """
        try:
            import pysbd
            self.pysbd_seg = pysbd.Segmenter(language=language, clean=False)
        except ImportError as e:
            raise ImportError(
                "è¯·å…ˆå®‰è£… pysbd: pip install pysbd>=0.3.4"
            ) from e

        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            # å­—å¹•ä¸“ç”¨é…ç½®ï¼šä¼˜å…ˆåœ¨æ ‡ç‚¹å¤„æ–­å¼€
            self.langchain_splitter = RecursiveCharacterTextSplitter(
                chunk_size=max_chars,
                chunk_overlap=0,
                keep_separator="end",  # ğŸ”‘ å…³é”®ï¼šåˆ†éš”ç¬¦ä¿ç•™åœ¨å‰ä¸€ä¸ªå—çš„ç»“å°¾
                separators=[
                    ", ",   # ä¼˜å…ˆçº§1ï¼šé€—å·åæ–­å¼€ï¼ˆè‡ªç„¶åœé¡¿ï¼‰
                    "; ",   # ä¼˜å…ˆçº§2ï¼šåˆ†å·åæ–­å¼€
                    ". ",   # ä¼˜å…ˆçº§3ï¼šå¥å·åæ–­å¼€
                    "! ",   # ä¼˜å…ˆçº§4ï¼šæ„Ÿå¹å·åæ–­å¼€
                    "? ",   # ä¼˜å…ˆçº§5ï¼šé—®å·åæ–­å¼€
                    " ",    # ä¼˜å…ˆçº§6ï¼šç©ºæ ¼
                    ""      # ä¼˜å…ˆçº§7ï¼šå¼ºåˆ¶æŒ‰å­—ç¬¦åˆ‡ï¼ˆæœ€åæ‰‹æ®µï¼‰
                ]
            )
        except ImportError as e:
            raise ImportError(
                "è¯·å…ˆå®‰è£… langchain-text-splitters: pip install langchain-text-splitters>=0.3.2"
            ) from e

        self.max_chars = max_chars
        self.language = language
        self.pysbd_sentences = []  # ä¿å­˜ PySBD åŸå§‹åˆ†å¥ç»“æœ

    def segment(
        self,
        text: str,
        words: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†å¥ã€‚

        Args:
            text: å¾…åˆ†å¥çš„æ–‡æœ¬
            words: è¯çº§æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            {
                segments: List[SubtitleSegment],
                total_segments: int,
                statistics: {...}
            }
        """
        start_time = time.time()

        # é˜¶æ®µ 1: PySBD è¯­ä¹‰åˆ†å¥
        pysbd_sentences = self.pysbd_seg.segment(text)
        self.pysbd_sentences = pysbd_sentences  # ä¿å­˜åŸå§‹åˆ†å¥ç»“æœ
        logger.debug(f"PySBD åˆ†å¥å®Œæˆï¼Œå…± {len(pysbd_sentences)} ä¸ªå¥å­")

        # é˜¶æ®µ 2: å¤„ç†å­—ç¬¦é™åˆ¶å’Œæ—¶é—´æˆ³æ˜ å°„
        if words:
            # æ„å»ºå­—ç¬¦æ˜ å°„
            char_map = ASRDataLoader.build_char_to_timestamp_map(text, words)
            logger.debug(f"å­—ç¬¦æ˜ å°„å®Œæˆï¼Œè¦†ç›– {len(char_map)} ä¸ªå­—ç¬¦")

            # å¸¦æ—¶é—´æˆ³å¤„ç†
            segments = self._process_with_langchain(
                pysbd_sentences, text, char_map, words
            )
        else:
            # ä»…æ–‡æœ¬å¤„ç†ï¼ˆæ— æ—¶é—´æˆ³ï¼‰
            segments = self._process_text_only(pysbd_sentences)

        execution_time = time.time() - start_time

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        stats = self._calculate_statistics(segments, pysbd_sentences, has_timestamps=bool(words))

        return {
            "segments": segments,
            "total_segments": len(segments),
            "execution_time": execution_time,
            "statistics": stats
        }

    def _process_text_only(self, sentences: List[str]) -> List[SubtitleSegment]:
        """
        å¤„ç†ä»…æ–‡æœ¬åˆ†å¥ï¼ˆæ— æ—¶é—´æˆ³ï¼‰ã€‚

        Args:
            sentences: PySBD åˆ†å¥ç»“æœ

        Returns:
            List[SubtitleSegment]
        """
        segments = []
        segment_id = 1

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å­—ç¬¦é™åˆ¶
            if len(sentence) <= self.max_chars:
                segments.append(SubtitleSegment(
                    id=segment_id,
                    text=sentence,
                    char_count=len(sentence)
                ))
                segment_id += 1
            else:
                # è¶…é™ï¼Œä½¿ç”¨ LangChain åˆ†å‰²
                lines = self.langchain_splitter.split_text(sentence)
                for line_text in lines:
                    line_text = line_text.strip()
                    if line_text:
                        segments.append(SubtitleSegment(
                            id=segment_id,
                            text=line_text,
                            char_count=len(line_text)
                        ))
                        segment_id += 1

        return segments

    def _process_with_langchain(
        self,
        sentences: List[str],
        full_text: str,
        char_map: Dict[int, Dict],
        timestamps: List[Dict]
    ) -> List[SubtitleSegment]:
        """
        ä½¿ç”¨ LangChain å¤„ç†å­—ç¬¦é™åˆ¶å¹¶æ˜ å°„æ—¶é—´æˆ³ã€‚

        Args:
            sentences: PySBD åˆ†å¥ç»“æœ
            full_text: å®Œæ•´æ–‡æœ¬
            char_map: å­—ç¬¦åˆ°æ—¶é—´æˆ³çš„æ˜ å°„
            timestamps: åŸå§‹è¯çº§æ—¶é—´æˆ³åˆ—è¡¨

        Returns:
            List[SubtitleSegment]
        """
        segments = []
        char_offset = 0
        segment_id = 1

        for idx, sentence in enumerate(sentences, 1):
            sentence = sentence.strip()
            if not sentence:
                continue

            # åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å¥å­ä½ç½®
            sent_start = full_text.find(sentence, char_offset)
            if sent_start == -1:
                logger.warning(f"å¥å­ {idx} æ— æ³•åœ¨åŸæ–‡ä¸­å®šä½ï¼Œè·³è¿‡")
                continue

            sent_end = sent_start + len(sentence) - 1

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å­—ç¬¦é™åˆ¶
            if len(sentence) <= self.max_chars:
                # ä¸è¶…é™ï¼Œç›´æ¥åˆ›å»ºç‰‡æ®µ
                seg = self._create_segment(
                    segment_id, sentence, sent_start, sent_end, char_map, timestamps
                )
                if seg:
                    segments.append(seg)
                    segment_id += 1
            else:
                # è¶…é™ï¼Œä½¿ç”¨ LangChain åˆ†å‰²
                lines = self.langchain_splitter.split_text(sentence)

                # ä¸ºæ¯è¡Œåˆ†é…æ—¶é—´æˆ³
                line_char_offset = sent_start
                for line_text in lines:
                    line_text = line_text.strip()
                    if not line_text:
                        continue

                    # åœ¨åŸå¥ä¸­æ‰¾åˆ°è¿™ä¸€è¡Œçš„ä½ç½®
                    line_start = full_text.find(line_text, line_char_offset)
                    if line_start == -1:
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ï¼‰
                        clean_line = line_text.strip('.,!?;: ')
                        line_start = full_text.find(clean_line, line_char_offset)

                    if line_start != -1:
                        line_end = line_start + len(line_text) - 1
                        seg = self._create_segment(
                            segment_id, line_text, line_start, line_end, char_map, timestamps
                        )
                        if seg:
                            segments.append(seg)
                            segment_id += 1
                        line_char_offset = line_end + 1

            char_offset = sent_end + 1

        return segments

    def _create_segment(
        self,
        segment_id: int,
        text: str,
        start_pos: int,
        end_pos: int,
        char_map: Dict[int, Dict],
        timestamps: List[Dict]
    ) -> Optional[SubtitleSegment]:
        """
        åˆ›å»ºå­—å¹•ç‰‡æ®µã€‚

        Args:
            segment_id: ç‰‡æ®µå”¯ä¸€æ ‡è¯†ï¼ˆä»1å¼€å§‹ï¼‰
            text: ç‰‡æ®µæ–‡æœ¬
            start_pos: åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
            end_pos: åœ¨åŸæ–‡ä¸­çš„ç»“æŸä½ç½®
            char_map: å­—ç¬¦æ˜ å°„è¡¨
            timestamps: åŸå§‹æ—¶é—´æˆ³åˆ—è¡¨

        Returns:
            SubtitleSegment or None
        """
        # è·å–èµ·å§‹å’Œç»“æŸæ—¶é—´æˆ³
        start_ts = char_map.get(start_pos, {}).get("start")
        end_ts = char_map.get(end_pos, {}).get("end")

        # å¦‚æœç›´æ¥æ‰¾ä¸åˆ°ï¼Œå‘å/å‘å‰æœç´¢
        if start_ts is None:
            for i in range(start_pos, min(end_pos + 1, start_pos + 100)):
                if i in char_map:
                    start_ts = char_map[i]["start"]
                    break

        if end_ts is None:
            for i in range(end_pos, max(start_pos - 1, end_pos - 100), -1):
                if i in char_map:
                    end_ts = char_map[i]["end"]
                    break

        if start_ts is None or end_ts is None:
            return None

        # æå–è¯çº§æ—¶é—´æˆ³
        word_indices = set()
        for char_pos in range(start_pos, end_pos + 1):
            if char_pos in char_map:
                word_idx = char_map[char_pos].get("word_index")
                if word_idx is not None:
                    word_indices.add(word_idx)

        word_indices = sorted(word_indices)
        segment_words = []
        for word_idx in word_indices:
            ts = timestamps[word_idx]
            segment_words.append({
                "text": ts["text"],
                "start": ts["start"],
                "end": ts["end"]
            })

        return SubtitleSegment(
            id=segment_id,
            text=text,
            start=start_ts,
            end=end_ts,
            duration=end_ts - start_ts,
            word_count=len(segment_words),
            char_count=len(text),
            words=segment_words
        )

    def _calculate_statistics(
        self,
        segments: List[SubtitleSegment],
        pysbd_sentences: List[str],
        has_timestamps: bool
    ) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not segments:
            return {}

        # æ£€æŸ¥å­—ç¬¦é™åˆ¶åˆè§„æ€§
        over_limit = [s for s in segments if s.char_count > self.max_chars]
        compliance_rate = (len(segments) - len(over_limit)) / len(segments) * 100

        stats = {
            "pysbd_sentences": len(pysbd_sentences),
            "final_segments": len(segments),
            "expansion_ratio": round(len(segments) / len(pysbd_sentences), 2),
            "character_limit": {
                "max_allowed": self.max_chars,
                "compliance_rate": round(compliance_rate, 2),
                "over_limit_count": len(over_limit)
            },
            "has_timestamps": has_timestamps
        }

        return stats

    def get_pysbd_segments(self, text: str, words: Optional[List[Dict]] = None) -> List[SubtitleSegment]:
        """
        è·å– PySBD åŸå§‹åˆ†å¥ç»“æœï¼ˆä¸ç»è¿‡ LangChain å­—ç¬¦é™åˆ¶å¤„ç†ï¼‰ã€‚

        Args:
            text: å¾…åˆ†å¥çš„æ–‡æœ¬
            words: è¯çº§æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            List[SubtitleSegment]: PySBD åŸå§‹åˆ†å¥åˆ—è¡¨
        """
        # æ‰§è¡Œ PySBD åˆ†å¥
        pysbd_sentences = self.pysbd_seg.segment(text)
        segments = []

        if words:
            # æ„å»ºå­—ç¬¦æ˜ å°„
            char_map = ASRDataLoader.build_char_to_timestamp_map(text, words)

            # ä¸ºæ¯ä¸ª PySBD å¥å­åˆ›å»º segmentï¼ˆä¸åˆ†å‰²ï¼‰
            current_char_index = 0
            for seg_id, sentence in enumerate(pysbd_sentences, start=1):
                sentence = sentence.strip()
                if not sentence:
                    continue

                # è®¡ç®—å¥å­åœ¨åŸæ–‡ä¸­çš„å­—ç¬¦èŒƒå›´
                sentence_start_idx = text.find(sentence, current_char_index)
                if sentence_start_idx == -1:
                    # æ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œè·³è¿‡
                    logger.warning(f"å¥å­æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…: {sentence[:30]}...")
                    continue

                sentence_end_idx = sentence_start_idx + len(sentence)

                # æå–è¯¥å¥å­èŒƒå›´å†…çš„è¯çº§æ—¶é—´æˆ³
                segment_words = []
                segment_start = None
                segment_end = None

                for char_idx in range(sentence_start_idx, sentence_end_idx):
                    if char_idx in char_map:
                        word_data = char_map[char_idx]
                        # é¿å…é‡å¤æ·»åŠ åŒä¸€ä¸ªè¯
                        if not segment_words or segment_words[-1]["text"] != word_data["word"]:
                            segment_words.append({
                                "text": word_data["word"],
                                "start": word_data["start"],
                                "end": word_data["end"]
                            })

                            # æ›´æ–° segment æ—¶é—´èŒƒå›´
                            if segment_start is None:
                                segment_start = word_data["start"]
                            segment_end = word_data["end"]

                # è®¡ç®—æ—¶é•¿
                duration = None
                if segment_start is not None and segment_end is not None:
                    duration = round(segment_end - segment_start, 2)

                segments.append(SubtitleSegment(
                    id=seg_id,
                    text=sentence,
                    start=segment_start,
                    end=segment_end,
                    duration=duration,
                    word_count=len(segment_words),
                    char_count=len(sentence),
                    words=segment_words if segment_words else None
                ))

                current_char_index = sentence_end_idx

        else:
            # æ— æ—¶é—´æˆ³ï¼Œä»…ä¿å­˜æ–‡æœ¬
            for seg_id, sentence in enumerate(pysbd_sentences, start=1):
                sentence = sentence.strip()
                if sentence:
                    segments.append(SubtitleSegment(
                        id=seg_id,
                        text=sentence,
                        char_count=len(sentence)
                    ))

        return segments


class WServiceSubtitleSegmentationExecutor(BaseNodeExecutor):
    """
    WService å­—å¹•åˆ†å¥æ‰§è¡Œå™¨ã€‚

    ä½¿ç”¨ PySBD + LangChain å®ç°è¯­ä¹‰åˆ†å¥å’Œå­—ç¬¦é™åˆ¶ã€‚

    è¾“å…¥å‚æ•°:
        - subtitle_text (str, å¯é€‰): å­—å¹•æ–‡æœ¬
        - words (list, å¯é€‰): è¯çº§æ—¶é—´æˆ³åˆ—è¡¨ [{text, start, end}, ...]
        - subtitle_file (str, å¯é€‰): åŒ…å«å­—å¹•æ•°æ®çš„ JSON æ–‡ä»¶è·¯å¾„
        - max_chars (int, å¯é€‰): æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼ˆé»˜è®¤ 42ï¼‰
        - language (str, å¯é€‰): è¯­è¨€ä»£ç ï¼ˆé»˜è®¤ "en"ï¼‰

    è¾“å‡ºå­—æ®µ:
        - segmented_subtitle_file (str): åˆ†å¥ç»“æœæ–‡ä»¶è·¯å¾„
        - statistics (dict): ç»Ÿè®¡ä¿¡æ¯
    """

    def __init__(self, stage_name: str, context):
        super().__init__(stage_name, context)
        self.segmenter = None
        self.file_service = get_file_service()

    def validate_input(self) -> None:
        """
        éªŒè¯è¾“å…¥å‚æ•°ã€‚

        è‡³å°‘éœ€è¦æä¾›ä»¥ä¸‹ä¹‹ä¸€ï¼š
        - subtitle_text
        - subtitle_file
        """
        input_data = self.get_input_data()

        has_text = get_param_with_fallback("subtitle_text", input_data, self.context) is not None
        has_file = get_param_with_fallback("subtitle_file", input_data, self.context) is not None

        if not has_text and not has_file:
            raise ValueError(
                "ç¼ºå°‘å¿…éœ€å‚æ•°: è¯·æä¾› subtitle_text æˆ– subtitle_file"
            )

        logger.info(f"[{self.context.workflow_id}] è¾“å…¥å‚æ•°éªŒè¯é€šè¿‡")

    def execute_core_logic(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå­—å¹•åˆ†å¥æ ¸å¿ƒé€»è¾‘ã€‚

        Returns:
            åŒ…å«åˆ†å¥ç»“æœçš„å­—å…¸
        """
        workflow_id = self.context.workflow_id
        input_data = self.get_input_data()

        logger.info(f"[{workflow_id}] å¼€å§‹å­—å¹•åˆ†å¥")

        # è·å–å­—å¹•æ•°æ®
        subtitle_data = self._get_subtitle_data(input_data)

        # è·å–å‚æ•°
        max_chars = get_param_with_fallback("max_chars", input_data, self.context) or 42
        language = get_param_with_fallback("language", input_data, self.context) or "en"

        logger.info(
            f"[{workflow_id}] é…ç½®: max_chars={max_chars}, language={language}"
        )

        # åˆ›å»ºåˆ†å¥å™¨
        self.segmenter = PySBDLangChainSubtitleSegmenter(
            max_chars=max_chars,
            language=language
        )

        # æ‰§è¡Œåˆ†å¥
        result = self.segmenter.segment(
            text=subtitle_data["text"],
            words=subtitle_data.get("words")
        )

        logger.info(
            f"[{workflow_id}] åˆ†å¥å®Œæˆï¼Œç”Ÿæˆ {result['total_segments']} ä¸ªç‰‡æ®µ"
        )

        # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆç»è¿‡ LangChain å­—ç¬¦é™åˆ¶å¤„ç†ï¼‰
        output_file = self._save_segmented_result(
            result,
            max_chars,
            language
        )

        # è·å–å¹¶ä¿å­˜ PySBD åŸå§‹ç»“æœï¼ˆä¸ç»è¿‡ LangChain å¤„ç†ï¼‰
        pysbd_segments = self.segmenter.get_pysbd_segments(
            text=subtitle_data["text"],
            words=subtitle_data.get("words")
        )
        pysbd_output_file = self._save_pysbd_result(
            pysbd_segments,
            language
        )

        logger.info(
            f"[{workflow_id}] PySBD åŸå§‹ç»“æœå·²ä¿å­˜: {pysbd_output_file}"
        )

        return {
            "segmented_subtitle_file": output_file,
            "segmented_subtitle_pysbd_file": pysbd_output_file,
            "statistics": result["statistics"]
        }

    def _get_subtitle_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å­—å¹•æ•°æ®ã€‚

        ä¼˜å…ˆçº§:
        1. ç›´æ¥ä¼ å…¥çš„ subtitle_text + words
        2. subtitle_file æ–‡ä»¶è·¯å¾„

        Args:
            input_data: è¾“å…¥æ•°æ®

        Returns:
            {text: str, words: List[Dict] | None}
        """
        workflow_id = self.context.workflow_id

        # 1. å°è¯•ç›´æ¥è·å–
        subtitle_text = get_param_with_fallback(
            "subtitle_text",
            input_data,
            self.context
        )
        if subtitle_text:
            words = get_param_with_fallback("words", input_data, self.context)
            logger.info(f"[{workflow_id}] ä» subtitle_text è·å–å­—å¹•æ•°æ®")
            return {
                "text": subtitle_text,
                "words": words or None
            }

        # 2. å°è¯•ä»æ–‡ä»¶åŠ è½½
        subtitle_file = get_param_with_fallback(
            "subtitle_file",
            input_data,
            self.context
        )
        if subtitle_file:
            subtitle_file = self._normalize_path(subtitle_file)
            logger.info(f"[{workflow_id}] ä»æ–‡ä»¶åŠ è½½å­—å¹•æ•°æ®: {subtitle_file}")
            subtitle_file = self._download_if_needed(subtitle_file)
            if subtitle_file:
                return self._load_subtitle_from_file(subtitle_file, input_data)

        raise ValueError("æ— æ³•è·å–å­—å¹•æ•°æ®: subtitle_text å’Œ subtitle_file å‡ä¸ºç©º")

    def _normalize_path(self, file_path: str) -> str:
        """
        è§„èŒƒåŒ–æ–‡ä»¶è·¯å¾„ï¼ˆå¤„ç† MinIO URLã€ç›¸å¯¹è·¯å¾„ã€ç»å¯¹è·¯å¾„ï¼‰ã€‚

        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„

        Returns:
            è§„èŒƒåŒ–åçš„è·¯å¾„
        """
        # å¦‚æœæ˜¯ MinIO URLï¼Œç›´æ¥è¿”å›
        if file_path.startswith("http://") or file_path.startswith("https://"):
            return file_path

        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not file_path.startswith("/"):
            file_path = f"/app/{file_path}"

        return file_path

    def _download_if_needed(self, file_path: str) -> Optional[str]:
        """
        å¦‚æœæ˜¯ MinIO URLï¼Œä¸‹è½½åˆ°æœ¬åœ°ï¼›å¦åˆ™ç›´æ¥è¿”å›è·¯å¾„ã€‚

        Args:
            file_path: æ–‡ä»¶è·¯å¾„æˆ– MinIO URL

        Returns:
            æœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ– None
        """
        workflow_id = self.context.workflow_id

        # å¦‚æœæ˜¯ MinIO URLï¼Œä¸‹è½½åˆ°æœ¬åœ°
        if file_path.startswith("http://") or file_path.startswith("https://"):
            try:
                local_path = self.file_service.download_from_minio(file_path)
                logger.info(f"[{workflow_id}] ä» MinIO ä¸‹è½½æ–‡ä»¶: {file_path} -> {local_path}")
                return local_path
            except Exception as e:
                logger.error(f"[{workflow_id}] MinIO æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
                return None

        # æœ¬åœ°æ–‡ä»¶è·¯å¾„
        return file_path

    def _extract_json_value(self, data: Dict[str, Any], path: str) -> Any:
        """
        ä» JSON æ•°æ®ä¸­æå–åµŒå¥—è·¯å¾„çš„å€¼ï¼ˆä½¿ç”¨ JMESPathï¼‰ã€‚

        JMESPath æ˜¯ AWS å®˜æ–¹ç»´æŠ¤çš„æˆç†Ÿ JSON æŸ¥è¯¢è¯­è¨€ï¼Œæ”¯æŒï¼š
        - åµŒå¥—è®¿é—®ï¼šmain.text
        - æ•°ç»„ç´¢å¼•ï¼šresults[0], results[-1]ï¼ˆè´Ÿæ•°ç´¢å¼•ï¼‰
        - æ•°ç»„åˆ‡ç‰‡ï¼šresults[0:2]
        - æŠ•å½±ï¼šresults[*].name
        - è¿‡æ»¤ï¼šresults[?age > 20]
        - å‡½æ•°ï¼šlength(results)

        Args:
            data: JSON æ•°æ®
            path: JMESPath è¡¨è¾¾å¼

        Returns:
            æå–çš„å€¼ï¼Œå¦‚æœè·¯å¾„ä¸å­˜åœ¨è¿”å› None

        Examples:
            >>> _extract_json_value({"main": {"text": "hello"}}, "main.text")
            "hello"
            >>> _extract_json_value({"arr": [1, 2, 3]}, "arr[-1]")
            3
            >>> _extract_json_value({"users": [{"name": "Alice"}, {"name": "Bob"}]}, "users[*].name")
            ["Alice", "Bob"]

        æ–‡æ¡£ï¼šhttps://jmespath.org/
        """
        if not path:
            return None

        try:
            import jmespath
            result = jmespath.search(path, data)
            return result
        except ImportError:
            # é™çº§åˆ°ç®€å•å®ç°ï¼ˆå‘åå…¼å®¹ï¼‰
            logger.warning(
                "jmespath æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–è·¯å¾„è§£æã€‚"
                "å»ºè®®å®‰è£…ï¼špip install jmespath>=1.0.1"
            )
            return self._extract_json_value_fallback(data, path)
        except jmespath.exceptions.JMESPathError as e:
            logger.error(f"JMESPath è¡¨è¾¾å¼é”™è¯¯: {path} - {e}")
            return None

    def _extract_json_value_fallback(self, data: Dict[str, Any], path: str) -> Any:
        """
        ç®€åŒ–çš„è·¯å¾„æå–å®ç°ï¼ˆJMESPath é™çº§æ–¹æ¡ˆï¼‰ã€‚

        ä»…æ”¯æŒåŸºæœ¬åŠŸèƒ½ï¼š
        - ç‚¹åˆ†éš”ç¬¦ï¼šmain.text
        - æ•°ç»„ç´¢å¼•ï¼šarr[0] æˆ– arr.0

        ä¸æ”¯æŒï¼šè´Ÿæ•°ç´¢å¼•ã€åˆ‡ç‰‡ã€è¿‡æ»¤ã€å‡½æ•°ç­‰
        """
        if not path:
            return None

        # æ›¿æ¢æ•°ç»„ç´¢å¼•è¯­æ³• [0] ä¸º .0
        path = path.replace("[", ".").replace("]", "")

        # æŒ‰ç‚¹åˆ†éš”ç¬¦æ‹†åˆ†è·¯å¾„
        parts = path.split(".")
        current = data

        for part in parts:
            if not part:  # è·³è¿‡ç©ºå­—ç¬¦ä¸²
                continue

            try:
                # å°è¯•ä½œä¸ºæ•°ç»„ç´¢å¼•
                if part.isdigit():
                    index = int(part)
                    if isinstance(current, list) and 0 <= index < len(current):
                        current = current[index]
                    else:
                        return None
                # ä½œä¸ºå­—å…¸é”®
                elif isinstance(current, dict) and part in current:
                    current = current[part]
                else:
                    return None
            except (KeyError, IndexError, TypeError):
                return None

        return current

    def _load_subtitle_from_file(self, file_path: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä» JSON æ–‡ä»¶åŠ è½½å­—å¹•æ•°æ®ã€‚

        Args:
            file_path: JSON æ–‡ä»¶è·¯å¾„
            input_data: è¾“å…¥å‚æ•°å­—å…¸ï¼ˆç”¨äºè·å– JSON å­—æ®µè·¯å¾„é…ç½®ï¼‰

        Returns:
            {text: str, words: List[Dict] | None}
        """
        workflow_id = self.context.workflow_id

        if not Path(file_path).exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # éªŒè¯ JSON æ ¼å¼
            if not isinstance(data, dict):
                raise ValueError("JSON æ–‡ä»¶å¿…é¡»æ˜¯å¯¹è±¡æ ¼å¼")

            # ä»è¾“å…¥å‚æ•°è·å– JSON å­—æ®µè·¯å¾„é…ç½®
            text_path = get_param_with_fallback(
                "text_json_path", input_data, self.context
            ) or "text"
            words_path = get_param_with_fallback(
                "words_json_path", input_data, self.context
            ) or "time_stamps,words"

            # æå–æ–‡æœ¬å­—æ®µï¼ˆæ”¯æŒå¤šçº§è·¯å¾„ï¼‰
            text_value = self._extract_json_value(data, text_path)
            if text_value is None:
                raise ValueError(f"JSON æ–‡ä»¶ç¼ºå°‘æŒ‡å®šå­—æ®µ: {text_path}")

            # æå–è¯çº§æ—¶é—´æˆ³ï¼ˆæ”¯æŒå¤šä¸ªå€™é€‰å­—æ®µï¼Œé€—å·åˆ†éš”ï¼Œæ”¯æŒå¤šçº§è·¯å¾„ï¼‰
            words = None
            for candidate_path in words_path.split(","):
                candidate_path = candidate_path.strip()
                words = self._extract_json_value(data, candidate_path)
                if words is not None:
                    logger.info(
                        f"[{workflow_id}] ä»å­—æ®µ '{candidate_path}' æå–è¯çº§æ—¶é—´æˆ³"
                    )
                    break

            logger.info(
                f"[{workflow_id}] æˆåŠŸåŠ è½½å­—å¹•æ•°æ®: {file_path} "
                f"(text_field={text_path}, has_timestamps={words is not None})"
            )

            return {
                "text": text_value,
                "words": words  # å¯é€‰å­—æ®µ
            }

        except json.JSONDecodeError as e:
            raise ValueError(f"JSON æ–‡ä»¶è§£æå¤±è´¥: {file_path}") from e

    def _save_segmented_result(
        self,
        result: Dict[str, Any],
        max_chars: int,
        language: str
    ) -> str:
        """
        ä¿å­˜åˆ†å¥ç»“æœåˆ° JSON æ–‡ä»¶ã€‚

        Args:
            result: åˆ†å¥ç»“æœ
            max_chars: å­—ç¬¦é™åˆ¶
            language: è¯­è¨€ä»£ç 

        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        workflow_id = self.context.workflow_id

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = build_node_output_path(
            task_id=workflow_id,
            node_name=self.stage_name,
            file_type="data",
            filename="segmented_subtitles.json"
        )
        ensure_directory(output_file)

        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = {
            "method": "PySBD + LangChain",
            "language": language,
            "max_chars": max_chars,
            "total_segments": result["total_segments"],
            "statistics": result["statistics"],
            "segments": [asdict(seg) for seg in result["segments"]]
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info(f"[{workflow_id}] åˆ†å¥ç»“æœå·²ä¿å­˜: {output_file}")

        return output_file

    def _save_pysbd_result(
        self,
        pysbd_segments: List[SubtitleSegment],
        language: str
    ) -> str:
        """
        ä¿å­˜ PySBD åŸå§‹åˆ†å¥ç»“æœåˆ° JSON æ–‡ä»¶ï¼ˆä¸ç»è¿‡ LangChain å­—ç¬¦é™åˆ¶å¤„ç†ï¼‰ã€‚

        Args:
            pysbd_segments: PySBD åŸå§‹åˆ†å¥åˆ—è¡¨
            language: è¯­è¨€ä»£ç 

        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        workflow_id = self.context.workflow_id

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = build_node_output_path(
            task_id=workflow_id,
            node_name=self.stage_name,
            file_type="data",
            filename="segmented_subtitles_pysbd.json"
        )
        ensure_directory(output_file)

        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = {
            "method": "PySBD (without LangChain)",
            "language": language,
            "total_segments": len(pysbd_segments),
            "segments": [asdict(seg) for seg in pysbd_segments]
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info(f"[{workflow_id}] PySBD åŸå§‹åˆ†å¥ç»“æœå·²ä¿å­˜: {output_file}")

        return output_file

    def get_cache_key_fields(self) -> List[str]:
        """
        è¿”å›ç¼“å­˜é”®å­—æ®µåˆ—è¡¨ã€‚

        Returns:
            ç¼“å­˜é”®å­—æ®µåˆ—è¡¨
        """
        return [
            "subtitle_text",
            "subtitle_file",
            "max_chars",
            "language",
            "text_json_path",
            "words_json_path"
        ]

    def get_required_output_fields(self) -> List[str]:
        """
        è¿”å›å¿…éœ€çš„è¾“å‡ºå­—æ®µåˆ—è¡¨ã€‚

        Returns:
            è¾“å‡ºå­—æ®µåˆ—è¡¨
        """
        return ["segmented_subtitle_file", "segmented_subtitle_pysbd_file"]
