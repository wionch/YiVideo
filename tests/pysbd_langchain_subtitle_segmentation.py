#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PySBD + LangChain å­—å¹•åˆ†å¥æµ‹è¯•è„šæœ¬

ä¸¤é˜¶æ®µå¤„ç†ï¼š
1. PySBD è¯­ä¹‰åˆ†å¥
2. LangChain RecursiveCharacterTextSplitter å­—ç¬¦é™åˆ¶å¤„ç†ï¼ˆä¿æŠ¤è¯­ä¹‰è¾¹ç•Œï¼‰
"""

import json
import time
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass, asdict


@dataclass
class SubtitleSegment:
    """å­—å¹•ç‰‡æ®µ"""
    id: int  # ç‰‡æ®µå”¯ä¸€æ ‡è¯†ï¼ˆä»1å¼€å§‹ï¼‰
    text: str
    start: float
    end: float
    duration: float
    word_count: int
    char_count: int
    words: List[Dict[str, Any]]  # è¯çº§æ—¶é—´æˆ³åˆ—è¡¨


class ASRDataLoader:
    """åŠ è½½ Qwen3 ASR æ•°æ®"""

    @staticmethod
    def load(file_path: str) -> Dict[str, Any]:
        """åŠ è½½ ASR JSON æ•°æ®"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    @staticmethod
    def build_char_to_timestamp_map(text: str, timestamps: List[Dict]) -> Dict[int, Dict]:
        """
        æ„å»ºå­—ç¬¦ä½ç½®åˆ°æ—¶é—´æˆ³çš„æ˜ å°„

        Args:
            text: å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…å«æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
            timestamps: è¯çº§æ—¶é—´æˆ³åˆ—è¡¨ [{text, start, end}, ...]

        Returns:
            {char_index: {word, start, end, word_index}, ...}
        """
        char_map = {}
        text_idx = 0

        for word_idx, ts in enumerate(timestamps):
            word = ts["text"]
            start = ts["start"]
            end = ts["end"]

            # è·³è¿‡å‰å¯¼ç©ºæ ¼å’Œæ ‡ç‚¹ï¼ˆä½†ä¸è·³è¿‡æ’‡å·ï¼‰
            while text_idx < len(text) and text[text_idx] in ' \t\n.,!?;:"':
                text_idx += 1

            if text_idx >= len(text):
                break

            word_len = len(word)

            # å°è¯•ç›´æ¥åŒ¹é…
            if text_idx + word_len <= len(text) and \
               text[text_idx:text_idx+word_len].lower() == word.lower():
                for i in range(text_idx, text_idx + word_len):
                    char_map[i] = {
                        "word": word,
                        "start": start,
                        "end": end,
                        "word_index": word_idx
                    }
                text_idx += word_len
            else:
                # åŒ¹é…å¤±è´¥ï¼Œå°è¯•åœ¨é™„è¿‘èŒƒå›´å†…æŸ¥æ‰¾
                search_end = min(text_idx + 100, len(text))
                search_text = text[text_idx:search_end]
                found_offset = search_text.lower().find(word.lower())

                if found_offset != -1:
                    found_pos = text_idx + found_offset
                    for i in range(found_pos, found_pos + word_len):
                        char_map[i] = {
                            "word": word,
                            "start": start,
                            "end": end,
                            "word_index": word_idx
                        }
                    text_idx = found_pos + word_len

        return char_map


class PySBDLangChainSubtitleSegmenter:
    """PySBD + LangChain å­—å¹•åˆ†å¥å™¨"""

    def __init__(self, max_chars: int = 42):
        """
        åˆå§‹åŒ–åˆ†å¥å™¨

        Args:
            max_chars: å•è¡Œå­—å¹•æœ€å¤§å­—ç¬¦æ•°
        """
        try:
            import pysbd
            self.pysbd_seg = pysbd.Segmenter(language="en", clean=False)
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£… pysbd: pip install pysbd")

        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            # å­—å¹•ä¸“ç”¨é…ç½®ï¼šä¼˜å…ˆåœ¨æ ‡ç‚¹å¤„æ–­å¼€
            self.langchain_splitter = RecursiveCharacterTextSplitter(
                chunk_size=max_chars,
                chunk_overlap=0,
                keep_separator="end",  # ğŸ”‘ å…³é”®ï¼šåˆ†éš”ç¬¦ä¿ç•™åœ¨å‰ä¸€ä¸ªå—çš„ç»“å°¾
                separators=[
                    ", ",   # ä¼˜å…ˆçº§1ï¼šé€—å·åæ–­å¼€ï¼ˆè‡ªç„¶åœé¡¿ï¼‰
                    "; ",   # ä¼˜å…ˆçº§2ï¼šåˆ†å·åæ–­å¼€
                    ". ",   # ä¼˜å…ˆçº§3ï¼šå¥å·åæ–­å¼€
                    "! ",   # ä¼˜å…ˆçº§4ï¼šæ„Ÿå¹å·åæ–­å¼€
                    "? ",   # ä¼˜å…ˆçº§5ï¼šé—®å·åæ–­å¼€
                    " ",    # ä¼˜å…ˆçº§6ï¼šç©ºæ ¼
                    ""      # ä¼˜å…ˆçº§7ï¼šå¼ºåˆ¶æŒ‰å­—ç¬¦åˆ‡ï¼ˆæœ€åæ‰‹æ®µï¼‰
                ]
            )
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£… langchain-text-splitters: pip install langchain-text-splitters")

        self.max_chars = max_chars

    def segment(self, asr_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†å¥

        Args:
            asr_data: ASR æ•°æ® {text, time_stamps, ...}

        Returns:
            {
                segments: List[SubtitleSegment],
                total_segments: int,
                execution_time: float,
                statistics: {...}
            }
        """
        start_time = time.time()

        text = asr_data["text"]
        time_stamps = asr_data["time_stamps"]

        print("=" * 80)
        print("å¼€å§‹ä¸¤é˜¶æ®µåˆ†å¥å¤„ç†")
        print("=" * 80)
        print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"æ—¶é—´æˆ³æ•°é‡: {len(time_stamps)} ä¸ªè¯")
        print(f"å­—ç¬¦é™åˆ¶: {self.max_chars} å­—ç¬¦/è¡Œ")
        print()

        # é˜¶æ®µ 1: PySBD è¯­ä¹‰åˆ†å¥
        print("é˜¶æ®µ 1: PySBD è¯­ä¹‰åˆ†å¥...")
        pysbd_sentences = self.pysbd_seg.segment(text)
        print(f"âœ“ åˆ†å¥å®Œæˆï¼Œå…± {len(pysbd_sentences)} ä¸ªå¥å­\n")

        # æ„å»ºå­—ç¬¦æ˜ å°„
        print("é˜¶æ®µ 2: æ„å»ºå­—ç¬¦åˆ°æ—¶é—´æˆ³çš„æ˜ å°„...")
        char_map = ASRDataLoader.build_char_to_timestamp_map(text, time_stamps)
        print(f"âœ“ æ˜ å°„å®Œæˆï¼Œè¦†ç›– {len(char_map)} ä¸ªå­—ç¬¦\n")

        # é˜¶æ®µ 3: LangChain å­—ç¬¦é™åˆ¶å¤„ç† + æ—¶é—´æˆ³æ˜ å°„
        print("é˜¶æ®µ 3: LangChain å­—ç¬¦é™åˆ¶å¤„ç†...")
        segments = self._process_with_langchain(
            pysbd_sentences, text, char_map, time_stamps
        )
        print(f"âœ“ å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç”Ÿæˆ {len(segments)} ä¸ªå­—å¹•ç‰‡æ®µ\n")

        execution_time = time.time() - start_time

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        stats = self._calculate_statistics(segments, pysbd_sentences)

        return {
            "segments": segments,
            "total_segments": len(segments),
            "execution_time": execution_time,
            "statistics": stats
        }

    def _process_with_langchain(self,
                                sentences: List[str],
                                full_text: str,
                                char_map: Dict[int, Dict],
                                timestamps: List[Dict]) -> List[SubtitleSegment]:
        """
        ä½¿ç”¨ LangChain å¤„ç†å­—ç¬¦é™åˆ¶å¹¶æ˜ å°„æ—¶é—´æˆ³

        Args:
            sentences: PySBD åˆ†å¥ç»“æœ
            full_text: å®Œæ•´æ–‡æœ¬
            char_map: å­—ç¬¦åˆ°æ—¶é—´æˆ³çš„æ˜ å°„
            timestamps: åŸå§‹è¯çº§æ—¶é—´æˆ³åˆ—è¡¨

        Returns:
            List[SubtitleSegment]
        """
        segments = []
        char_offset = 0
        over_limit_count = 0
        segment_id = 1  # ä» 1 å¼€å§‹è®¡æ•°

        for idx, sentence in enumerate(sentences, 1):
            sentence = sentence.strip()
            if not sentence:
                continue

            # åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å¥å­ä½ç½®
            sent_start = full_text.find(sentence, char_offset)
            if sent_start == -1:
                print(f"  âš  å¥å­ {idx} æ— æ³•åœ¨åŸæ–‡ä¸­å®šä½ï¼Œè·³è¿‡")
                continue

            sent_end = sent_start + len(sentence) - 1

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å­—ç¬¦é™åˆ¶
            if len(sentence) <= self.max_chars:
                # ä¸è¶…é™ï¼Œç›´æ¥åˆ›å»ºç‰‡æ®µ
                seg = self._create_segment(
                    segment_id, sentence, sent_start, sent_end, char_map, timestamps
                )
                if seg:
                    segments.append(seg)
                    segment_id += 1
            else:
                # è¶…é™ï¼Œä½¿ç”¨ LangChain åˆ†å‰²
                over_limit_count += 1

                # LangChain åˆ†å‰²
                lines = self.langchain_splitter.split_text(sentence)

                # ä¸ºæ¯è¡Œåˆ†é…æ—¶é—´æˆ³
                line_char_offset = sent_start
                for line_text in lines:
                    line_text = line_text.strip()
                    if not line_text:
                        continue

                    # åœ¨åŸå¥ä¸­æ‰¾åˆ°è¿™ä¸€è¡Œçš„ä½ç½®
                    line_start = full_text.find(line_text, line_char_offset)
                    if line_start == -1:
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ï¼‰
                        clean_line = line_text.strip('.,!?;: ')
                        line_start = full_text.find(clean_line, line_char_offset)

                    if line_start != -1:
                        line_end = line_start + len(line_text) - 1
                        seg = self._create_segment(
                            segment_id, line_text, line_start, line_end, char_map, timestamps
                        )
                        if seg:
                            segments.append(seg)
                            segment_id += 1
                        line_char_offset = line_end + 1

            char_offset = sent_end + 1

        if over_limit_count > 0:
            print(f"  ğŸ“Š å…± {over_limit_count} ä¸ªå¥å­è¶…è¿‡ {self.max_chars} å­—ç¬¦é™åˆ¶ï¼Œå·²ä½¿ç”¨ LangChain åˆ†å‰²")

        return segments

    def _create_segment(self,
                       segment_id: int,
                       text: str,
                       start_pos: int,
                       end_pos: int,
                       char_map: Dict[int, Dict],
                       timestamps: List[Dict]) -> SubtitleSegment:
        """
        åˆ›å»ºå­—å¹•ç‰‡æ®µ

        Args:
            segment_id: ç‰‡æ®µå”¯ä¸€æ ‡è¯†ï¼ˆä»1å¼€å§‹ï¼‰
            text: ç‰‡æ®µæ–‡æœ¬
            start_pos: åœ¨åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
            end_pos: åœ¨åŸæ–‡ä¸­çš„ç»“æŸä½ç½®
            char_map: å­—ç¬¦æ˜ å°„è¡¨
            timestamps: åŸå§‹æ—¶é—´æˆ³åˆ—è¡¨

        Returns:
            SubtitleSegment or None
        """
        # è·å–èµ·å§‹å’Œç»“æŸæ—¶é—´æˆ³
        start_ts = char_map.get(start_pos, {}).get("start")
        end_ts = char_map.get(end_pos, {}).get("end")

        # å¦‚æœç›´æ¥æ‰¾ä¸åˆ°ï¼Œå‘å/å‘å‰æœç´¢
        if start_ts is None:
            for i in range(start_pos, min(end_pos + 1, start_pos + 100)):
                if i in char_map:
                    start_ts = char_map[i]["start"]
                    break

        if end_ts is None:
            for i in range(end_pos, max(start_pos - 1, end_pos - 100), -1):
                if i in char_map:
                    end_ts = char_map[i]["end"]
                    break

        if start_ts is None or end_ts is None:
            return None

        # æå–è¯çº§æ—¶é—´æˆ³
        word_indices = set()
        for char_pos in range(start_pos, end_pos + 1):
            if char_pos in char_map:
                word_idx = char_map[char_pos].get("word_index")
                if word_idx is not None:
                    word_indices.add(word_idx)

        word_indices = sorted(word_indices)
        segment_words = []
        for word_idx in word_indices:
            ts = timestamps[word_idx]
            segment_words.append({
                "text": ts["text"],
                "start": ts["start"],
                "end": ts["end"]
            })

        return SubtitleSegment(
            id=segment_id,
            text=text,
            start=start_ts,
            end=end_ts,
            duration=end_ts - start_ts,
            word_count=len(segment_words),
            char_count=len(text),
            words=segment_words
        )

    def _calculate_statistics(self,
                             segments: List[SubtitleSegment],
                             pysbd_sentences: List[str]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not segments:
            return {}

        # æ£€æŸ¥å­—ç¬¦é™åˆ¶åˆè§„æ€§
        over_limit = [s for s in segments if s.char_count > self.max_chars]
        compliance_rate = (len(segments) - len(over_limit)) / len(segments) * 100

        total_duration = sum(s.duration for s in segments)
        total_words = sum(s.word_count for s in segments)
        total_chars = sum(s.char_count for s in segments)

        durations = [s.duration for s in segments]
        word_counts = [s.word_count for s in segments]
        char_counts = [s.char_count for s in segments]

        return {
            "pysbd_sentences": len(pysbd_sentences),
            "final_segments": len(segments),
            "expansion_ratio": len(segments) / len(pysbd_sentences),
            "character_limit": {
                "max_allowed": self.max_chars,
                "compliance_rate": compliance_rate,
                "over_limit_count": len(over_limit),
                "max_char_count": max(char_counts) if char_counts else 0
            },
            "duration": {
                "total": total_duration,
                "average": total_duration / len(segments),
                "min": min(durations),
                "max": max(durations)
            },
            "word_count": {
                "total": total_words,
                "average": total_words / len(segments),
                "min": min(word_counts),
                "max": max(word_counts)
            },
            "char_count": {
                "total": total_chars,
                "average": total_chars / len(segments),
                "min": min(char_counts),
                "max": max(char_counts)
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    # ASR æ•°æ®æ–‡ä»¶è·¯å¾„
    asr_file = "/share/workflows/task_id/nodes/qwen3_asr.transcribe_audio/data/raw_transcribe_result_task_id.json"

    print("=" * 80)
    print("PySBD + LangChain å­—å¹•åˆ†å¥æµ‹è¯•".center(80))
    print("=" * 80)
    print()

    # åŠ è½½æ•°æ®
    print(f"æ•°æ®æº: {asr_file}\n")
    asr_data = ASRDataLoader.load(asr_file)

    # åˆ›å»ºåˆ†å¥å™¨
    segmenter = PySBDLangChainSubtitleSegmenter(max_chars=42)

    # æ‰§è¡Œåˆ†å¥
    result = segmenter.segment(asr_data)

    # æ‰“å°ç»“æœ
    print("=" * 80)
    print("æµ‹è¯•ç»“æœ".center(80))
    print("=" * 80)
    print()

    stats = result["statistics"]
    print(f"æ‰§è¡Œæ—¶é—´: {result['execution_time']:.4f} ç§’")
    print()
    print(f"PySBD åˆ†å¥æ•°: {stats['pysbd_sentences']}")
    print(f"æœ€ç»ˆå­—å¹•ç‰‡æ®µæ•°: {stats['final_segments']}")
    print(f"æ‰©å±•æ¯”ä¾‹: {stats['expansion_ratio']:.2f}x")
    print()
    print("å­—ç¬¦é™åˆ¶åˆè§„æ€§:")
    print(f"  æœ€å¤§å…è®¸: {stats['character_limit']['max_allowed']} å­—ç¬¦")
    print(f"  åˆè§„ç‡: {stats['character_limit']['compliance_rate']:.1f}%")
    print(f"  è¶…é™æ•°é‡: {stats['character_limit']['over_limit_count']}")
    print(f"  æœ€é•¿ç‰‡æ®µ: {stats['character_limit']['max_char_count']} å­—ç¬¦")
    print()
    print("ç‰‡æ®µæ—¶é•¿ç»Ÿè®¡:")
    print(f"  æ€»æ—¶é•¿: {stats['duration']['total']:.2f}s")
    print(f"  å¹³å‡: {stats['duration']['average']:.2f}s")
    print(f"  æœ€çŸ­: {stats['duration']['min']:.2f}s")
    print(f"  æœ€é•¿: {stats['duration']['max']:.2f}s")
    print()
    print("ç‰‡æ®µè¯æ•°ç»Ÿè®¡:")
    print(f"  æ€»è¯æ•°: {stats['word_count']['total']}")
    print(f"  å¹³å‡: {stats['word_count']['average']:.1f}")
    print(f"  æœ€å°‘: {stats['word_count']['min']}")
    print(f"  æœ€å¤š: {stats['word_count']['max']}")
    print()
    print("ç‰‡æ®µå­—ç¬¦æ•°ç»Ÿè®¡:")
    print(f"  æ€»å­—ç¬¦: {stats['char_count']['total']}")
    print(f"  å¹³å‡: {stats['char_count']['average']:.1f}")
    print(f"  æœ€å°‘: {stats['char_count']['min']}")
    print(f"  æœ€å¤š: {stats['char_count']['max']}")
    print()

    # å±•ç¤ºå‰ 10 ä¸ªç‰‡æ®µ
    print("=" * 80)
    print("å‰ 10 ä¸ªå­—å¹•ç‰‡æ®µï¼ˆå«è¯çº§æ—¶é—´æˆ³ï¼‰")
    print("=" * 80)
    for i, seg in enumerate(result["segments"][:10], 1):
        marker = "âš " if seg.char_count > 42 else "âœ“"
        print(f"\n{marker} {i}. [{seg.start:.2f}s - {seg.end:.2f}s] ({seg.duration:.2f}s, {seg.char_count}å­—ç¬¦, {seg.word_count}è¯)")
        print(f"   {seg.text}")
        if seg.words:
            print(f"   è¯çº§æ—¶é—´æˆ³ ({len(seg.words)} ä¸ªè¯):")
            for j in range(0, len(seg.words), 5):
                words_slice = seg.words[j:j+5]
                words_str = " | ".join([f"{w['text']}[{w['start']:.2f}-{w['end']:.2f}]"
                                       for w in words_slice])
                print(f"      {words_str}")

    # å¦‚æœæœ‰è¶…é™ç‰‡æ®µï¼Œå±•ç¤ºå®ƒä»¬
    over_limit_segs = [s for s in result["segments"] if s.char_count > 42]
    if over_limit_segs:
        print("\n" + "=" * 80)
        print(f"âš  è¶…è¿‡ 42 å­—ç¬¦çš„ç‰‡æ®µ ({len(over_limit_segs)} ä¸ª)")
        print("=" * 80)
        for i, seg in enumerate(over_limit_segs[:5], 1):
            print(f"\n{i}. [{seg.start:.2f}s - {seg.end:.2f}s] ({seg.char_count} å­—ç¬¦)")
            print(f"   {seg.text}")

    # ä¿å­˜ç»“æœ
    output_dir = Path("/app/tmp")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / "pysbd_langchain_subtitle_result.json"
    output_data = {
        "method": "PySBD + LangChain",
        "max_chars": 42,
        "execution_time": result["execution_time"],
        "total_segments": result["total_segments"],
        "statistics": stats,
        "segments": [asdict(seg) for seg in result["segments"]]
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 80)
    print(f"âœ“ ç»“æœå·²ä¿å­˜: {output_file}")
    print("=" * 80)


if __name__ == "__main__":
    main()
