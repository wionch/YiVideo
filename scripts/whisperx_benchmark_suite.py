#!/usr/bin/env python3
"""
WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
å¯¹æ¯”åŸç”Ÿåç«¯å’Œ Faster-Whisper åç«¯çš„æ€§èƒ½å·®å¼‚
"""

import sys
import os
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Tuple
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/app/services')

class WhisperXBenchmarkSuite:
    def __init__(self):
        self.test_audio = "videos/223.mp4"
        self.results_file = "logs/whisperx_benchmark_results.json"
        self.report_file = "logs/whisperx_benchmark_report.html"

        # æµ‹è¯•é…ç½®
        self.test_configs = [
            {"name": "Native Backend", "use_faster_whisper": False, "threads": 1},
            {"name": "Faster-Whisper (2 threads)", "use_faster_whisper": True, "threads": 2},
            {"name": "Faster-Whisper (4 threads)", "use_faster_whisper": True, "threads": 4},
            {"name": "Faster-Whisper (8 threads)", "use_faster_whisper": True, "threads": 8},
        ]

        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def load_whisperx_models(self, config: Dict[str, Any]) -> Tuple[Any, float]:
        """åŠ è½½ WhisperX æ¨¡å‹"""
        try:
            from services.workers.whisperx_service.app.tasks import get_whisperx_models
            import whisperx

            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            from services.common.config_loader import CONFIG
            original_config = CONFIG.get('whisperx_service', {}).copy()

            # åº”ç”¨æµ‹è¯•é…ç½®
            CONFIG['whisperx_service'].update(config)

            # æµ‹é‡åŠ è½½æ—¶é—´
            start_time = time.time()
            get_whisperx_models()
            load_time = time.time() - start_time

            # æ¢å¤åŸå§‹é…ç½®
            CONFIG['whisperx_service'].update(original_config)

            return whisperx, load_time

        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None, 0

    def benchmark_transcription(self, whisperx, config: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•è½¬å½•æ€§èƒ½"""
        result = {
            "config_name": config["name"],
            "use_faster_whisper": config["use_faster_whisper"],
            "threads": config["threads"],
            "load_time": 0,
            "inference_time": 0,
            "total_time": 0,
            "segments_count": 0,
            "memory_usage": 0,
            "success": False,
            "error": None
        }

        try:
            # åŠ è½½æ¨¡å‹
            start_time = time.time()
            whisperx, load_time = self.load_whisperx_models(config)
            result["load_time"] = load_time

            if not whisperx:
                result["error"] = "æ¨¡å‹åŠ è½½å¤±è´¥"
                return result

            # åŠ è½½éŸ³é¢‘
            audio = whisperx.load_audio(self.test_audio)

            # è·å–æ¨¡å‹
            from services.workers.whisperx_service.app.tasks import ASR_MODEL
            if ASR_MODEL is None:
                result["error"] = "æ¨¡å‹æœªæ­£ç¡®åŠ è½½"
                return result

            # æµ‹é‡æ¨ç†æ—¶é—´
            start_time = time.time()
            transcription = ASR_MODEL.transcribe(
                audio,
                batch_size=config.get("batch_size", 4),
                language=config.get("language", "zh")
            )
            inference_time = time.time() - start_time

            result["inference_time"] = inference_time
            result["total_time"] = result["load_time"] + inference_time
            result["segments_count"] = len(transcription.get("segments", []))
            result["success"] = True

            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            try:
                import psutil
                process = psutil.Process()
                result["memory_usage"] = process.memory_info().rss / 1024 / 1024  # MB
            except:
                pass

        except Exception as e:
            result["error"] = str(e)

        return result

    def run_comprehensive_benchmark(self) -> List[Dict[str, Any]]:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹ WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print(f"æµ‹è¯•æ–‡ä»¶: {self.test_audio}")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now()}")
        print("=" * 60)

        results = []

        for config in self.test_configs:
            print(f"\næµ‹è¯•é…ç½®: {config['name']}")
            print("-" * 40)

            try:
                # è¿è¡Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
                test_runs = []
                for run in range(3):  # æ¯ä¸ªé…ç½®è¿è¡Œ3æ¬¡
                    print(f"  è¿è¡Œ {run + 1}/3...")
                    result = self.benchmark_transcription(None, config)
                    test_runs.append(result)

                    if result["success"]:
                        print(f"  âœ… å®Œæˆ - åŠ è½½: {result['load_time']:.2f}s, æ¨ç†: {result['inference_time']:.2f}s")
                    else:
                        print(f"  âŒ å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    # ç­‰å¾…ä¸€ä¸‹è®©ç³»ç»Ÿå†·å´
                    time.sleep(5)

                # è®¡ç®—å¹³å‡å€¼
                successful_runs = [r for r in test_runs if r["success"]]
                if successful_runs:
                    avg_result = self._calculate_average(successful_runs, config)
                    results.append(avg_result)
                    print(f"  ğŸ“Š å¹³å‡ç»“æœ - æ€»æ—¶é—´: {avg_result['total_time']:.2f}s")
                else:
                    # æ‰€æœ‰è¿è¡Œéƒ½å¤±è´¥äº†
                    failed_result = {
                        "config_name": config["name"],
                        "use_faster_whisper": config["use_faster_whisper"],
                        "threads": config["threads"],
                        "success": False,
                        "error": "æ‰€æœ‰æµ‹è¯•è¿è¡Œéƒ½å¤±è´¥äº†"
                    }
                    results.append(failed_result)
                    print(f"  âŒ æ‰€æœ‰æµ‹è¯•è¿è¡Œéƒ½å¤±è´¥äº†")

            except Exception as e:
                error_result = {
                    "config_name": config["name"],
                    "use_faster_whisper": config["use_faster_whisper"],
                    "threads": config["threads"],
                    "success": False,
                    "error": str(e)
                }
                results.append(error_result)
                print(f"  âŒ æµ‹è¯•å¼‚å¸¸: {e}")

        return results

    def _calculate_average(self, results: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å¹³å‡å€¼"""
        avg_result = {
            "config_name": config["name"],
            "use_faster_whisper": config["use_faster_whisper"],
            "threads": config["threads"],
            "load_time": statistics.mean([r["load_time"] for r in results]),
            "inference_time": statistics.mean([r["inference_time"] for r in results]),
            "total_time": statistics.mean([r["total_time"] for r in results]),
            "segments_count": statistics.mean([r["segments_count"] for r in results]),
            "memory_usage": statistics.mean([r["memory_usage"] for r in results]),
            "success": True,
            "error": None,
            "runs_count": len(results),
            "load_time_std": statistics.stdev([r["load_time"] for r in results]),
            "inference_time_std": statistics.stdev([r["inference_time"] for r in results]),
        }

        return avg_result

    def analyze_results(self, results: List[Dict[str, Any]]):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print("\n" + "=" * 60)
        print("æ€§èƒ½åˆ†æç»“æœ")
        print("=" * 60)

        # æ‰¾åˆ°åŸºå‡†ï¼ˆåŸç”Ÿåç«¯ï¼‰
        baseline = next((r for r in results if r["config_name"] == "Native Backend" and r["success"]), None)
        if not baseline:
            print("âŒ æœªæ‰¾åˆ°åŸºå‡†æµ‹è¯•ç»“æœï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½åˆ†æ")
            return

        print(f"åŸºå‡† (Native Backend): {baseline['total_time']:.2f}s")
        print("-" * 50)

        # åˆ†ææ¯ä¸ª Faster-Whisper é…ç½®
        for result in results:
            if result["config_name"] != "Native Backend" and result["success"]:
                speedup = baseline["total_time"] / result["total_time"]
                memory_saving = (baseline["memory_usage"] - result["memory_usage"]) / baseline["memory_usage"] * 100

                print(f"{result['config_name']}:")
                print(f"  - é€Ÿåº¦æå‡: {speedup:.2f}x")
                print(f"  - å†…å­˜èŠ‚çœ: {memory_saving:.1f}%")
                print(f"  - æ€»æ—¶é—´: {result['total_time']:.2f}s (åŸºå‡†: {baseline['total_time']:.2f}s)")
                print(f"  - æ¨ç†æ—¶é—´: {result['inference_time']:.2f}s")
                print(f"  - å†…å­˜ä½¿ç”¨: {result['memory_usage']:.1f}MB")
                print()

    def save_results(self, results: List[Dict[str, Any]]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(os.path.dirname(self.results_file), exist_ok=True)

        data = {
            "timestamp": datetime.now().isoformat(),
            "test_file": self.test_audio,
            "results": results
        }

        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")

    def generate_html_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆ HTML æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; }}
        .section {{ margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 10px; }}
        .success {{ background: #d4edda; }}
        .warning {{ background: #fff3cd; }}
        .error {{ background: #f8d7da; }}
        .performance-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .performance-table th, .performance-table td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        .performance-table th {{ background: #f2f2f2; font-weight: bold; }}
        .improvement {{ color: #28a745; font-weight: bold; }}
        .benchmark {{ font-size: 1.2em; font-weight: bold; color: #007bff; }}
        .chart-container {{ margin: 20px 0; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š</h1>
        <p>æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>æµ‹è¯•æ–‡ä»¶: {self.test_audio}</p>
    </div>

    <div class="section">
        <h2>ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ</h2>
        <table class="performance-table">
            <thead>
                <tr>
                    <th>é…ç½®</th>
                    <th>åŠ è½½æ—¶é—´</th>
                    <th>æ¨ç†æ—¶é—´</th>
                    <th>æ€»æ—¶é—´</th>
                    <th>å†…å­˜ä½¿ç”¨</th>
                    <th>ç‰‡æ®µæ•°é‡</th>
                    <th>çŠ¶æ€</th>
                </tr>
            </thead>
            <tbody>
"""

        # æ·»åŠ æµ‹è¯•ç»“æœè¡Œ
        for result in results:
            status_class = "success" if result["success"] else "error"
            status_text = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"

            if result["success"]:
                load_time = f"{result['load_time']:.2f}s"
                inference_time = f"{result['inference_time']:.2f}s"
                total_time = f"<span class='benchmark'>{result['total_time']:.2f}s</span>"
                memory_usage = f"{result['memory_usage']:.1f}MB"
                segments_count = f"{result['segments_count']:.0f}"
            else:
                load_time = "N/A"
                inference_time = "N/A"
                total_time = "N/A"
                memory_usage = "N/A"
                segments_count = "N/A"

            html_content += f"""
                <tr class="{status_class}">
                    <td>{result['config_name']}</td>
                    <td>{load_time}</td>
                    <td>{inference_time}</td>
                    <td>{total_time}</td>
                    <td>{memory_usage}</td>
                    <td>{segments_count}</td>
                    <td>{status_text}</td>
                </tr>
"""

        html_content += """
            </tbody>
        </table>
    </div>

    <div class="section">
        <h2>ğŸ“ˆ æ€§èƒ½æå‡åˆ†æ</h2>
"""

        # æ·»åŠ æ€§èƒ½åˆ†æ
        baseline = next((r for r in results if r["config_name"] == "Native Backend" and r["success"]), None)
        if baseline:
            html_content += f"""
        <p><strong>åŸºå‡†é…ç½® (Native Backend):</strong> {baseline['total_time']:.2f}s</p>
        <table class="performance-table">
            <thead>
                <tr>
                    <th>é…ç½®</th>
                    <th>é€Ÿåº¦æå‡</th>
                    <th>å†…å­˜èŠ‚çœ</th>
                    <th>æ—¶é—´èŠ‚çœ</th>
                </tr>
            </thead>
            <tbody>
"""

            for result in results:
                if result["config_name"] != "Native Backend" and result["success"]:
                    speedup = baseline["total_time"] / result["total_time"]
                    memory_saving = (baseline["memory_usage"] - result["memory_usage"]) / baseline["memory_usage"] * 100
                    time_saving = (baseline["total_time"] - result["total_time"]) / baseline["total_time"] * 100

                    html_content += f"""
                <tr>
                    <td>{result['config_name']}</td>
                    <td><span class="improvement">{speedup:.2f}x</span></td>
                    <td><span class="improvement">{memory_saving:.1f}%</span></td>
                    <td><span class="improvement">{time_saving:.1f}%</span></td>
                </tr>
"""

            html_content += """
            </tbody>
        </table>
"""

        html_content += f"""
    </div>

    <div class="section">
        <h2>ğŸ“ æµ‹è¯•ç»“è®º</h2>
        <ul>
"""

        # æ·»åŠ æµ‹è¯•ç»“è®º
        successful_configs = [r for r in results if r["success"]]
        if successful_configs:
            fastest = min(successful_configs, key=lambda x: x["total_time"])
            lowest_memory = min(successful_configs, key=lambda x: x["memory_usage"])

            html_content += f"""
            <li>ğŸ† æœ€å¿«é…ç½®: <strong>{fastest['config_name']}</strong> ({fastest['total_time']:.2f}s)</li>
            <li>ğŸ’¾ æœ€çœå†…å­˜é…ç½®: <strong>{lowest_memory['config_name']}</strong> ({lowest_memory['memory_usage']:.1f}MB)</li>
"""

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é¢„æœŸç›®æ ‡
        if baseline and any(r["success"] for r in results if r["config_name"] != "Native Backend"):
            fastest_fw = min([r for r in results if r["success"] and r["config_name"] != "Native Backend"], key=lambda x: x["total_time"])
            speedup = baseline["total_time"] / fastest_fw["total_time"]

            if speedup >= 4:
                html_content += f'<li>ğŸ¯ <span class="improvement">è¾¾åˆ°é¢„æœŸç›®æ ‡: 4x é€Ÿåº¦æå‡ (å®é™…: {speedup:.2f}x)</span></li>\n'
            else:
                html_content += f'<li>âš ï¸ æœªè¾¾åˆ°é¢„æœŸç›®æ ‡: é¢„æœŸ 4x é€Ÿåº¦æå‡ (å®é™…: {speedup:.2f}x)</li>\n'

        html_content += f"""
        </ul>
    </div>

    <div class="section">
        <h2>ğŸ”§ æŠ€æœ¯ç»†èŠ‚</h2>
        <p>æµ‹è¯•ç¯å¢ƒ:</p>
        <ul>
            <li>åŸºç¡€é•œåƒ: PaddlePaddle 3.1.1 + CUDA 11.8</li>
            <li>WhisperX ç‰ˆæœ¬: 3.4.2</li>
            <li>Faster-Whisper ç‰ˆæœ¬: 1.1.1+</li>
            <li>æµ‹è¯•æ–¹æ³•: æ¯ä¸ªé…ç½®è¿è¡Œ3æ¬¡å–å¹³å‡å€¼</li>
        </ul>
    </div>
</body>
</html>
        """

        os.makedirs(os.path.dirname(self.report_file), exist_ok=True)
        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {self.report_file}")

    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        try:
            # è¿è¡Œæµ‹è¯•
            results = self.run_comprehensive_benchmark()

            # åˆ†æç»“æœ
            self.analyze_results(results)

            # ä¿å­˜ç»“æœ
            self.save_results(results)

            # ç”ŸæˆæŠ¥å‘Š
            self.generate_html_report(results)

            # è¾“å‡ºæ‘˜è¦
            successful_tests = [r for r in results if r["success"]]
            print(f"\n=== æµ‹è¯•æ‘˜è¦ ===")
            print(f"æ€»æµ‹è¯•æ•°: {len(results)}")
            print(f"æˆåŠŸæµ‹è¯•: {len(successful_tests)}")
            print(f"å¤±è´¥æµ‹è¯•: {len(results) - len(successful_tests)}")

            if successful_tests:
                fastest = min(successful_tests, key=lambda x: x["total_time"])
                print(f"æœ€å¿«é…ç½®: {fastest['config_name']} ({fastest['total_time']:.2f}s)")

            return results

        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return []

if __name__ == "__main__":
    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    test_file = "videos/223.mp4"
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        exit(1)

    benchmark = WhisperXBenchmarkSuite()
    results = benchmark.run_full_benchmark()
    exit(0 if results and any(r["success"] for r in results) else 1)