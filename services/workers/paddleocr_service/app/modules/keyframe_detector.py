# app/modules/keyframe_detector.py
import gc  # ğŸ†• å†…å­˜ä¼˜åŒ–: å¼•å…¥åƒåœ¾å›æ”¶æ¨¡å—
import json
import os
from datetime import datetime
from typing import Dict
from typing import List
from typing import Tuple

import cv2
import numpy as np
import torch

from .decoder import GPUDecoder
from .base_detector import BaseDetector, ConfigManager, ProgressTracker
from services.common.logger import get_logger

logger = get_logger('keyframe_detector')


class KeyFrameDetector(BaseDetector):
    """
    å…³é”®å¸§æ£€æµ‹å™¨ - ç®€åŒ–ç‰ˆæœ¬ (ä»…dHash)
    åŸºäºdHashç›¸ä¼¼åº¦çš„å…³é”®å¸§æ£€æµ‹ï¼Œå·²ç§»é™¤æ ‡å‡†å·®å’Œå¤§æ´¥ç®—æ³•
    
    å®ç°é€»è¾‘:
    1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
    2. é€å¸§æ¯”å¯¹: 1vs0, 2vs1, 3vs2...
    3. dHashç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
    
    å·²æ³¨é‡ŠåŠŸèƒ½:
    - æ ‡å‡†å·®è®¡ç®— (ç©ºç™½å¸§æ£€æµ‹)
    - å¤§æ´¥ç®—æ³• (è‡ªé€‚åº”é˜ˆå€¼)
    - ç©ºç™½å¸§vså†…å®¹å¸§çš„åˆ†ç±»é€»è¾‘
    """
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–å…³é”®å¸§æ£€æµ‹å™¨

        Args:
            config: æ£€æµ‹å™¨é…ç½®
        """
        # ä½¿ç”¨ConfigManageréªŒè¯å’Œè§„èŒƒåŒ–é…ç½®
        required_keys = []  # å…³é”®å¸§æ£€æµ‹å™¨æ²¡æœ‰å¿…éœ€çš„é…ç½®é¡¹
        optional_keys = {
            'dhash_size': 8,
            'similarity_threshold': 0.90,
            'frame_memory_estimate_mb': 0.307,
            'dhash_focus_ratio': 3.0,
            'min_focus_width': 200,
            'progress_interval_frames': 1000,
            'progress_interval_batches': 50
        }

        validated_config = ConfigManager.validate_config(config, required_keys, optional_keys)

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(validated_config)

        # è®¾ç½®å…³é”®å¸§æ£€æµ‹å™¨ç‰¹æœ‰çš„é…ç½®
        self.hash_size = ConfigManager.validate_range(
            validated_config['dhash_size'], 1, 32, 'dhash_size'
        )

        self.similarity_threshold = ConfigManager.validate_range(
            validated_config['similarity_threshold'], 0.0, 1.0, 'similarity_threshold'
        )

        self.dhash_focus_ratio = ConfigManager.validate_range(
            validated_config['dhash_focus_ratio'], 0.1, 10.0, 'dhash_focus_ratio'
        )

        self.min_focus_width = ConfigManager.validate_range(
            validated_config['min_focus_width'], 1, 1000, 'min_focus_width'
        )

        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
        self.progress_tracker = None

        logger.info(f"å…³é”®å¸§æ£€æµ‹å™¨å·²åŠ è½½ - ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold:.0%}, "
                   f"dHashç„¦ç‚¹åŒºåŸŸ: é«˜å·®Ã—{self.dhash_focus_ratio}")

    def _optimize_dhash_region(self, subtitle_area: Tuple[int, int, int, int]) -> Tuple[int, int, int, int]:
        """
        ä¼˜åŒ–dHashåˆ†æåŒºåŸŸï¼Œèšç„¦å­—å¹•æ¡ä¸­å¿ƒéƒ¨åˆ†
        
        ç­–ç•¥: ä½¿ç”¨é«˜å·®Ã—ç³»æ•°è®¡ç®—ä¸­å¿ƒåŒºåŸŸå®½åº¦ï¼Œé¿å…èƒŒæ™¯å˜åŒ–å¹²æ‰°
        
        Args:
            subtitle_area: åŸå§‹å­—å¹•åŒºåŸŸ (x1, y1, x2, y2)
            
        Returns:
            ä¼˜åŒ–åçš„dHashåˆ†æåŒºåŸŸ (x1, y1, x2, y2)
        """
        orig_x1, orig_y1, orig_x2, orig_y2 = subtitle_area
        
        # è®¡ç®—åŸåŒºåŸŸå°ºå¯¸
        orig_width = orig_x2 - orig_x1
        orig_height = orig_y2 - orig_y1
        
        # è®¡ç®—ä¸­å¿ƒç„¦ç‚¹åŒºåŸŸå®½åº¦ (é«˜å·®Ã—ç³»æ•°)
        focus_width = int(orig_height * self.dhash_focus_ratio)
        
        # åº”ç”¨æœ€å°å®½åº¦ä¿æŠ¤
        focus_width = max(focus_width, self.min_focus_width)
        
        # ç¡®ä¿ç„¦ç‚¹åŒºåŸŸä¸è¶…è¿‡åŸåŒºåŸŸ
        focus_width = min(focus_width, orig_width)
        
        # è®¡ç®—å±…ä¸­çš„ç„¦ç‚¹åŒºåŸŸè¾¹ç•Œ
        center_x = (orig_x1 + orig_x2) // 2
        focus_x1 = center_x - focus_width // 2
        focus_x2 = focus_x1 + focus_width
        
        # è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿åœ¨åŸåŒºåŸŸå†…
        focus_x1 = max(focus_x1, orig_x1)
        focus_x2 = min(focus_x2, orig_x2)
        
        # é«˜åº¦ä¿æŒä¸å˜
        focus_y1, focus_y2 = orig_y1, orig_y2
        
        optimized_region = (focus_x1, focus_y1, focus_x2, focus_y2)
        
        # è®¡ç®—å®é™…çš„ä¼˜åŒ–æ•ˆæœ (åŸºäºè¾¹ç•Œè°ƒæ•´åçš„çœŸå®å®½åº¦)
        actual_focus_width = focus_x2 - focus_x1
        reduction_ratio = (1 - (actual_focus_width / orig_width)) * 100
        print(f"ğŸ¯ dHashåŒºåŸŸä¼˜åŒ–: {orig_width}Ã—{orig_height} â†’ {actual_focus_width}Ã—{orig_height} "
              f"(å‡å°‘{reduction_ratio:.1f}%èƒŒæ™¯å¹²æ‰°)")
        
        return optimized_region

    def _extract_dhash_region_from_cache(self, cached_frame: np.ndarray, 
                                       dhash_region: Tuple[int, int, int, int],
                                       subtitle_area: Tuple[int, int, int, int]) -> np.ndarray:
        """
        ä»ç¼“å­˜çš„å®Œæ•´å­—å¹•æ¡ä¸­æå–dHashåˆ†æåŒºåŸŸ
        
        Args:
            cached_frame: ç¼“å­˜çš„å®Œæ•´å­—å¹•æ¡å›¾åƒ (H, W, C)
            dhash_region: dHashåˆ†æåŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            subtitle_area: å®Œæ•´å­—å¹•åŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            
        Returns:
            æå–çš„dHashåŒºåŸŸå›¾åƒ (ç°åº¦å›¾)
        """
        dhash_x1, dhash_y1, dhash_x2, dhash_y2 = dhash_region
        sub_x1, sub_y1, sub_x2, sub_y2 = subtitle_area
        
        # è®¡ç®—dHashåŒºåŸŸåœ¨ç¼“å­˜å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®
        rel_x1 = dhash_x1 - sub_x1
        rel_y1 = dhash_y1 - sub_y1
        rel_x2 = dhash_x2 - sub_x1
        rel_y2 = dhash_y2 - sub_y1
        
        # è¾¹ç•Œæ£€æŸ¥
        rel_x1 = max(0, rel_x1)
        rel_y1 = max(0, rel_y1)
        rel_x2 = min(cached_frame.shape[1], rel_x2)
        rel_y2 = min(cached_frame.shape[0], rel_y2)
        
        # æå–dHashåŒºåŸŸ
        dhash_region_img = cached_frame[rel_y1:rel_y2, rel_x1:rel_x2]
        
        # è½¬æ¢ä¸ºç°åº¦å›¾ (ä¸dHashè®¡ç®—ä¿æŒä¸€è‡´)
        if len(dhash_region_img.shape) == 3:
            # RGBè½¬ç°åº¦: 0.299*R + 0.587*G + 0.114*B
            dhash_gray = cv2.cvtColor(dhash_region_img, cv2.COLOR_RGB2GRAY)
        else:
            dhash_gray = dhash_region_img
            
        return dhash_gray

    def detect_keyframes(self, video_path: str, decoder: GPUDecoder, 
                        subtitle_area: Tuple[int, int, int, int]) -> List[int]:
        """
        æ£€æµ‹è§†é¢‘ä¸­æ‰€æœ‰å…³é”®å¸§ (ç®€åŒ–ç‰ˆæœ¬ï¼Œåªä½¿ç”¨dHash)
        
        å®ç°é€»è¾‘:
        1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        2. é€å¸§æ¯”å¯¹: 1vs0, 2vs1, 3vs2...
        3. dHashç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            subtitle_area: å­—å¹•åŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            
        Returns:
            å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ [0, 45, 89, 156, ...]
        """
        # ğŸ†• ä¼˜åŒ–dHashåˆ†æåŒºåŸŸï¼Œèšç„¦å­—å¹•ä¸­å¿ƒéƒ¨åˆ†  
        dhash_region = self._optimize_dhash_region(subtitle_area)
        keyframes, _ = self._compute_frame_features_and_detect(video_path, decoder, dhash_region)
        return keyframes
    
    def _compute_frame_features_and_detect(self, video_path: str, decoder: GPUDecoder, 
                                         dhash_region: Tuple[int, int, int, int]) -> Tuple[List[int], List[np.ndarray]]:
        """
        è®¡ç®—å¸§ç‰¹å¾å¹¶æ£€æµ‹å…³é”®å¸§ (ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸å¸¦ç¼“å­˜)
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            dhash_region: dHashåˆ†æåŒºåŸŸ
            
        Returns:
            Tuple[List[int], List[np.ndarray]]: å…³é”®å¸§ç´¢å¼•åˆ—è¡¨å’Œæ‰€æœ‰hash
        """
        all_hashes = self._compute_frame_features(video_path, decoder, dhash_region)
        keyframes = self._detect_keyframes_sequential(all_hashes)
        return keyframes, all_hashes

    def detect_keyframes_with_cache(self, video_path: str, decoder: GPUDecoder, 
                                   subtitle_area: Tuple[int, int, int, int]) -> Tuple[List[int], Dict[int, np.ndarray]]:
        """
        æ£€æµ‹è§†é¢‘ä¸­æ‰€æœ‰å…³é”®å¸§ + åŒæ­¥ç¼“å­˜å…³é”®å¸§å›¾åƒæ•°æ®
        
        ğŸ†• æ–°å¢åŠŸèƒ½: åœ¨å…³é”®å¸§æ£€æµ‹è¿‡ç¨‹ä¸­åŒæ­¥ç¼“å­˜å…³é”®å¸§çš„å›¾åƒæ•°æ®ï¼Œ
        é¿å…åç»­OCRè¯†åˆ«é˜¶æ®µçš„é‡å¤è§†é¢‘è§£ç 
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            subtitle_area: å­—å¹•åŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            
        Returns:
            Tuple[List[int], Dict[int, np.ndarray]]: 
            - å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ [0, 45, 89, ...]
            - å…³é”®å¸§å›¾åƒç¼“å­˜ {0: image_array, 45: image_array, ...}
        """
        print("ğŸ” å¼€å§‹å…³é”®å¸§æ£€æµ‹ (åŒæ­¥ç¼“å­˜æ¨¡å¼)...")
        x1, y1, x2, y2 = subtitle_area

        # ğŸ†• ä¼˜åŒ–dHashåˆ†æåŒºåŸŸï¼Œèšç„¦å­—å¹•ä¸­å¿ƒéƒ¨åˆ†  
        dhash_region = self._optimize_dhash_region(subtitle_area)

        # 1. æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„ç‰¹å¾ + åŒæ­¥ç¼“å­˜
        all_hashes, keyframe_cache = self._compute_frame_features_with_cache(
            video_path, decoder, dhash_region, subtitle_area
        )
        print(f"ğŸ“Š å®Œæˆç‰¹å¾è®¡ç®—: {len(all_hashes)} å¸§")

        # 2. å…³é”®å¸§é€å¸§æ£€æµ‹ - åªä½¿ç”¨dHash
        keyframes = self._detect_keyframes_sequential_with_logging(all_hashes, keyframe_cache, video_path, dhash_region, subtitle_area)
        
        # 4. åªä¿ç•™æ£€æµ‹åˆ°çš„å…³é”®å¸§ç¼“å­˜ï¼Œé‡Šæ”¾å…¶ä»–ç¼“å­˜
        final_keyframe_cache = {k: keyframe_cache[k] for k in keyframes if k in keyframe_cache}
        
        # 5. ğŸ†• å†…å­˜ä¼˜åŒ–: æ˜¾å¼åˆ é™¤ä¸´æ—¶ç¼“å­˜å¹¶å¼ºåˆ¶åƒåœ¾å›æ”¶
        del keyframe_cache
        gc.collect()
        
        # 6. æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ (ä½¿ç”¨é…ç½®çš„å†…å­˜ä¼°ç®—)
        cache_size_mb = len(final_keyframe_cache) * self.frame_memory_estimate_mb  
        print(f"âœ… æ£€æµ‹åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        print(f"ğŸ—‚ï¸  å…³é”®å¸§ç¼“å­˜: {len(final_keyframe_cache)} å¸§ï¼Œçº¦ {cache_size_mb:.1f}MB")
        
        return keyframes, final_keyframe_cache
    
    def _detect_keyframes_sequential_with_logging(self, hashes: List[np.ndarray], 
                                                keyframe_cache: Dict[int, np.ndarray],
                                                video_path: str,
                                                dhash_region: Tuple[int, int, int, int],
                                                subtitle_area: Tuple[int, int, int, int]) -> List[int]:
        """
        æŒ‰ç…§æ–°é€»è¾‘è¿›è¡Œå…³é”®å¸§æ£€æµ‹ + è¯¦ç»†æ—¥å¿—è®°å½•
        å®ç°ç”¨æˆ·éœ€æ±‚çš„å…·ä½“ç®—æ³• + ä¿å­˜dHashå¯¹æ¯”æ•°æ®å’Œå­—å¹•æ¡å›¾ç‰‡
        
        Args:
            hashes: æ‰€æœ‰å¸§çš„dHashç‰¹å¾åˆ—è¡¨
            keyframe_cache: å…³é”®å¸§å›¾åƒç¼“å­˜ (å®Œæ•´å­—å¹•åŒºåŸŸ)
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
            dhash_region: dHashåˆ†æåŒºåŸŸ (ç”¨äºæˆªå›¾ä¿å­˜)
            subtitle_area: å®Œæ•´å­—å¹•åŒºåŸŸ (ç”¨äºå‚è€ƒ)
            
        Returns:
            å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
        """
        # è¾¹ç•Œæƒ…å†µæ£€æŸ¥
        if not hashes or len(hashes) == 0:
            print("âš ï¸ è­¦å‘Š: æ²¡æœ‰å¸§æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        if len(hashes) == 1:
            print("ğŸ“Œ å•å¸§è§†é¢‘ï¼Œè¿”å›ç¬¬0å¸§ä½œä¸ºå…³é”®å¸§")
            return [0]
        
        keyframes = []
        dhash_log_data = []  # ç”¨äºä¿å­˜è¯¦ç»†çš„dHashå¯¹æ¯”æ•°æ®
        
        # ç»Ÿè®¡æ•°æ®åˆå§‹åŒ– - ä½¿ç”¨åŠ¨æ€å˜é‡å
        similarity_stats = {
            'gte_threshold': 0,  # >=é˜ˆå€¼çš„å¸§æ•°
            'lt_threshold': 0,   # <é˜ˆå€¼çš„å¸§æ•°
        }
        
        # 1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        keyframes.append(0)
        print(f"ğŸ“Œ å…³é”®å¸§ 0: é»˜è®¤ç¬¬ä¸€å¸§")
        
        # è®°å½•ç¬¬ä¸€å¸§çš„æ—¥å¿—æ•°æ®
        dhash_log_data.append({
            "frame_index": 0,
            "threshold": self.similarity_threshold,
            "similarity_with_previous": None,  # ç¬¬ä¸€å¸§æ²¡æœ‰å‰ä¸€å¸§
            "is_keyframe": True,
            "subtitle_frame_path": None
        })
        
        print(f"ğŸ”„ æ­£åœ¨åˆ†æ {len(hashes)} å¸§çš„ç›¸ä¼¼åº¦...")
        
        # 2. ä»ç¬¬1å¸§å¼€å§‹é€å¸§æ¯”å¯¹
        for curr_frame in range(1, len(hashes)):
            prev_frame = curr_frame - 1
            
            # è®¡ç®—ç›¸ä¼¼åº¦ - åªä½¿ç”¨dHash
            similarity = self._calculate_similarity(
                hashes[prev_frame], hashes[curr_frame]
            )
            
            # ç»Ÿè®¡ç›¸ä¼¼åº¦åˆ†å¸ƒ - ä½¿ç”¨åŠ¨æ€é˜ˆå€¼
            if similarity >= self.similarity_threshold:
                similarity_stats['gte_threshold'] += 1
            else:
                similarity_stats['lt_threshold'] += 1
            
            # 3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
            is_keyframe = similarity < self.similarity_threshold
            if is_keyframe:
                keyframes.append(curr_frame)
            
            # è®°å½•è¯¦ç»†çš„dHashå¯¹æ¯”æ•°æ®
            dhash_log_data.append({
                "frame_index": curr_frame,
                "threshold": self.similarity_threshold,
                "similarity_with_previous": round(similarity, 4),
                "is_keyframe": is_keyframe,
                "subtitle_frame_path": None
            })
            
            # è¿›åº¦æ˜¾ç¤º (æŒ‰é…ç½®é—´éš”æ˜¾ç¤º)
            if curr_frame % self.progress_interval_frames == 0:
                progress = (curr_frame / len(hashes)) * 100
                threshold_percent = int(self.similarity_threshold * 100)
                print(f"  ğŸ” æ£€æµ‹è¿›åº¦: {curr_frame}/{len(hashes)} ({progress:.1f}%) | "
                      f"ç›¸ä¼¼åº¦åˆ†å¸ƒ: >={threshold_percent}%å¸§:{similarity_stats['gte_threshold']}/"
                      f"<{threshold_percent}%å¸§:{similarity_stats['lt_threshold']} | "
                      f"å…³é”®å¸§:{len(keyframes)}ä¸ª")
        
        # # ä»»åŠ¡1: æ³¨é‡Šæ—¥å¿—ä¿å­˜åŠŸèƒ½
        # # ä¿å­˜dHashå¯¹æ¯”æ—¥å¿—æ–‡ä»¶
        # video_name = os.path.splitext(os.path.basename(video_path))[0]
        # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # log_filename = f'./logs/dhash_analysis_{video_name}_{timestamp}.json'
        
        # log_summary = {
        #     "video_path": video_path,
        #     "total_frames": len(hashes),
        #     "similarity_threshold": self.similarity_threshold,
        #     "detected_keyframes": len(keyframes),
        #     "keyframe_ratio": len(keyframes) / len(hashes),
        #     "analysis_timestamp": datetime.now().isoformat(),
        #     "frames_data": dhash_log_data
        # }
        
        # # ç¡®ä¿logsç›®å½•å­˜åœ¨
        # os.makedirs('./logs', exist_ok=True)
        # with open(log_filename, 'w', encoding='utf-8') as f:
        #     json.dump(log_summary, f, indent=2, ensure_ascii=False)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ - ä½¿ç”¨åŠ¨æ€å˜é‡å
        total_compared = len(hashes) - 1  # ç¬¬ä¸€å¸§ä¸å‚ä¸æ¯”è¾ƒ
        threshold_percent = int(self.similarity_threshold * 100)
        print(f"ğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡(æ€»è®¡{total_compared}å¸§): >={threshold_percent}%å¸§:{similarity_stats['gte_threshold']}/"
              f"<{threshold_percent}%å¸§:{similarity_stats['lt_threshold']}")
        print(f"âœ… å…³é”®å¸§æ£€æµ‹å®Œæˆ: å…±æ‰¾åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        # print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜: {log_filename}") # ä»»åŠ¡1: æ³¨é‡Šæ—¥å¿—ä¿å­˜åŠŸèƒ½
        
        return keyframes
    
    def _detect_keyframes_sequential(self, hashes: List[np.ndarray]) -> List[int]:
        """
        æŒ‰ç…§æ–°é€»è¾‘è¿›è¡Œå…³é”®å¸§æ£€æµ‹
        å®ç°ç”¨æˆ·éœ€æ±‚çš„å…·ä½“ç®—æ³•
        """
        # è¾¹ç•Œæƒ…å†µæ£€æŸ¥
        if not hashes or len(hashes) == 0:
            print("âš ï¸ è­¦å‘Š: æ²¡æœ‰å¸§æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        if len(hashes) == 1:
            print("ğŸ“Œ å•å¸§è§†é¢‘ï¼Œè¿”å›ç¬¬0å¸§ä½œä¸ºå…³é”®å¸§")
            return [0]
        
        keyframes = []
        
        # ç»Ÿè®¡æ•°æ®åˆå§‹åŒ– - ä½¿ç”¨åŠ¨æ€å˜é‡å
        similarity_stats = {
            'gte_threshold': 0,  # >=é˜ˆå€¼çš„å¸§æ•°
            'lt_threshold': 0,   # <é˜ˆå€¼çš„å¸§æ•°
        }
        
        # 1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        keyframes.append(0)
        print(f"ğŸ“Œ å…³é”®å¸§ 0: é»˜è®¤ç¬¬ä¸€å¸§")
        
        print(f"ğŸ”„ æ­£åœ¨åˆ†æ {len(hashes)} å¸§çš„ç›¸ä¼¼åº¦...")
        
        # 2. ä»ç¬¬1å¸§å¼€å§‹é€å¸§æ¯”å¯¹
        for curr_frame in range(1, len(hashes)):
            prev_frame = curr_frame - 1
            
            # è®¡ç®—ç›¸ä¼¼åº¦ - åªä½¿ç”¨dHash
            similarity = self._calculate_similarity(
                hashes[prev_frame], hashes[curr_frame]
            )
            
            # ç»Ÿè®¡ç›¸ä¼¼åº¦åˆ†å¸ƒ - ä½¿ç”¨åŠ¨æ€é˜ˆå€¼
            if similarity >= self.similarity_threshold:
                similarity_stats['gte_threshold'] += 1
            else:
                similarity_stats['lt_threshold'] += 1
            
            # 3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
            if similarity < self.similarity_threshold:
                keyframes.append(curr_frame)
            
            # è¿›åº¦æ˜¾ç¤º (æŒ‰é…ç½®é—´éš”æ˜¾ç¤º)
            if curr_frame % self.progress_interval_frames == 0:
                progress = (curr_frame / len(hashes)) * 100
                threshold_percent = int(self.similarity_threshold * 100)
                print(f"  ğŸ” æ£€æµ‹è¿›åº¦: {curr_frame}/{len(hashes)} ({progress:.1f}%) | "
                      f"ç›¸ä¼¼åº¦åˆ†å¸ƒ: >={threshold_percent}%å¸§:{similarity_stats['gte_threshold']}/"
                      f"<{threshold_percent}%å¸§:{similarity_stats['lt_threshold']} | "
                      f"å…³é”®å¸§:{len(keyframes)}ä¸ª")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ - ä½¿ç”¨åŠ¨æ€å˜é‡å
        total_compared = len(hashes) - 1  # ç¬¬ä¸€å¸§ä¸å‚ä¸æ¯”è¾ƒ
        threshold_percent = int(self.similarity_threshold * 100)
        print(f"ğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡(æ€»è®¡{total_compared}å¸§): >={threshold_percent}%å¸§:{similarity_stats['gte_threshold']}/"
              f"<{threshold_percent}%å¸§:{similarity_stats['lt_threshold']}")
        print(f"âœ… å…³é”®å¸§æ£€æµ‹å®Œæˆ: å…±æ‰¾åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        return keyframes
    
    def _calculate_similarity(self, hash1: np.ndarray, hash2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        ç›¸ä¼¼åº¦è®¡ç®—è§„åˆ™:
        - åŸºäºdHashçš„æ±‰æ˜è·ç¦»è®¡ç®—
        
        åŸºäºè¡Œä¸šæ ‡å‡†Dr. Neal Krawetzçš„ç ”ç©¶æˆæœ
        """
        # è¾¹ç•Œæƒ…å†µæ£€æŸ¥
        if hash1 is None or hash2 is None:
            return 0.0
        
        if hash1.size == 0 or hash2.size == 0:
            return 0.0
            
        if hash1.size != hash2.size:
            print(f"âš ï¸ è­¦å‘Š: hashå°ºå¯¸ä¸åŒ¹é…: {hash1.size} vs {hash2.size}")
            return 0.0
        
        # ç›´æ¥åŸºäºdHashè®¡ç®—ç›¸ä¼¼åº¦
        hamming_distance = np.count_nonzero(hash1 != hash2)
        max_possible_distance = hash1.size  # 64 for 8x8 dHash
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if max_possible_distance == 0:
            return 1.0  # å¦‚æœä¸¤ä¸ªéƒ½æ˜¯ç©ºæ•°ç»„ï¼Œè®¤ä¸ºç›¸åŒ
        
        # ç›¸ä¼¼åº¦ = 1 - (æ±‰æ˜è·ç¦» / æœ€å¤§å¯èƒ½è·ç¦»)
        similarity = 1.0 - (hamming_distance / max_possible_distance)
        
        return similarity
    
    def _compute_frame_features(self, video_path: str, decoder: GPUDecoder, 
                               dhash_region: Tuple[int, int, int, int]) -> List[np.ndarray]:
        """
        æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„dHash (ä½¿ç”¨ä¼˜åŒ–åçš„ä¸­å¿ƒåŒºåŸŸ)
        """
        all_hashes = []
        x1, y1, x2, y2 = dhash_region

        frame_count = 0
        batch_count = 0
        
        print("ğŸ”„ æ­£åœ¨è®¡ç®—è§†é¢‘ç‰¹å¾ (ä½¿ç”¨ä¼˜åŒ–çš„dHashåŒºåŸŸ)...")
        
        for batch_tensor, _ in decoder.decode_gpu(video_path):
            # è£å‰ªä¼˜åŒ–åçš„dHashåŒºåŸŸ  
            dhash_cropped_batch = batch_tensor[:, :, y1:y2, x1:x2]

            # è®¡ç®—dHash (å¸¦GPUå†…å­˜ä¿æŠ¤)
            batch_hashes = None  # åˆå§‹åŒ–é¿å…æœªå®šä¹‰é”™è¯¯
            try:
                grayscale_batch = dhash_cropped_batch.float().mean(dim=1, keepdim=True)
                resized_batch = torch.nn.functional.interpolate(
                    grayscale_batch, 
                    size=(self.hash_size, self.hash_size + 1), 
                    mode='bilinear', align_corners=False
                )
                diff = resized_batch[:, :, :, 1:] > resized_batch[:, :, :, :-1]
                hashes_np = diff.cpu().numpy().astype(np.uint8).reshape(diff.shape[0], -1)
                all_hashes.extend(hashes_np)
                
                # æ˜¾å¼æ¸…ç†ä¸­é—´GPUå˜é‡ï¼Œé‡Šæ”¾æ˜¾å­˜
                del grayscale_batch, resized_batch, diff, hashes_np
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡batch {batch_count}: {e}")
                    # è·³è¿‡å½“å‰batchï¼Œä½†ä»éœ€æ›´æ–°è®¡æ•°å™¨
                    frame_count += batch_tensor.size(0)
                    batch_count += 1
                    continue
                else:
                    raise e
            
            frame_count += batch_tensor.size(0)
            batch_count += 1
            
            # æ˜¾å¼åˆ é™¤æ‰¹æ¬¡tensorï¼Œé‡Šæ”¾GPUå†…å­˜
            del batch_tensor, dhash_cropped_batch
            
            # æ¯é…ç½®é—´éš”æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if batch_count % self.progress_interval_batches == 0:
                print(f"  ğŸ“Š å·²å¤„ç† {frame_count} å¸§...")
                # é—´éš”æ€§å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
            
        print(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: å…±å¤„ç† {frame_count} å¸§")
        
        # GPU èµ„æºé‡Šæ”¾
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
            
        return all_hashes
    
    def _compute_frame_features_with_cache(self, video_path: str, decoder: GPUDecoder, 
                                          dhash_region: Tuple[int, int, int, int],
                                          cache_region: Tuple[int, int, int, int]) -> Tuple[List[np.ndarray], Dict[int, np.ndarray]]:
        """
        æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„dHash + æ™ºèƒ½ç¼“å­˜å…³é”®å¸§å›¾åƒ
        
        ğŸ†• åŒºåŸŸåˆ†ç¦»ç­–ç•¥:
        - dhash_region: ç”¨äºdHashè®¡ç®—çš„ä¼˜åŒ–åŒºåŸŸ(èšç„¦ä¸­å¿ƒï¼Œå‡å°‘èƒŒæ™¯å¹²æ‰°)  
        - cache_region: ç”¨äºå›¾åƒç¼“å­˜çš„å®Œæ•´å­—å¹•åŒºåŸŸ(OCRè¯†åˆ«éœ€è¦)
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹ 
            dhash_region: dHashåˆ†æåŒºåŸŸ (x1, y1, x2, y2) - ä¸­å¿ƒç„¦ç‚¹åŒºåŸŸ
            cache_region: å›¾åƒç¼“å­˜åŒºåŸŸ (x1, y1, x2, y2) - å®Œæ•´å­—å¹•åŒºåŸŸ
            
        Returns:
            Tuple[List[np.ndarray], Dict[int, np.ndarray]]:
            - all_hashes: æ‰€æœ‰å¸§çš„dHashç‰¹å¾åˆ—è¡¨
            - keyframe_cache: å€™é€‰å…³é”®å¸§çš„å›¾åƒç¼“å­˜å­—å…¸
        """
        all_hashes = []
        keyframe_cache = {}
        
        # dHashè®¡ç®—åŒºåŸŸ
        dhash_x1, dhash_y1, dhash_x2, dhash_y2 = dhash_region
        # å›¾åƒç¼“å­˜åŒºåŸŸ  
        cache_x1, cache_y1, cache_x2, cache_y2 = cache_region

        frame_count = 0
        batch_count = 0
        prev_hash = None
        cached_frames_count = 0
        
        print("ğŸ”„ æ­£åœ¨è®¡ç®—è§†é¢‘ç‰¹å¾å¹¶æ™ºèƒ½ç¼“å­˜...")
        
        for batch_tensor, _ in decoder.decode_gpu(video_path):
            # ğŸ¯ å…ˆè£å‰ªdHashè®¡ç®—åŒºåŸŸ (ä¼˜å…ˆå¤„ç†ï¼Œå‡å°‘å†…å­˜å ç”¨)
            dhash_cropped = batch_tensor[:, :, dhash_y1:dhash_y2, dhash_x1:dhash_x2]

            # è®¡ç®—dHash (å¸¦GPUå†…å­˜ä¿æŠ¤) - ä½¿ç”¨ä¼˜åŒ–åçš„ä¸­å¿ƒåŒºåŸŸ
            batch_hashes = None  # åˆå§‹åŒ–é¿å…æœªå®šä¹‰é”™è¯¯
            try:
                grayscale_batch = dhash_cropped.float().mean(dim=1, keepdim=True)
                resized_batch = torch.nn.functional.interpolate(
                    grayscale_batch, 
                    size=(self.hash_size, self.hash_size + 1), 
                    mode='bilinear', align_corners=False
                )
                diff = resized_batch[:, :, :, 1:] > resized_batch[:, :, :, :-1]
                batch_hashes = diff.cpu().numpy().astype(np.uint8).reshape(diff.shape[0], -1)
                all_hashes.extend(batch_hashes)
                
                # æ˜¾å¼æ¸…ç†ä¸­é—´GPUå˜é‡ï¼Œé‡Šæ”¾æ˜¾å­˜
                del grayscale_batch, resized_batch, diff
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡batch {batch_count}: {e}")
                    # è·³è¿‡å½“å‰batchï¼Œä½†ä»éœ€æ›´æ–°è®¡æ•°å™¨
                    frame_count += batch_tensor.size(0)
                    batch_count += 1
                    continue
                else:
                    raise e
            
            # ğŸ†• æ™ºèƒ½ç¼“å­˜å€™é€‰å…³é”®å¸§ (åªæœ‰åœ¨GPUè®¡ç®—æˆåŠŸæ—¶æ‰æ‰§è¡Œ)
            # ä¼˜åŒ–ç­–ç•¥ï¼šåªåœ¨éœ€è¦ç¼“å­˜æ—¶æ‰è£å‰ªcacheåŒºåŸŸï¼Œå‡å°‘ä¸å¿…è¦çš„GPUæ“ä½œ
            if batch_hashes is not None:
                cache_cropped = None  # å»¶è¿Ÿåˆå§‹åŒ–
                
                for i, curr_hash in enumerate(batch_hashes):
                    frame_idx = frame_count + i
                    
                    # ç¬¬ä¸€å¸§é»˜è®¤ç¼“å­˜
                    if frame_idx == 0:
                        if cache_cropped is None:
                            cache_cropped = batch_tensor[:, :, cache_y1:cache_y2, cache_x1:cache_x2]
                        # ç¼“å­˜å®Œæ•´å­—å¹•æ¡åŒºåŸŸ (ç”¨äºOCRè¯†åˆ«)
                        frame_np = cache_cropped[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)
                        keyframe_cache[frame_idx] = frame_np
                        prev_hash = curr_hash
                        cached_frames_count += 1
                        continue
                    
                    # å¿«é€Ÿç›¸ä¼¼åº¦é¢„åˆ¤æ–­ (ç²—ç­›) - æ— æ¡ä»¶æ›´æ–°prev_hash
                    if prev_hash is not None:
                        hamming_distance = np.count_nonzero(curr_hash != prev_hash)
                        rough_similarity = 1.0 - (hamming_distance / curr_hash.size)
                        
                        # ç¼“å­˜å¯èƒ½æ˜¯å…³é”®å¸§çš„å¸§ (ä½¿ç”¨ç›¸åŒçš„ç›¸ä¼¼åº¦é˜ˆå€¼)
                        if rough_similarity < self.similarity_threshold:
                            if cache_cropped is None:
                                cache_cropped = batch_tensor[:, :, cache_y1:cache_y2, cache_x1:cache_x2]
                            # ç¼“å­˜å®Œæ•´å­—å¹•æ¡åŒºåŸŸ (ç”¨äºOCRè¯†åˆ«)
                            frame_np = cache_cropped[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)
                            keyframe_cache[frame_idx] = frame_np
                            cached_frames_count += 1
                        
                        # æ— æ¡ä»¶æ›´æ–°prev_hashä»¥ä¿æŒè¿ç»­æ€§
                        prev_hash = curr_hash
            
            frame_count += batch_tensor.size(0)
            batch_count += 1
            
            # æ˜¾å¼åˆ é™¤æ‰¹æ¬¡æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
            del batch_tensor
            if cache_cropped is not None:
                del cache_cropped
            if batch_hashes is not None:
                del batch_hashes
            
            # æ¯é…ç½®é—´éš”æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ + ç¼“å­˜ç»Ÿè®¡
            if batch_count % self.progress_interval_batches == 0:
                cache_mb = cached_frames_count * self.frame_memory_estimate_mb
                cache_ratio = (cached_frames_count / frame_count) * 100
                print(f"  ğŸ“Š å·²å¤„ç† {frame_count} å¸§ï¼Œé¢„ç¼“å­˜ {cached_frames_count} å¸§ ({cache_ratio:.1f}%, ~{cache_mb:.1f}MB)")
                # é—´éš”æ€§å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
            
        # æœ€ç»ˆç»Ÿè®¡ (ä½¿ç”¨é…ç½®çš„å†…å­˜ä¼°ç®—)
        final_cache_mb = cached_frames_count * self.frame_memory_estimate_mb
        cache_ratio = (cached_frames_count / frame_count) * 100
        print(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: å…±å¤„ç† {frame_count} å¸§")
        print(f"ğŸ—‚ï¸  é¢„ç¼“å­˜ç»Ÿè®¡: {cached_frames_count} å¸§ ({cache_ratio:.1f}%), çº¦ {final_cache_mb:.1f}MB")
        
        # GPU èµ„æºé‡Šæ”¾
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        return all_hashes, keyframe_cache
    
    def generate_subtitle_segments(self, keyframes: List[int], 
                                 fps: float, total_frames: int) -> List[Dict]:
        """
        ä»å…³é”®å¸§åˆ—è¡¨ç”Ÿæˆå­—å¹•æ®µè½
        æ¯ä¸¤ä¸ªè¿ç»­å…³é”®å¸§ä¹‹é—´å½¢æˆä¸€ä¸ªæ®µè½
        
        Args:
            keyframes: å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
            fps: è§†é¢‘å¸§ç‡
            total_frames: è§†é¢‘æ€»å¸§æ•°
            
        Returns:
            æ®µè½åˆ—è¡¨ï¼ŒåŒ…å«å…³é”®å¸§ä¿¡æ¯
        """
        # è¾“å…¥éªŒè¯
        if not keyframes:
            print("âš ï¸ è­¦å‘Š: å…³é”®å¸§åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºæ®µè½åˆ—è¡¨")
            return []
        
        if fps <= 0:
            raise ValueError(f"fpså¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {fps}")
        
        if total_frames <= 0:
            raise ValueError(f"total_frameså¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {total_frames}")
        
        if not isinstance(keyframes, list):
            raise TypeError(f"keyframeså¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(keyframes)}")
        
        # éªŒè¯å…³é”®å¸§ç´¢å¼•çš„æœ‰æ•ˆæ€§
        if any(frame < 0 or frame >= total_frames for frame in keyframes):
            invalid_frames = [frame for frame in keyframes if frame < 0 or frame >= total_frames]
            raise ValueError(f"å…³é”®å¸§ç´¢å¼•è¶…å‡ºæœ‰æ•ˆèŒƒå›´[0, {total_frames-1}]: {invalid_frames}")
        
        segments = []
        
        print(f"ğŸ—ï¸ ä» {len(keyframes)} ä¸ªå…³é”®å¸§ç”Ÿæˆå­—å¹•æ®µè½...")
        
        for i in range(len(keyframes)):
            start_frame = keyframes[i]
            
            # ç¡®å®šç»“æŸå¸§
            if i + 1 < len(keyframes):
                end_frame = keyframes[i + 1] - 1  # ä¸‹ä¸€å…³é”®å¸§çš„å‰ä¸€å¸§
            else:
                end_frame = total_frames - 1  # è§†é¢‘çš„æœ€åä¸€å¸§
            
            # è®¡ç®—æ—¶é—´æˆ³
            start_time = start_frame / fps
            end_time = end_frame / fps
            
            segments.append({
                'key_frame': start_frame,      # ğŸ†• å…³é”®å¸§ä¿¡æ¯
                'start_frame': start_frame,
                'end_frame': end_frame, 
                'start_time': start_time,
                'end_time': end_time,
                'duration': end_time - start_time
            })
        
        print(f"âœ… ç”Ÿæˆäº† {len(segments)} ä¸ªå­—å¹•æ®µè½")
        return segments

    def detect(self, video_path: str, decoder, subtitle_area: Tuple[int, int, int], **kwargs) -> List[int]:
        """
        å®ç°åŸºç±»çš„æŠ½è±¡æ£€æµ‹æ–¹æ³•

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            subtitle_area: å­—å¹•åŒºåŸŸåæ ‡
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
        """
        self._start_processing()
        try:
            keyframes = self.detect_keyframes(video_path, decoder, subtitle_area)
            return keyframes
        finally:
            self._finish_processing()

    def get_detector_name(self) -> str:
        """
        è·å–æ£€æµ‹å™¨åç§°

        Returns:
            æ£€æµ‹å™¨åç§°
        """
        return "KeyFrameDetector"
