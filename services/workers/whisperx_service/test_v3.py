#!/usr/bin/env python3
"""
faster-whisper-large-v2 è°ƒä¼˜ç‰ˆæµ‹è¯•è„šæœ¬
ç”¨æ³•ï¼špython test_fw_v2_optim.py 111.wav
"""
import os
import sys
import time
import logging
import torch
from faster_whisper import WhisperModel

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# -------------------------------------------------
# å¯è°ƒå¸¸é‡
# -------------------------------------------------
MODEL_NAME = "Systran/faster-whisper-large-v3"   # çœŸæ­£è¦æµ‹çš„æ¨¡å‹
COMPUTE_TYPE = "float16"                         # é€Ÿåº¦/æ˜¾å­˜ç”œç‚¹
BEAM_SIZE = 3                                    # å®˜æ–¹é»˜è®¤ 5ï¼Œ3 æ›´å¹³è¡¡
HOTWORDS = ["ç‹æ€èª", "ç‹å¥æ—", "æ”¿æ³•å­¦é™¢", "å›½æ°‘è€å…¬"]  # ä¸­æ–‡ä¸“å
AUDIO_PATH = sys.argv[1] if len(sys.argv) > 1 else "/app/services/workers/whisperx_service/111.wav"

# -------------------------------------------------
# æ ¸å¿ƒè½¬å½•å‡½æ•°
# -------------------------------------------------
def transcribe(audio_path: str):
    """è¿”å› (results, info, load_time, transcribe_time)"""
    logger.info("=== æµ‹è¯• %s ===", MODEL_NAME)

    # 1. åŠ è½½æ¨¡å‹
    t0 = time.time()
    model = WhisperModel(
        MODEL_NAME,
        device="cuda",
        compute_type=COMPUTE_TYPE
    )
    load_time = time.time() - t0
    logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ - è€—æ—¶: %.3fs", load_time)

    # 2. è½¬å½•ï¼ˆè°ƒä¼˜å‚æ•°å…¨éƒ¨ç»™å‡ºï¼‰
    t0 = time.time()
    segments, info = model.transcribe(
        audio_path,
        beam_size=BEAM_SIZE,
        best_of=BEAM_SIZE,
        temperature=(0.0, 0.2, 0.4, 0.6),
        condition_on_previous_text=False,      # æŠ‘åˆ¶å¾ªç¯å¹»è§‰
        compression_ratio_threshold=2.0,
        # logprob_threshold=-1.0,
        no_speech_threshold=0.5,
        # hotwords=HOTWORDS,
        word_timestamps=True
    )

    results = []
    for idx, seg in enumerate(segments, 1):
        logger.info("   ç‰‡æ®µ%d: [%.2fs-%.2fs] %s",
                    idx, seg.start, seg.end, seg.text.strip())
        results.append({"start": seg.start, "end": seg.end, "text": seg.text.strip()})

    transcribe_time = time.time() - t0
    logger.info("âœ… è½¬å½•å®Œæˆ - è€—æ—¶: %.3fs", transcribe_time)
    logger.info("ğŸ“Š æ£€æµ‹è¯­è¨€: %s (æ¦‚ç‡: %.2f)", info.language, info.language_probability)
    logger.info("ğŸ“Š å¤„ç†ç‰‡æ®µ: %d ä¸ª", len(results))
    if results:
        logger.info("ğŸ“Š éŸ³é¢‘æ€»æ—¶é•¿: %.2fs", results[-1]["end"])

    # 3. æ¸…ç†
    del model
    torch.cuda.empty_cache()

    return results, info, load_time, transcribe_time

# -------------------------------------------------
# ç»Ÿä¸€æ€»ç»“æ¨¡æ¿
# -------------------------------------------------
def print_summary(success: bool, load_t: float, trans_t: float):
    total = load_t + trans_t
    logger.info("=" * 60)
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info("=" * 60)
    logger.info("æ€»è€—æ—¶: %.3fs", total)
    if success:
        logger.info("ğŸ‰ %s æµ‹è¯•æˆåŠŸ", MODEL_NAME)
        logger.info("")
        logger.info("ğŸ’¡ æœ¬æ¬¡å·²å¯ç”¨è°ƒä¼˜å‚æ•°ï¼š")
        logger.info("   - beam_size=%d + best_of=%d", BEAM_SIZE, BEAM_SIZE)
        logger.info("   - condition_on_previous_text=False")
        logger.info("   - hotwords ä¸­æ–‡ä¸“å")
        logger.info("   - float16 æ¨ç†")
    else:
        logger.error("âŒ %s æµ‹è¯•å¤±è´¥", MODEL_NAME)

# -------------------------------------------------
# CLI å…¥å£
# -------------------------------------------------
def main():
    if not os.path.exists(AUDIO_PATH):
        logger.error("âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: %s", AUDIO_PATH)
        sys.exit(1)

    try:
        _, _, load_t, trans_t = transcribe(AUDIO_PATH)
        print_summary(True, load_t, trans_t)
    except Exception as e:
        logger.exception("æµ‹è¯•å¼‚å¸¸: %s", e)
        print_summary(False, 0, 0)
        sys.exit(2)

if __name__ == "__main__":
    main()