# ============================================================
# 模块化配置 - 标量锚点版本 (可选方案)
# ============================================================
#
# 此文件展示如何使用标量锚点实现 volumes 的集中管理
# 优势: 路径修改只需改一处,避免拼写错误
# 劣势: 配置行数不减少,可读性略降
#
# 使用方法:
#   cp docker-compose.volumes-anchors.yml docker-compose.yml
#   docker compose config --quiet  # 验证
# ============================================================

# --- 环境变量分组 ---
x-base-env: &base-env
    PYTHONPATH: /app
    REDIS_HOST: ${REDIS_HOST}
    REDIS_PORT: ${REDIS_PORT}
    MINIO_HOST: ${MINIO_HOST}
    MINIO_PORT: ${MINIO_PORT}
    MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
    MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    MINIO_DEFAULT_BUCKET: ${MINIO_DEFAULT_BUCKET:-yivideo}

x-gpu-env: &gpu-env
    NVIDIA_VISIBLE_DEVICES: all
    CUDA_VISIBLE_DEVICES: '0'

x-cache-env: &cache-env
    HF_HOME: /app/.cache/huggingface
    HF_HUB_ENABLE_HF_TRANSFER: '0'
    TRANSFORMERS_CACHE: /root/.cache/transformers

x-ai-env: &ai-env
    GEMINI_API_KEY: ${GEMINI_API_KEY}
    DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY}
    ZHIPU_API_KEY: ${ZHIPU_API_KEY}
    VOLCENGINE_API_KEY: ${VOLCENGINE_API_KEY}
    OPENAI_COMPATIBLE_API_KEY: ${OPENAI_COMPATIBLE_API_KEY}

# --- 通用配置块 ---
x-common-config: &common-config
    user: '0:0'
    restart: always
    networks:
        - wionch

# --- Volumes 标量锚点 (集中管理路径) ---
x-volume-paths:
    services: &vol-services '${YIVIDEO_ROOT}/services:/app/services'
    videos: &vol-videos '${YIVIDEO_ROOT}/videos:/app/videos'
    locks: &vol-locks '${YIVIDEO_ROOT}/locks:/app/locks'
    tmp: &vol-tmp '${YIVIDEO_ROOT}/tmp:/app/tmp'
    worktrees: &vol-worktrees '${YIVIDEO_ROOT}/.worktrees:/app/.worktrees'
    tests: &vol-tests '${YIVIDEO_ROOT}/tests:/app/tests'
    share: &vol-share '${YIVIDEO_ROOT}/share:/share'
    config-yml: &vol-config-yml '${YIVIDEO_ROOT}/config.yml:/app/config.yml'
    config-dir: &vol-config-dir '${YIVIDEO_ROOT}/config:/app/config'
    ssh: &vol-ssh '~/.ssh:/root/.ssh'
    gemini: &vol-gemini '~/.gemini:/root/.gemini'

# --- 缓存卷锚点 ---
x-cache-volumes:
    huggingface: &vol-hf-cache 'huggingface_cache_volume:/app/.cache'
    transformers: &vol-tf-cache 'transformers_cache_volume:/root/.cache'

# --- GPU 部署配置锚点 ---
x-gpu-deploy: &gpu-deploy
    resources:
        reservations:
            devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]

# --- Celery Worker 基础配置锚点 ---
x-celery-worker-base: &celery-worker-base
    <<: *common-config
    entrypoint: ''

# --- Build 基础配置锚点 ---
x-build-base: &build-base
    context: .

# ============================================================
# 服务定义
# ============================================================

services:
    api_gateway:
        <<: *common-config
        container_name: api_gateway
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/api_gateway/Dockerfile
        ports:
            - '8788:80'
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-share
            - *vol-tests
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
        environment:
            <<: *base-env

    ffmpeg_service:
        <<: *celery-worker-base
        container_name: ffmpeg_service
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/ffmpeg_service/Dockerfile
        command: ['celery', '-A', 'app.tasks.celery_app', 'worker', '-l', 'info', '-Q', 'ffmpeg_queue']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
        environment:
            <<: *base-env
            NVIDIA_VISIBLE_DEVICES: all
        deploy:
            <<: *gpu-deploy

    paddleocr_service:
        <<: *celery-worker-base
        container_name: paddleocr_service
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/paddleocr_service/Dockerfile
        command: ['/opt/venv/bin/celery', '-A', 'app.tasks.celery_app', 'worker', '-l', 'info', '-Q', 'paddleocr_queue']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
            - paddle_models_volume:/root/.paddlex/official_models
        environment:
            <<: *base-env
            NVIDIA_VISIBLE_DEVICES: all
        deploy:
            <<: *gpu-deploy

    faster_whisper_service:
        <<: *celery-worker-base
        container_name: faster_whisper_service
        runtime: nvidia
        restart: on-failure
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/faster_whisper_service/Dockerfile
        command: ['celery', '-A', 'services.workers.faster_whisper_service.app.celery_app', 'worker', '-l', 'info', '-Q', 'faster_whisper_queue']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
            - *vol-hf-cache
            - *vol-tf-cache
        environment:
            <<: [*base-env, *gpu-env, *cache-env, *ai-env]
            HF_TOKEN: ${HF_TOKEN}
        deploy:
            <<: *gpu-deploy

    pyannote_audio_service:
        <<: *celery-worker-base
        container_name: pyannote_audio_service
        runtime: nvidia
        restart: unless-stopped
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/pyannote_audio_service/Dockerfile
        command: ['celery', '-A', 'services.workers.pyannote_audio_service.app.celery_app', 'worker', '-l', 'info', '-Q', 'pyannote_audio_queue']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
            - *vol-hf-cache
            - *vol-tf-cache
        environment:
            <<: [*base-env, *gpu-env, *cache-env]
            HF_TOKEN: ${HF_TOKEN}
            PYANNOTEAI_API_KEY: ${PYANNOTEAI_API_KEY}
            PYANNOTE_SKIP_DEPENDENCY_CHECK: '0'
            TORCH_CUDA_ARCH_LIST: '8.6'
            CUDA_LAUNCH_BLOCKING: '1'
        deploy:
            <<: *gpu-deploy

    audio_separator_service:
        <<: *celery-worker-base
        container_name: audio_separator_service
        runtime: nvidia
        restart: on-failure
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/audio_separator_service/Dockerfile
        command: ['celery', '-A', 'services.workers.audio_separator_service.app.celery_app', 'worker', '-l', 'info', '-Q', 'audio_separator_queue']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - audio_separator_models_volume:/models
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
            - *vol-hf-cache
            - *vol-tf-cache
        environment:
            <<: [*base-env, *gpu-env]
        deploy:
            <<: *gpu-deploy

    qwen3_asr_service:
        <<: *celery-worker-base
        container_name: qwen3_asr_service
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/qwen3_asr_service/Dockerfile
        command: ['celery', '-A', 'app.celery_app', 'worker', '-l', 'info', '-Q', 'qwen3_asr_queue', '--concurrency=1', '-n', 'qwen3_asr_worker@%h']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
            - *vol-hf-cache
            - *vol-tf-cache
        environment:
            <<: [*base-env, *gpu-env, *cache-env]
            CELERY_WORKER_NAME: qwen3_asr_worker
            HF_TOKEN: ${HF_TOKEN}
        deploy:
            <<: *gpu-deploy

    wservice:
        <<: *celery-worker-base
        container_name: wservice
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/wservice/Dockerfile
        command: ['celery', '-A', 'services.workers.wservice.app.celery_app', 'worker', '-l', 'info', '-Q', 'wservice_queue']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
        environment:
            <<: [*base-env, *ai-env]

    indextts_service:
        <<: *celery-worker-base
        container_name: indextts_service
        runtime: nvidia
        build:
            <<: *build-base
            dockerfile: ${YIVIDEO_ROOT}/services/workers/indextts_service/Dockerfile
        ports:
            - '7860:7860'
        command: ['celery', '-A', 'services.workers.indextts_service.app.celery_app', 'worker', '-l', 'info', '-Q', 'indextts_queue', '--concurrency=1']
        volumes:
            - *vol-services
            - *vol-videos
            - *vol-locks
            - *vol-tmp
            - *vol-worktrees
            - *vol-tests
            - *vol-share
            - *vol-config-yml
            - *vol-config-dir
            - *vol-ssh
            - *vol-gemini
            - indextts_models_volume:/models/indextts
            - huggingface_cache_volume:/app/.cache/huggingface
            - transformers_cache_volume:/app/.cache/transformers
            - indextts_cache_volume:/app/.cache/indextts
            - torch_cache_volume:/app/.cache/torch
        environment:
            <<: [*base-env, *gpu-env, *cache-env]
            INDEX_TTS_MODEL_DIR: /models/indextts
            INDEX_TTS_CHECKPOINTS_DIR: /models/indextts/checkpoints
            TORCH_HOME: /app/.cache/torch
            INDEX_TTS_USE_FP16: 'true'
            INDEX_TTS_USE_DEEPSPEED: 'false'
            INDEX_TTS_USE_CUDA_KERNEL: 'false'
        deploy:
            <<: *gpu-deploy
            resources:
                reservations:
                    memory: 4G
                limits:
                    memory: 8G
        healthcheck:
            test: ['CMD', 'python', '-c', "import torch; print('GPU Available:' if torch.cuda.is_available() else 'GPU Not Available')"]
            interval: 60s
            timeout: 30s
            retries: 3
            start_period: 180s

volumes:
    npm_cache: {}
    paddle_models_volume: {}
    huggingface_cache_volume: {}
    transformers_cache_volume: {}
    audio_separator_models_volume: {}
    indextts_models_volume: {}
    indextts_cache_volume: {}
    torch_cache_volume: {}
    serena_cache_volume: {}

networks:
    wionch:
        external: true
