# services/workers/common/decoders/base_decoder.py
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€è§£ç å™¨åŸºç±»

ä¸ºæ‰€æœ‰è§£ç å™¨æä¾›é€šç”¨åŠŸèƒ½å’Œæ¥å£å®šä¹‰
"""

import gc
from abc import ABC, abstractmethod
from typing import Dict, Any, Generator, Tuple, Optional, List
import torch
import numpy as np

from services.common.logger import get_logger

logger = get_logger('base_decoder')


class BaseDecoder(ABC):
    """
    ç»Ÿä¸€è§£ç å™¨æŠ½è±¡åŸºç±»

    æä¾›æ‰€æœ‰è§£ç å™¨çš„é€šç”¨åŠŸèƒ½ï¼š
    - ç»Ÿä¸€çš„åˆå§‹åŒ–å’Œé…ç½®ç®¡ç†
    - GPUèµ„æºç®¡ç†
    - å†…å­˜ä¼˜åŒ–
    - è¿›åº¦æ˜¾ç¤º
    - é”™è¯¯å¤„ç†
    """

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åŸºç¡€è§£ç å™¨

        Args:
            config: è§£ç å™¨é…ç½®å­—å…¸
        """
        self.config = config
        self.device = self._get_device()
        self._validate_common_config()

        # é€šç”¨é…ç½®éªŒè¯å’Œè®¾ç½®
        self.batch_size = config.get('batch_size', 32)
        self.frame_memory_estimate_mb = config.get('frame_memory_estimate_mb', 0.307)
        self.progress_interval_frames = config.get('progress_interval_frames', 1000)
        self.progress_interval_batches = config.get('progress_interval_batches', 50)

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'frames_processed': 0,
            'batches_processed': 0,
            'gpu_memory_errors': 0,
            'start_time': None,
            'total_frames': 0,
            'decoding_errors': 0
        }

        logger.info(f"{self.__class__.__name__} åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")

    def _get_device(self) -> str:
        """è·å–è®¡ç®—è®¾å¤‡"""
        return 'cuda' if torch.cuda.is_available() else 'cpu'

    def _validate_common_config(self):
        """éªŒè¯é€šç”¨é…ç½®å‚æ•°"""
        # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
        required_configs = []
        for config_key in required_configs:
            if config_key not in self.config:
                raise ValueError(f"ç¼ºå°‘å¿…è¦é…ç½®é¡¹: {config_key}")

    @abstractmethod
    def decode(self, video_path: str, **kwargs) -> Generator[Tuple[torch.Tensor, Dict], None, None]:
        """
        æŠ½è±¡è§£ç æ–¹æ³•

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            (å¸§å¼ é‡, å…ƒæ•°æ®å­—å…¸) çš„å…ƒç»„
        """
        pass

    @abstractmethod
    def get_video_info(self, video_path: str) -> Dict[str, Any]:
        """
        è·å–è§†é¢‘ä¿¡æ¯

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            è§†é¢‘ä¿¡æ¯å­—å…¸
        """
        pass

    @abstractmethod
    def get_capabilities(self) -> Dict[str, bool]:
        """
        è·å–è§£ç å™¨èƒ½åŠ›

        Returns:
            èƒ½åŠ›å­—å…¸
        """
        pass

    def _log_progress(self, current: int, total: int, stage: str = "å¤„ç†"):
        """
        è®°å½•è¿›åº¦ä¿¡æ¯

        Args:
            current: å½“å‰è¿›åº¦
            total: æ€»æ•°
            stage: å¤„ç†é˜¶æ®µæè¿°
        """
        if current % self.progress_interval_frames == 0:
            progress = (current / total) * 100
            logger.info(f"ğŸ“Š {stage}è¿›åº¦: {current}/{total} ({progress:.1f}%)")

    def _log_batch_progress(self, batch_count: int, frame_count: int, additional_info: str = ""):
        """
        è®°å½•æ‰¹æ¬¡å¤„ç†è¿›åº¦

        Args:
            batch_count: æ‰¹æ¬¡æ•°é‡
            frame_count: å¸§æ•°é‡
            additional_info: é™„åŠ ä¿¡æ¯
        """
        if batch_count % self.progress_interval_batches == 0:
            info = f"æ‰¹æ¬¡: {batch_count}, å¸§: {frame_count}"
            if additional_info:
                info += f", {additional_info}"
            logger.info(f"  ğŸ“Š å·²å¤„ç† {info}")

    def _optimize_gpu_memory(self):
        """ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

    def _safe_gpu_operation(self, operation_func, *args, **kwargs):
        """
        å®‰å…¨æ‰§è¡ŒGPUæ“ä½œï¼Œå¤„ç†å†…å­˜ä¸è¶³å¼‚å¸¸

        Args:
            operation_func: è¦æ‰§è¡Œçš„æ“ä½œå‡½æ•°
            *args, **kwargs: æ“ä½œå‡½æ•°çš„å‚æ•°

        Returns:
            æ“ä½œç»“æœæˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        try:
            return operation_func(*args, **kwargs)
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                self.stats['gpu_memory_errors'] += 1
                logger.warning(f"âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ“ä½œ: {e}")
                self._optimize_gpu_memory()
                return None
            else:
                raise e

    def _estimate_memory_usage(self, frame_count: int) -> float:
        """
        ä¼°ç®—å†…å­˜ä½¿ç”¨é‡

        Args:
            frame_count: å¸§æ•°é‡

        Returns:
            ä¼°ç®—çš„å†…å­˜ä½¿ç”¨é‡(MB)
        """
        return frame_count * self.frame_memory_estimate_mb

    def _start_processing(self):
        """å¼€å§‹å¤„ç†å‰çš„å‡†å¤‡å·¥ä½œ"""
        import time
        self.stats['start_time'] = time.time()
        logger.info(f"ğŸš€ å¼€å§‹ {self.__class__.__name__} å¤„ç†")

    def _finish_processing(self):
        """å®Œæˆå¤„ç†åçš„æ¸…ç†å·¥ä½œ"""
        import time
        if self.stats['start_time']:
            duration = time.time() - self.stats['start_time']
            logger.info(f"âœ… {self.__class__.__name__} å¤„ç†å®Œæˆ")
            logger.info(f"â±ï¸  å¤„ç†æ—¶é—´: {duration:.2f}ç§’")
            logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: å¸§={self.stats['frames_processed']}, "
                       f"æ‰¹æ¬¡={self.stats['batches_processed']}, "
                       f"GPUé”™è¯¯={self.stats['gpu_memory_errors']}, "
                       f"è§£ç é”™è¯¯={self.stats['decoding_errors']}")

        # æœ€ç»ˆå†…å­˜ä¼˜åŒ–
        self._optimize_gpu_memory()

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'frames_processed': 0,
            'batches_processed': 0,
            'gpu_memory_errors': 0,
            'start_time': None,
            'total_frames': 0,
            'decoding_errors': 0
        }

    def get_decoder_name(self) -> str:
        """
        è·å–è§£ç å™¨åç§°

        Returns:
            è§£ç å™¨åç§°
        """
        return self.__class__.__name__

    def validate_video_file(self, video_path: str) -> bool:
        """
        éªŒè¯è§†é¢‘æ–‡ä»¶

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        import os
        if not os.path.exists(video_path):
            logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return False

        if not os.path.isfile(video_path):
            logger.error(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {video_path}")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(video_path)
        if file_size == 0:
            logger.error(f"è§†é¢‘æ–‡ä»¶ä¸ºç©º: {video_path}")
            return False

        return True

    def process_batch(self, batch_data: Any) -> torch.Tensor:
        """
        å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼ˆå­ç±»å¯é‡å†™æ­¤æ–¹æ³•ï¼‰

        Args:
            batch_data: æ‰¹æ¬¡æ•°æ®

        Returns:
            å¤„ç†åçš„å¼ é‡
        """
        # é»˜è®¤å®ç°ï¼šç›´æ¥è¿”å›æ•°æ®
        return batch_data

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºé‡Šæ”¾"""
        try:
            self._optimize_gpu_memory()
        except:
            pass


class DecoderCapability:
    """è§£ç å™¨èƒ½åŠ›å¸¸é‡"""
    GPU_ACCELERATION = "gpu_acceleration"
    CONCURRENT_PROCESSING = "concurrent_processing"
    BATCH_PROCESSING = "batch_processing"
    MEMORY_OPTIMIZATION = "memory_optimization"
    HARDWARE_DECODING = "hardware_decoding"
    MULTI_FORMAT_SUPPORT = "multi_format_support"
    SEEK_SUPPORT = "seek_support"
    METADATA_EXTRACTION = "metadata_extraction"