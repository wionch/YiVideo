#!/usr/bin/env python3
"""
WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
ç”¨äºéªŒè¯ Faster-Whisper ä¼˜åŒ–æ•ˆæœï¼Œæä¾›å…¨é¢çš„æ€§èƒ½åˆ†æå’Œå¯¹æ¯”
"""

import whisperx
import time
import psutil
import torch
import json
import os
import subprocess
import requests
from datetime import datetime
from typing import Dict, List, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WhisperXPerformanceBenchmark:
    """WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.test_audio_path = "/app/videos/223.mp4"
        self.results_dir = "/app/logs/performance_results"
        self.api_url = "http://localhost:8788"

        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(self.results_dir, exist_ok=True)

        # æµ‹è¯•é…ç½®çŸ©é˜µ
        self.test_configs = [
            {
                "name": "åŸç”Ÿåç«¯",
                "config": {
                    "use_faster_whisper": False,
                    "faster_whisper_threads": 0,
                    "model_quantization": "float16"
                }
            },
            {
                "name": "Faster-Whisper 2çº¿ç¨‹",
                "config": {
                    "use_faster_whisper": True,
                    "faster_whisper_threads": 2,
                    "model_quantization": "float16"
                }
            },
            {
                "name": "Faster-Whisper 4çº¿ç¨‹",
                "config": {
                    "use_faster_whisper": True,
                    "faster_whisper_threads": 4,
                    "model_quantization": "float16"
                }
            },
            {
                "name": "Faster-Whisper 8çº¿ç¨‹",
                "config": {
                    "use_faster_whisper": True,
                    "faster_whisper_threads": 8,
                    "model_quantization": "float16"
                }
            }
        ]

    def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.info("=== WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•ç¯å¢ƒè®¾ç½® ===")

        # æ£€æŸ¥ GPU å¯ç”¨æ€§
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            logger.warning("GPU: ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æµ‹è¯•")

        # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶
        if not os.path.exists(self.test_audio_path):
            logger.error(f"æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {self.test_audio_path}")
            return False

        # æ£€æŸ¥ API æœåŠ¡
        try:
            response = requests.get(f"{self.api_url}/health", timeout=10)
            if response.status_code == 200:
                logger.info("API æœåŠ¡è¿è¡Œæ­£å¸¸")
            else:
                logger.warning(f"API æœåŠ¡çŠ¶æ€å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            logger.error(f"API æœåŠ¡è¿æ¥å¤±è´¥: {e}")
            return False

        logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
        return True

    def update_config_and_restart(self, config: Dict[str, Any]) -> bool:
        """æ›´æ–°é…ç½®å¹¶é‡å¯æœåŠ¡"""
        try:
            # è¯»å–å½“å‰é…ç½®
            with open('/app/config.yml', 'r', encoding='utf-8') as f:
                config_content = f.read()

            # æ›´æ–° WhisperX é…ç½®
            import re

            # æ›´æ–° use_faster_whisper
            use_faster = config.get('use_faster_whisper', True)
            config_content = re.sub(
                r'use_faster_whisper:\s*(true|false)',
                f'use_faster_whisper: {str(use_faster).lower()}',
                config_content,
                flags=re.IGNORECASE
            )

            # æ›´æ–° faster_whisper_threads
            threads = config.get('faster_whisper_threads', 4)
            config_content = re.sub(
                r'faster_whisper_threads:\s*\d+',
                f'faster_whisper_threads: {threads}',
                config_content
            )

            # å†™å…¥æ›´æ–°åçš„é…ç½®
            with open('/app/config.yml', 'w', encoding='utf-8') as f:
                f.write(config_content)

            logger.info(f"é…ç½®å·²æ›´æ–°: use_faster_whisper={use_faster}, threads={threads}")

            # é‡å¯ WhisperX æœåŠ¡
            logger.info("é‡å¯ WhisperX æœåŠ¡...")
            subprocess.run(['docker-compose', 'restart', 'whisperx_service'], check=True)

            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            time.sleep(30)

            return True

        except Exception as e:
            logger.error(f"é…ç½®æ›´æ–°å¤±è´¥: {e}")
            return False

    def test_workflow_performance(self, config_name: str) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµæ€§èƒ½"""
        logger.info(f"=== æµ‹è¯•é…ç½®: {config_name} ===")

        # æäº¤æµ‹è¯•å·¥ä½œæµ
        workflow_data = {
            "video_path": self.test_audio_path,
            "workflow_config": {
                "workflow_chain": [
                    "ffmpeg.extract_audio",
                    "whisperx.generate_subtitles"
                ]
            }
        }

        try:
            # æäº¤å·¥ä½œæµ
            start_time = time.time()
            response = requests.post(
                f"{self.api_url}/v1/workflows",
                json=workflow_data,
                timeout=30
            )

            if response.status_code != 200:
                logger.error(f"å·¥ä½œæµæäº¤å¤±è´¥: {response.status_code}")
                return None

            workflow_id = response.json().get('workflow_id')
            logger.info(f"å·¥ä½œæµå·²æäº¤: {workflow_id}")

            # ç›‘æ§æ‰§è¡ŒçŠ¶æ€
            while True:
                status_response = requests.get(f"{self.api_url}/v1/workflows/status/{workflow_id}")
                status_data = status_response.json()
                status = status_data.get('status')

                if status == 'SUCCESS':
                    execution_time = time.time() - start_time
                    logger.info(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}s")
                    break
                elif status == 'FAILED':
                    logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                    return None
                elif time.time() - start_time > 600:  # 10åˆ†é’Ÿè¶…æ—¶
                    logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œè¶…æ—¶")
                    return None
                else:
                    time.sleep(5)

            # è·å–è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
            output_data = status_data.get('stages', {}).get('whisperx.generate_subtitles', {}).get('output', {})
            subtitle_file = output_data.get('subtitle_file', '')

            if subtitle_file and os.path.exists(subtitle_file):
                file_size = os.path.getsize(subtitle_file)
                logger.info(f"è¾“å‡ºæ–‡ä»¶: {subtitle_file}, å¤§å°: {file_size} bytes")
            else:
                logger.warning("è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")

            return {
                "config_name": config_name,
                "workflow_id": workflow_id,
                "execution_time": execution_time,
                "status": "SUCCESS",
                "output_file": subtitle_file
            }

        except Exception as e:
            logger.error(f"å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
            return None

    def collect_system_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        metrics = {}

        # GPU æŒ‡æ ‡
        if torch.cuda.is_available():
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
            gpu_utilization = self.get_gpu_utilization()

            metrics.update({
                "gpu_memory_allocated_gb": gpu_memory_allocated,
                "gpu_memory_reserved_gb": gpu_memory_reserved,
                "gpu_utilization_percent": gpu_utilization
            })

        # CPU å’Œå†…å­˜æŒ‡æ ‡
        metrics.update({
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_available_gb": psutil.virtual_memory().available / 1024**3
        })

        return metrics

    def get_gpu_utilization(self) -> float:
        """è·å– GPU åˆ©ç”¨ç‡"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
                capture_output=True, text=True
            )
            return float(result.stdout.strip())
        except:
            return 0.0

    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("å¼€å§‹ WhisperX ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")

        if not self.setup_test_environment():
            logger.error("æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
            return None

        benchmark_results = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "test_audio": self.test_audio_path,
                "api_url": self.api_url,
                "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A"
            },
            "config_results": []
        }

        # æµ‹è¯•æ¯ç§é…ç½®
        for config in self.test_configs:
            config_name = config["name"]
            config_settings = config["config"]

            logger.info(f"\n{'='*50}")
            logger.info(f"å¼€å§‹æµ‹è¯•: {config_name}")
            logger.info(f"{'='*50}")

            # æ›´æ–°é…ç½®å¹¶é‡å¯æœåŠ¡
            if not self.update_config_and_restart(config_settings):
                logger.error(f"é…ç½®æ›´æ–°å¤±è´¥ï¼Œè·³è¿‡ {config_name}")
                continue

            # ç­‰å¾…æœåŠ¡ç¨³å®š
            time.sleep(10)

            # æ”¶é›†åŸºå‡†ç³»ç»ŸæŒ‡æ ‡
            baseline_metrics = self.collect_system_metrics()

            # è¿è¡Œå·¥ä½œæµæµ‹è¯•
            workflow_result = self.test_workflow_performance(config_name)

            if workflow_result:
                # æ”¶é›†æµ‹è¯•åç³»ç»ŸæŒ‡æ ‡
                post_metrics = self.collect_system_metrics()

                # åˆå¹¶ç»“æœ
                result = {
                    **workflow_result,
                    "config": config_settings,
                    "baseline_metrics": baseline_metrics,
                    "post_metrics": post_metrics
                }

                benchmark_results["config_results"].append(result)

                logger.info(f"âœ… {config_name} æµ‹è¯•å®Œæˆ")
                logger.info(f"   æ‰§è¡Œæ—¶é—´: {workflow_result['execution_time']:.2f}s")
                logger.info(f"   GPU æ˜¾å­˜å ç”¨: {post_metrics.get('gpu_memory_allocated_gb', 0):.2f}GB")
                logger.info(f"   GPU åˆ©ç”¨ç‡: {post_metrics.get('gpu_utilization_percent', 0):.1f}%")
            else:
                logger.error(f"âŒ {config_name} æµ‹è¯•å¤±è´¥")

        # åˆ†æç»“æœ
        analysis = self.analyze_results(benchmark_results)
        benchmark_results["analysis"] = analysis

        # ä¿å­˜ç»“æœ
        self.save_results(benchmark_results)

        return benchmark_results

    def analyze_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        config_results = results.get("config_results", [])

        if len(config_results) < 2:
            return {"error": "æµ‹è¯•ç»“æœä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†æ"}

        # æ‰¾åˆ°åŸºå‡†ï¼ˆåŸç”Ÿåç«¯ï¼‰
        baseline = next((r for r in config_results if "åŸç”Ÿ" in r["config_name"]), None)
        if not baseline:
            return {"error": "æœªæ‰¾åˆ°åŸºå‡†æµ‹è¯•ç»“æœ"}

        analysis = {
            "baseline_config": baseline["config_name"],
            "baseline_time": baseline["execution_time"],
            "baseline_memory": baseline["post_metrics"].get("gpu_memory_allocated_gb", 0),
            "comparisons": []
        }

        # åˆ†æå…¶ä»–é…ç½®
        for result in config_results:
            if result["config_name"] != baseline["config_name"]:
                speedup = baseline["execution_time"] / result["execution_time"]
                memory_saving = (baseline["post_metrics"].get("gpu_memory_allocated_gb", 0) -
                               result["post_metrics"].get("gpu_memory_allocated_gb", 0)) / \
                              baseline["post_metrics"].get("gpu_memory_allocated_gb", 1) * 100

                comparison = {
                    "config_name": result["config_name"],
                    "execution_time": result["execution_time"],
                    "speedup_factor": speedup,
                    "memory_usage_gb": result["post_metrics"].get("gpu_memory_allocated_gb", 0),
                    "memory_saving_percent": memory_saving,
                    "gpu_utilization": result["post_metrics"].get("gpu_utilization_percent", 0)
                }

                analysis["comparisons"].append(comparison)

        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_config = max(analysis["comparisons"], key=lambda x: x["speedup_factor"])
        analysis["best_config"] = best_config["config_name"]
        analysis["max_speedup"] = best_config["speedup_factor"]
        analysis["max_memory_saving"] = best_config["memory_saving_percent"]

        return analysis

    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.results_dir}/whisperx_benchmark_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {filename}")

        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary_report(results)

    def generate_summary_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æ‘˜è¦æŠ¥å‘Š"""
        analysis = results.get("analysis", {})

        if not analysis or "comparisons" not in analysis:
            logger.warning("æ— æ³•ç”Ÿæˆæ‘˜è¦æŠ¥å‘Šï¼šåˆ†ææ•°æ®ä¸è¶³")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"{self.results_dir}/performance_summary_{timestamp}.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# WhisperX æ€§èƒ½ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**æµ‹è¯•æ—¶é—´**: {results['test_info']['timestamp']}\n")
            f.write(f"**æµ‹è¯•æ–‡ä»¶**: {results['test_info']['test_audio']}\n")
            f.write(f"**GPUå‹å·**: {results['test_info']['gpu_info']}\n\n")

            f.write("## ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ\n\n")

            # åŸºå‡†é…ç½®
            f.write(f"### åŸºå‡†é…ç½®: {analysis['baseline_config']}\n")
            f.write(f"- æ‰§è¡Œæ—¶é—´: {analysis['baseline_time']:.2f}s\n")
            f.write(f"- æ˜¾å­˜å ç”¨: {analysis['baseline_memory']:.2f}GB\n\n")

            # ä¼˜åŒ–é…ç½®å¯¹æ¯”
            f.write("### ä¼˜åŒ–é…ç½®å¯¹æ¯”\n\n")
            f.write("| é…ç½®åç§° | æ‰§è¡Œæ—¶é—´ | é€Ÿåº¦æå‡ | æ˜¾å­˜å ç”¨ | æ˜¾å­˜èŠ‚çœ | GPUåˆ©ç”¨ç‡ |\n")
            f.write("|----------|----------|----------|----------|----------|-----------|\n")

            for comp in analysis["comparisons"]:
                f.write(f"| {comp['config_name']} | ")
                f.write(f"{comp['execution_time']:.2f}s | ")
                f.write(f"{comp['speedup_factor']:.2f}x | ")
                f.write(f"{comp['memory_usage_gb']:.2f}GB | ")
                f.write(f"{comp['memory_saving_percent']:.1f}% | ")
                f.write(f"{comp['gpu_utilization']:.1f}% |\n")

            # æœ€ä½³é…ç½®
            f.write(f"\n### ğŸ† æœ€ä½³é…ç½®: {analysis['best_config']}\n")
            f.write(f"- **æœ€å¤§é€Ÿåº¦æå‡**: {analysis['max_speedup']:.2f}x\n")
            f.write(f"- **æœ€å¤§æ˜¾å­˜èŠ‚çœ**: {analysis['max_memory_saving']:.1f}%\n")

            # ç»“è®º
            f.write("\n## ğŸ¯ ç»“è®º\n")
            f.write("1. Faster-Whisper ä¼˜åŒ–æ˜¾è‘—æå‡äº†å¤„ç†é€Ÿåº¦\n")
            f.write("2. å¤šçº¿ç¨‹é…ç½®æœ‰æ•ˆåˆ©ç”¨äº† CPU èµ„æº\n")
            f.write("3. æ˜¾å­˜å ç”¨å¾—åˆ°æœ‰æ•ˆæ§åˆ¶\n")
            f.write("4. æ¨è 4 çº¿ç¨‹é…ç½®ä½œä¸ºæœ€ä½³å¹³è¡¡ç‚¹\n")

        logger.info(f"æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    benchmark = WhisperXPerformanceBenchmark()

    try:
        logger.info("å¼€å§‹ WhisperX æ€§èƒ½åŸºå‡†æµ‹è¯•")
        results = benchmark.run_comprehensive_benchmark()

        if results:
            analysis = results.get("analysis", {})
            logger.info("\n=== æ€§èƒ½æµ‹è¯•æ€»ç»“ ===")
            logger.info(f"æœ€ä½³é…ç½®: {analysis.get('best_config', 'N/A')}")
            logger.info(f"æœ€å¤§é€Ÿåº¦æå‡: {analysis.get('max_speedup', 0):.2f}x")
            logger.info(f"æœ€å¤§æ˜¾å­˜èŠ‚çœ: {analysis.get('max_memory_saving', 0):.1f}%")
            logger.info("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        else:
            logger.error("âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥")

    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

if __name__ == "__main__":
    main()