# app/modules/keyframe_detector.py
import torch
import numpy as np
import cv2
from typing import List, Tuple, Dict
from .decoder import GPUDecoder

class KeyFrameDetector:
    """
    å…³é”®å¸§æ£€æµ‹å™¨ - é‡æ„ç‰ˆæœ¬
    åŸºäºç›¸ä¼¼åº¦çš„å…³é”®å¸§æ£€æµ‹ï¼Œæ›¿ä»£åŸæœ‰çš„äº‹ä»¶æ£€æµ‹ç³»ç»Ÿ
    
    å®ç°ç”¨æˆ·éœ€æ±‚çš„å…³é”®å¸§é€»è¾‘:
    1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
    2. é€å¸§æ¯”å¯¹: 1vs0, 2vs1, 3vs2...
    3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
    """
    
    def __init__(self, config):
        self.config = config
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # dHashé…ç½®
        self.hash_size = config.get('dhash_size', 8)
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½® (æ–°å¢ï¼ŒåŸºäºè¡Œä¸šæ ‡å‡†)
        self.similarity_threshold = config.get('similarity_threshold', 0.90)  # 90%é»˜è®¤
        
        # ä»ç›¸ä¼¼åº¦æ¢ç®—æ±‰æ˜è·ç¦»é˜ˆå€¼
        max_bits = self.hash_size * self.hash_size
        self.hamming_threshold = int((1 - self.similarity_threshold) * max_bits)
        
        print(f"æ¨¡å—: å…³é”®å¸§æ£€æµ‹å™¨å·²åŠ è½½ (é‡æ„ç‰ˆæœ¬) - ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold:.0%}, "
              f"æ±‰æ˜é˜ˆå€¼: {self.hamming_threshold}")

    def detect_keyframes(self, video_path: str, decoder: GPUDecoder, 
                        subtitle_area: Tuple[int, int, int, int]) -> List[int]:
        """
        æ£€æµ‹è§†é¢‘ä¸­æ‰€æœ‰å…³é”®å¸§ (åŸç‰ˆæœ¬ï¼Œä¸ç¼“å­˜)
        
        å®ç°é€»è¾‘:
        1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        2. é€å¸§æ¯”å¯¹: 1vs0, 2vs1, 3vs2...
        3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            subtitle_area: å­—å¹•åŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            
        Returns:
            å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ [0, 45, 89, 156, ...]
        """
        keyframes, _ = self.detect_keyframes_with_cache(video_path, decoder, subtitle_area)
        return keyframes

    def detect_keyframes_with_cache(self, video_path: str, decoder: GPUDecoder, 
                                   subtitle_area: Tuple[int, int, int, int]) -> Tuple[List[int], Dict[int, np.ndarray]]:
        """
        æ£€æµ‹è§†é¢‘ä¸­æ‰€æœ‰å…³é”®å¸§ + åŒæ­¥ç¼“å­˜å…³é”®å¸§å›¾åƒæ•°æ®
        
        ğŸ†• æ–°å¢åŠŸèƒ½: åœ¨å…³é”®å¸§æ£€æµ‹è¿‡ç¨‹ä¸­åŒæ­¥ç¼“å­˜å…³é”®å¸§çš„å›¾åƒæ•°æ®ï¼Œ
        é¿å…åç»­OCRè¯†åˆ«é˜¶æ®µçš„é‡å¤è§†é¢‘è§£ç 
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹
            subtitle_area: å­—å¹•åŒºåŸŸåæ ‡ (x1, y1, x2, y2)
            
        Returns:
            Tuple[List[int], Dict[int, np.ndarray]]: 
            - å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ [0, 45, 89, ...]
            - å…³é”®å¸§å›¾åƒç¼“å­˜ {0: image_array, 45: image_array, ...}
        """
        print("ğŸ” å¼€å§‹å…³é”®å¸§æ£€æµ‹ (åŒæ­¥ç¼“å­˜æ¨¡å¼)...")
        x1, y1, x2, y2 = subtitle_area

        # 1. æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„ç‰¹å¾ + åŒæ­¥ç¼“å­˜
        all_hashes, all_stds, keyframe_cache = self._compute_frame_features_with_cache(
            video_path, decoder, (x1, y1, x2, y2)
        )
        print(f"ğŸ“Š å®Œæˆç‰¹å¾è®¡ç®—: {len(all_hashes)} å¸§")

        # 2. ä½¿ç”¨å¤§æ´¥æ³•ç¡®å®šç©ºç™½å¸§é˜ˆå€¼
        blank_threshold = self._get_otsu_threshold(all_stds)
        print(f"ğŸ¯ ç©ºç™½å¸§é˜ˆå€¼: {blank_threshold:.4f}")

        # 3. å…³é”®å¸§é€å¸§æ£€æµ‹
        keyframes = self._detect_keyframes_sequential(all_hashes, all_stds, blank_threshold)
        
        # 4. åªä¿ç•™æ£€æµ‹åˆ°çš„å…³é”®å¸§ç¼“å­˜ï¼Œé‡Šæ”¾å…¶ä»–ç¼“å­˜
        final_keyframe_cache = {k: keyframe_cache[k] for k in keyframes if k in keyframe_cache}
        
        # 5. æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        cache_size_mb = len(final_keyframe_cache) * 0.307  # æ¯å¸§çº¦307KB
        print(f"âœ… æ£€æµ‹åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        print(f"ğŸ—‚ï¸  å…³é”®å¸§ç¼“å­˜: {len(final_keyframe_cache)} å¸§ï¼Œçº¦ {cache_size_mb:.1f}MB")
        
        return keyframes, final_keyframe_cache
    
    def _detect_keyframes_sequential(self, hashes: List[np.ndarray], 
                                   stds: np.ndarray, blank_threshold: float) -> List[int]:
        """
        æŒ‰ç…§æ–°é€»è¾‘è¿›è¡Œå…³é”®å¸§æ£€æµ‹
        å®ç°ç”¨æˆ·éœ€æ±‚çš„å…·ä½“ç®—æ³•
        """
        keyframes = []
        
        # ç»Ÿè®¡æ•°æ®åˆå§‹åŒ–
        similarity_stats = {
            'gte_80_percent': 0,  # >=80%çš„å¸§æ•°
            'lt_80_percent': 0,   # <80%ä½†>0%çš„å¸§æ•°
            'zero_percent': 0     # 0%çš„å¸§æ•°
        }
        
        # 1. ç¬¬ä¸€å¸§é»˜è®¤ä¸ºå…³é”®å¸§
        keyframes.append(0)
        print(f"ğŸ“Œ å…³é”®å¸§ 0: é»˜è®¤ç¬¬ä¸€å¸§")
        
        print(f"ğŸ”„ æ­£åœ¨åˆ†æ {len(hashes)} å¸§çš„ç›¸ä¼¼åº¦...")
        
        # 2. ä»ç¬¬1å¸§å¼€å§‹é€å¸§æ¯”å¯¹
        for curr_frame in range(1, len(hashes)):
            prev_frame = curr_frame - 1
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self._calculate_similarity(
                hashes[prev_frame], hashes[curr_frame],
                stds[prev_frame], stds[curr_frame], 
                blank_threshold
            )
            
            # ç»Ÿè®¡ç›¸ä¼¼åº¦åˆ†å¸ƒ
            if similarity == 0.0:
                similarity_stats['zero_percent'] += 1
            elif similarity >= 0.80:
                similarity_stats['gte_80_percent'] += 1
            else:
                similarity_stats['lt_80_percent'] += 1
            
            # 3. ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ â†’ æ–°å…³é”®å¸§
            if similarity < self.similarity_threshold:
                keyframes.append(curr_frame)
            
            # è¿›åº¦æ˜¾ç¤º (æ¯1000å¸§æ›´æ–°ä¸€æ¬¡)
            if curr_frame % 1000 == 0:
                progress = (curr_frame / len(hashes)) * 100
                print(f"  ğŸ” æ£€æµ‹è¿›åº¦: {curr_frame}/{len(hashes)} ({progress:.1f}%) | "
                      f"ç›¸ä¼¼åº¦åˆ†å¸ƒ: >={int(0.8*100)}%å¸§:{similarity_stats['gte_80_percent']}/"
                      f"<{int(0.8*100)}%å¸§:{similarity_stats['lt_80_percent']}/"
                      f"0%å¸§:{similarity_stats['zero_percent']} | "
                      f"å…³é”®å¸§:{len(keyframes)}ä¸ª")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        total_compared = len(hashes) - 1  # ç¬¬ä¸€å¸§ä¸å‚ä¸æ¯”è¾ƒ
        print(f"ğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡(æ€»è®¡{total_compared}å¸§): >={int(0.8*100)}%å¸§:{similarity_stats['gte_80_percent']}/"
              f"<{int(0.8*100)}%å¸§:{similarity_stats['lt_80_percent']}/0%å¸§:{similarity_stats['zero_percent']}")
        print(f"âœ… å…³é”®å¸§æ£€æµ‹å®Œæˆ: å…±æ‰¾åˆ° {len(keyframes)} ä¸ªå…³é”®å¸§")
        return keyframes
    
    def _calculate_similarity(self, hash1: np.ndarray, hash2: np.ndarray,
                            std1: float, std2: float, blank_threshold: float) -> float:
        """
        è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        ç›¸ä¼¼åº¦è®¡ç®—è§„åˆ™:
        - ç©ºç™½å¸§ vs ç©ºç™½å¸§: 100%
        - ç©ºç™½å¸§ vs å†…å®¹å¸§: 0%  
        - å†…å®¹å¸§ vs å†…å®¹å¸§: åŸºäºdHashçš„æ±‰æ˜è·ç¦»
        
        åŸºäºè¡Œä¸šæ ‡å‡†Dr. Neal Krawetzçš„ç ”ç©¶æˆæœ
        """
        # åˆ¤æ–­å¸§ç±»å‹
        is_blank1 = std1 < blank_threshold
        is_blank2 = std2 < blank_threshold
        
        # Case 1: ä¸¤å¸§éƒ½æ˜¯ç©ºç™½å¸§ â†’ ç›¸ä¼¼åº¦100%
        if is_blank1 and is_blank2:
            return 1.0
        
        # Case 2: ä¸€ä¸ªç©ºç™½ä¸€ä¸ªéç©ºç™½ â†’ ç›¸ä¼¼åº¦0% (å®Œå…¨ä¸åŒ)
        if is_blank1 != is_blank2:
            return 0.0
        
        # Case 3: ä¸¤å¸§éƒ½æœ‰å†…å®¹ â†’ åŸºäºdHashè®¡ç®—ç›¸ä¼¼åº¦
        hamming_distance = np.count_nonzero(hash1 != hash2)
        max_possible_distance = hash1.size  # 64 for 8x8 dHash
        
        # ç›¸ä¼¼åº¦ = 1 - (æ±‰æ˜è·ç¦» / æœ€å¤§å¯èƒ½è·ç¦»)
        similarity = 1.0 - (hamming_distance / max_possible_distance)
        
        return similarity
    
    def _compute_frame_features(self, video_path: str, decoder: GPUDecoder, 
                               crop_rect: Tuple[int, int, int, int]) -> Tuple[List[np.ndarray], np.ndarray]:
        """
        æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„dHashå’Œæ ‡å‡†å·®
        å¤ç”¨åŸæœ‰çš„GPUæ‰¹é‡è®¡ç®—é€»è¾‘
        """
        all_hashes = []
        all_stds = []
        x1, y1, x2, y2 = crop_rect

        frame_count = 0
        batch_count = 0
        
        print("ğŸ”„ æ­£åœ¨è®¡ç®—è§†é¢‘ç‰¹å¾...")
        
        for batch_tensor, _ in decoder.decode(video_path):
            # è£å‰ªå­—å¹•åŒºåŸŸ
            cropped_batch = batch_tensor[:, :, y1:y2, x1:x2]

            # --- åœ¨GPUä¸Šæ‰¹é‡è®¡ç®— --- #
            # 1. è®¡ç®—æ ‡å‡†å·®
            stds = torch.std(cropped_batch.float().view(cropped_batch.size(0), -1), dim=1)
            all_stds.extend(stds.cpu().numpy())

            # 2. è®¡ç®—dHash
            grayscale_batch = cropped_batch.float().mean(dim=1, keepdim=True)
            resized_batch = torch.nn.functional.interpolate(
                grayscale_batch, 
                size=(self.hash_size, self.hash_size + 1), 
                mode='bilinear', align_corners=False
            )
            diff = resized_batch[:, :, :, 1:] > resized_batch[:, :, :, :-1]
            hashes_np = diff.cpu().numpy().astype(np.uint8).reshape(diff.shape[0], -1)
            all_hashes.extend(hashes_np)
            
            frame_count += batch_tensor.size(0)
            batch_count += 1
            
            # æ¯50ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if batch_count % 50 == 0:
                print(f"  ğŸ“Š å·²å¤„ç† {frame_count} å¸§...")
            
        print(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: å…±å¤„ç† {frame_count} å¸§")
        return all_hashes, np.array(all_stds)
    
    def _compute_frame_features_with_cache(self, video_path: str, decoder: GPUDecoder, 
                                          crop_rect: Tuple[int, int, int, int]) -> Tuple[List[np.ndarray], np.ndarray, Dict[int, np.ndarray]]:
        """
        æ‰¹é‡è®¡ç®—æ‰€æœ‰å¸§çš„dHashå’Œæ ‡å‡†å·® + æ™ºèƒ½ç¼“å­˜å…³é”®å¸§å›¾åƒ
        
        ğŸ†• æ–°åŠŸèƒ½: åœ¨è®¡ç®—ç‰¹å¾çš„åŒæ—¶ï¼Œæ™ºèƒ½ç¼“å­˜å¯èƒ½æˆä¸ºå…³é”®å¸§çš„å›¾åƒæ•°æ®
        é€šè¿‡ç²—ç­›é€‰æœºåˆ¶ï¼Œå‡å°‘æ— å…³å¸§çš„ç¼“å­˜ï¼Œæ§åˆ¶å†…å­˜ä½¿ç”¨
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            decoder: GPUè§£ç å™¨å®ä¾‹ 
            crop_rect: è£å‰ªåŒºåŸŸ (x1, y1, x2, y2)
            
        Returns:
            Tuple[List[np.ndarray], np.ndarray, Dict[int, np.ndarray]]:
            - all_hashes: æ‰€æœ‰å¸§çš„dHashç‰¹å¾åˆ—è¡¨
            - all_stds: æ‰€æœ‰å¸§çš„æ ‡å‡†å·®æ•°ç»„
            - keyframe_cache: å€™é€‰å…³é”®å¸§çš„å›¾åƒç¼“å­˜å­—å…¸
        """
        all_hashes = []
        all_stds = []
        keyframe_cache = {}
        x1, y1, x2, y2 = crop_rect

        frame_count = 0
        batch_count = 0
        prev_hash = None
        cached_frames_count = 0
        
        print("ğŸ”„ æ­£åœ¨è®¡ç®—è§†é¢‘ç‰¹å¾å¹¶æ™ºèƒ½ç¼“å­˜...")
        
        for batch_tensor, _ in decoder.decode(video_path):
            # è£å‰ªå­—å¹•åŒºåŸŸ
            cropped_batch = batch_tensor[:, :, y1:y2, x1:x2]

            # --- åœ¨GPUä¸Šæ‰¹é‡è®¡ç®—ç‰¹å¾ --- #
            # 1. è®¡ç®—æ ‡å‡†å·®
            stds = torch.std(cropped_batch.float().view(cropped_batch.size(0), -1), dim=1)
            batch_stds = stds.cpu().numpy()
            all_stds.extend(batch_stds)

            # 2. è®¡ç®—dHash
            grayscale_batch = cropped_batch.float().mean(dim=1, keepdim=True)
            resized_batch = torch.nn.functional.interpolate(
                grayscale_batch, 
                size=(self.hash_size, self.hash_size + 1), 
                mode='bilinear', align_corners=False
            )
            diff = resized_batch[:, :, :, 1:] > resized_batch[:, :, :, :-1]
            batch_hashes = diff.cpu().numpy().astype(np.uint8).reshape(diff.shape[0], -1)
            all_hashes.extend(batch_hashes)
            
            # --- ğŸ†• æ™ºèƒ½ç¼“å­˜å€™é€‰å…³é”®å¸§ --- #
            for i, curr_hash in enumerate(batch_hashes):
                frame_idx = frame_count + i
                
                # ç¬¬ä¸€å¸§é»˜è®¤ç¼“å­˜
                if frame_idx == 0:
                    frame_np = cropped_batch[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)
                    keyframe_cache[frame_idx] = frame_np
                    prev_hash = curr_hash
                    cached_frames_count += 1
                    continue
                
                # å¿«é€Ÿç›¸ä¼¼åº¦é¢„åˆ¤æ–­ (ç²—ç­›)
                if prev_hash is not None:
                    hamming_distance = np.count_nonzero(curr_hash != prev_hash)
                    rough_similarity = 1.0 - (hamming_distance / curr_hash.size)
                    
                    # ç¼“å­˜å¯èƒ½æ˜¯å…³é”®å¸§çš„å¸§ (ç›¸ä¼¼åº¦ < 95%ï¼Œå®½æ¾é¢„ç­›é€‰)
                    if rough_similarity < 0.95:
                        frame_np = cropped_batch[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)
                        keyframe_cache[frame_idx] = frame_np
                        prev_hash = curr_hash
                        cached_frames_count += 1
            
            frame_count += batch_tensor.size(0)
            batch_count += 1
            
            # æ¯50ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ + ç¼“å­˜ç»Ÿè®¡
            if batch_count % 50 == 0:
                cache_mb = cached_frames_count * 0.307  # æ¯å¸§çº¦307KB
                cache_ratio = (cached_frames_count / frame_count) * 100
                print(f"  ğŸ“Š å·²å¤„ç† {frame_count} å¸§ï¼Œé¢„ç¼“å­˜ {cached_frames_count} å¸§ ({cache_ratio:.1f}%, ~{cache_mb:.1f}MB)")
            
        # æœ€ç»ˆç»Ÿè®¡
        final_cache_mb = cached_frames_count * 0.307
        cache_ratio = (cached_frames_count / frame_count) * 100
        print(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: å…±å¤„ç† {frame_count} å¸§")
        print(f"ğŸ—‚ï¸  é¢„ç¼“å­˜ç»Ÿè®¡: {cached_frames_count} å¸§ ({cache_ratio:.1f}%), çº¦ {final_cache_mb:.1f}MB")
        
        return all_hashes, np.array(all_stds), keyframe_cache
    
    def _get_otsu_threshold(self, stds: np.ndarray) -> float:
        """ä½¿ç”¨å¤§æ´¥æ³•è®¡ç®—æœ€ä½³ç©ºç™½å¸§é˜ˆå€¼"""
        if stds.max() == stds.min(): 
            return 0.0
        
        stds_normalized = (255 * (stds - stds.min()) / (stds.max() - stds.min())).astype(np.uint8)
        threshold_otsu, _ = cv2.threshold(stds_normalized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        original_threshold = threshold_otsu / 255 * (stds.max() - stds.min()) + stds.min()
        return float(original_threshold)

    def generate_subtitle_segments(self, keyframes: List[int], 
                                 fps: float, total_frames: int) -> List[Dict]:
        """
        ä»å…³é”®å¸§åˆ—è¡¨ç”Ÿæˆå­—å¹•æ®µè½
        æ¯ä¸¤ä¸ªè¿ç»­å…³é”®å¸§ä¹‹é—´å½¢æˆä¸€ä¸ªæ®µè½
        
        Args:
            keyframes: å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
            fps: è§†é¢‘å¸§ç‡
            total_frames: è§†é¢‘æ€»å¸§æ•°
            
        Returns:
            æ®µè½åˆ—è¡¨ï¼ŒåŒ…å«å…³é”®å¸§ä¿¡æ¯
        """
        segments = []
        
        print(f"ğŸ—ï¸ ä» {len(keyframes)} ä¸ªå…³é”®å¸§ç”Ÿæˆå­—å¹•æ®µè½...")
        
        for i in range(len(keyframes)):
            start_frame = keyframes[i]
            
            # ç¡®å®šç»“æŸå¸§
            if i + 1 < len(keyframes):
                end_frame = keyframes[i + 1] - 1  # ä¸‹ä¸€å…³é”®å¸§çš„å‰ä¸€å¸§
            else:
                end_frame = total_frames - 1  # è§†é¢‘çš„æœ€åä¸€å¸§
            
            # è®¡ç®—æ—¶é—´æˆ³
            start_time = start_frame / fps
            end_time = end_frame / fps
            
            segments.append({
                'key_frame': start_frame,      # ğŸ†• å…³é”®å¸§ä¿¡æ¯
                'start_frame': start_frame,
                'end_frame': end_frame, 
                'start_time': start_time,
                'end_time': end_time,
                'duration': end_time - start_time
            })
        
        print(f"âœ… ç”Ÿæˆäº† {len(segments)} ä¸ªå­—å¹•æ®µè½")
        return segments